{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Twitter Sentiment Analysis**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "data = 'tweets.txt'\n",
    "# df = pd.read_csv(data, sep='\\t', header=None)\n",
    "# df.columns = ['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treat each line as a dictionary\n",
    "tweets = []\n",
    "for line in open(data, 'r'):\n",
    "    tweet = json.loads(line)\n",
    "    try:\n",
    "        #print(tweet.get('quoted_status').get('text'))\n",
    "        desired = tweet.get('quoted_status').get('text')\n",
    "        tweets.append(desired)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    \n",
    "    #print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the tweets to a dataframe\n",
    "df = pd.DataFrame(tweets)\n",
    "df.columns = ['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gonna make a peach acc too :)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@RebeccaFMusic It would be amazing if one day ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you woke up in a middle of a night and noticed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>All of our tagline and hashtags are trending N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All of our tagline and hashtags are trending N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet\n",
       "0                      gonna make a peach acc too :)\n",
       "1  @RebeccaFMusic It would be amazing if one day ...\n",
       "2  you woke up in a middle of a night and noticed...\n",
       "3  All of our tagline and hashtags are trending N...\n",
       "4  All of our tagline and hashtags are trending N..."
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column 'mood' and takes happy :) or sad :( emoticons fro the tweets\n",
    "df['mood'] = df['tweet'].apply(lambda tweet: ':)' if ':)' in tweet else ':(')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>mood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gonna make a peach acc too :)</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@RebeccaFMusic It would be amazing if one day ...</td>\n",
       "      <td>:(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you woke up in a middle of a night and noticed...</td>\n",
       "      <td>:(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>All of our tagline and hashtags are trending N...</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All of our tagline and hashtags are trending N...</td>\n",
       "      <td>:)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet mood\n",
       "0                      gonna make a peach acc too :)   :)\n",
       "1  @RebeccaFMusic It would be amazing if one day ...   :(\n",
       "2  you woke up in a middle of a night and noticed...   :(\n",
       "3  All of our tagline and hashtags are trending N...   :)\n",
       "4  All of our tagline and hashtags are trending N...   :)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column 'text' and takes the text from the tweets without the emoticons\n",
    "df['text'] = df['tweet'].apply(lambda tweet: tweet.replace(':)', '').replace(':(', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('tweet', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mood</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>:)</td>\n",
       "      <td>gonna make a peach acc too</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>:(</td>\n",
       "      <td>@RebeccaFMusic It would be amazing if one day ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>:(</td>\n",
       "      <td>you woke up in a middle of a night and noticed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>:)</td>\n",
       "      <td>All of our tagline and hashtags are trending N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>:)</td>\n",
       "      <td>All of our tagline and hashtags are trending N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mood                                               text\n",
       "0   :)                        gonna make a peach acc too \n",
       "1   :(  @RebeccaFMusic It would be amazing if one day ...\n",
       "2   :(  you woke up in a middle of a night and noticed...\n",
       "3   :)  All of our tagline and hashtags are trending N...\n",
       "4   :)  All of our tagline and hashtags are trending N..."
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(377, 2)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the text column\n",
    "import re\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'@\\w+', '', text) # remove mentions\n",
    "    text = re.sub(r'#\\w+', '', text) # remove hashtags\n",
    "    text = re.sub(r'http\\S+', '', text) # remove links\n",
    "    text = re.sub(r'\\d+', '', text) # remove digits\n",
    "    text = text.strip('\\'\"') # remove quotation marks\n",
    "    text = text.lower() # lower case\n",
    "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text) # remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text) # remove multiple spaces\n",
    "    # remove punctuation\n",
    "    text = text.replace('[^\\w\\s]','')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the clean_text function to the text column\n",
    "df['text'] = df['text'].apply(lambda tweet: clean_text(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mood</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>:)</td>\n",
       "      <td>gonna make a peach acc too</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>:(</td>\n",
       "      <td>it would be amazing if one day the industry w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>:(</td>\n",
       "      <td>you woke up in a middle of a night and noticed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>:)</td>\n",
       "      <td>all of our tagline and hashtags are trending n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>:)</td>\n",
       "      <td>all of our tagline and hashtags are trending n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mood                                               text\n",
       "0   :)                        gonna make a peach acc too \n",
       "1   :(   it would be amazing if one day the industry w...\n",
       "2   :(  you woke up in a middle of a night and noticed...\n",
       "3   :)  all of our tagline and hashtags are trending n...\n",
       "4   :)  all of our tagline and hashtags are trending n..."
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing \n",
    "\n",
    "# For tokenization\n",
    "import nltk\n",
    "\n",
    "# For converting words into frequency counts\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step in pipeline\n",
    "# Keep words that appear in atleast 2 documents, keeps 5000 most common words\n",
    "preprocessor = CountVectorizer(min_df=2, tokenizer=nltk.word_tokenize, max_features=1000, binary=True)\n",
    "# preprocessor = TfidfVectorizer(min_df=2, tokenizer=nltk.word_tokenize, max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing output shape: (377, 533)\n",
      "First datapoint: gonna make a peach acc too \n",
      "First datapoint tokens: ['gon', 'na', 'make', 'a', 'peach', 'acc', 'too']\n",
      "First datapoint Binary Bag of Words (sparse) representation:\n",
      "  (0, 0)\t1\n",
      "  (0, 182)\t1\n",
      "  (0, 263)\t1\n",
      "  (0, 287)\t1\n",
      "  (0, 450)\t1\n",
      "First datapoint Binary Bag of Words (dense) representation:\n",
      "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickokwir/miniconda/envs/DeepLearning/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gon': 182,\n",
       " 'na': 287,\n",
       " 'make': 263,\n",
       " 'a': 0,\n",
       " 'too': 450,\n",
       " 'it': 221,\n",
       " 'would': 514,\n",
       " 'be': 48,\n",
       " 'amazing': 23,\n",
       " 'if': 209,\n",
       " 'one': 305,\n",
       " 'day': 110,\n",
       " 'the': 427,\n",
       " 'industry': 215,\n",
       " 'let': 244,\n",
       " 'artists': 38,\n",
       " 'completely': 92,\n",
       " 'free': 164,\n",
       " 'unfortunately': 465,\n",
       " 'they': 432,\n",
       " 'you': 528,\n",
       " 'up': 467,\n",
       " 'in': 212,\n",
       " 'of': 301,\n",
       " 'night': 295,\n",
       " 'and': 26,\n",
       " 'at': 41,\n",
       " 'your': 530,\n",
       " 'all': 17,\n",
       " 'our': 312,\n",
       " 'tagline': 418,\n",
       " 'hashtags': 192,\n",
       " 'are': 33,\n",
       " 'trending': 455,\n",
       " 'nationwide': 288,\n",
       " 'worldwide': 512,\n",
       " 'great': 184,\n",
       " 'job': 229,\n",
       " 'fam': 147,\n",
       " 's': 361,\n",
       " 'keep': 237,\n",
       " 'on': 304,\n",
       " 'tweeting': 459,\n",
       " 'f': 145,\n",
       " 'most': 280,\n",
       " 'with': 507,\n",
       " 'pls': 331,\n",
       " 'its': 222,\n",
       " 'so': 392,\n",
       " 'here': 200,\n",
       " 'is': 220,\n",
       " 'replay': 354,\n",
       " 'artist': 37,\n",
       " 'out': 313,\n",
       " 'may': 267,\n",
       " 'will': 506,\n",
       " 'to': 447,\n",
       " 'help': 197,\n",
       " 'rt': 360,\n",
       " 'wts': 517,\n",
       " 'lfb': 246,\n",
       " 'ph': 324,\n",
       " 'photocards': 325,\n",
       " 'clean': 78,\n",
       " 'amp': 24,\n",
       " 'onhand': 306,\n",
       " 'check': 75,\n",
       " 'i': 208,\n",
       " 'think': 435,\n",
       " 'community': 89,\n",
       " 'san': 366,\n",
       " 'absolutely': 2,\n",
       " 'happy': 188,\n",
       " 'an': 25,\n",
       " 'ive': 224,\n",
       " 'seen': 375,\n",
       " 'this': 437,\n",
       " 'film': 152,\n",
       " 'before': 52,\n",
       " 'didnt': 118,\n",
       " 'like': 249,\n",
       " 'endinggg': 134,\n",
       " 'shot': 383,\n",
       " 'puno': 343,\n",
       " 'yeah': 520,\n",
       " 'just': 233,\n",
       " 'had': 186,\n",
       " 'those': 438,\n",
       " 'how': 206,\n",
       " 'much': 283,\n",
       " 'them': 429,\n",
       " 'w': 482,\n",
       " 'tag': 417,\n",
       " 'wan': 486,\n",
       " 'know': 238,\n",
       " 'better': 55,\n",
       " 'any': 30,\n",
       " 'downside': 125,\n",
       " 'thanks': 425,\n",
       " 'more': 277,\n",
       " 'yipppeeee': 526,\n",
       " 'lost': 256,\n",
       " 'my': 285,\n",
       " 'wallet': 485,\n",
       " 'going': 181,\n",
       " 'from': 169,\n",
       " 'worse': 513,\n",
       " 'ini': 217,\n",
       " 'sometimes': 399,\n",
       " 'even': 137,\n",
       " 'hi': 201,\n",
       " 'jeno': 227,\n",
       " 'fee': 148,\n",
       " 'years': 522,\n",
       " 'ago': 12,\n",
       " 'today': 448,\n",
       " 'their': 428,\n",
       " 'tour': 452,\n",
       " 'by': 68,\n",
       " 'popo': 335,\n",
       " 'juseyooo': 232,\n",
       " 'baby': 45,\n",
       " 'g': 171,\n",
       " 'getting': 176,\n",
       " 'excited': 142,\n",
       " 'as': 39,\n",
       " 'soon': 401,\n",
       " 'arrived': 35,\n",
       " 'germany': 174,\n",
       " 'im': 211,\n",
       " 'not': 298,\n",
       " 'crying': 104,\n",
       " 'u': 461,\n",
       " 'left': 243,\n",
       " 'comment': 85,\n",
       " 'under': 463,\n",
       " 'post': 336,\n",
       " 'when': 500,\n",
       " 'walks': 484,\n",
       " 'proud': 342,\n",
       " 'ok': 303,\n",
       " 'we': 493,\n",
       " 'has': 190,\n",
       " 'now': 299,\n",
       " 'been': 51,\n",
       " 'see': 374,\n",
       " 'some': 396,\n",
       " 't': 416,\n",
       " 'batch': 47,\n",
       " 'non': 297,\n",
       " 'generative': 172,\n",
       " 'characters': 73,\n",
       " 'using': 474,\n",
       " 'geometric': 173,\n",
       " 'shapes': 380,\n",
       " 'sama': 364,\n",
       " 'grid': 185,\n",
       " 'system': 415,\n",
       " 'click': 79,\n",
       " 'collect': 80,\n",
       " 'she': 382,\n",
       " 'looks': 255,\n",
       " 'don': 122,\n",
       " 'tomorrow': 449,\n",
       " 'both': 62,\n",
       " 'red': 352,\n",
       " 'cries': 103,\n",
       " 'cutest': 106,\n",
       " 'thing': 433,\n",
       " 'ever': 138,\n",
       " 'such': 413,\n",
       " 'cute': 105,\n",
       " 'his': 203,\n",
       " 'dino': 119,\n",
       " 'checking': 76,\n",
       " 'face': 146,\n",
       " 'because': 49,\n",
       " 'someone': 397,\n",
       " 'said': 363,\n",
       " 'he': 195,\n",
       " 'finished': 155,\n",
       " 'whole': 504,\n",
       " 'bottle': 63,\n",
       " 'for': 161,\n",
       " 'thread': 439,\n",
       " 'well': 496,\n",
       " 'do': 121,\n",
       " 'have': 193,\n",
       " 'over': 314,\n",
       " 'me': 268,\n",
       " 'says': 370,\n",
       " 'what': 499,\n",
       " 'want': 487,\n",
       " 'children': 77,\n",
       " 'll': 252,\n",
       " 'always': 20,\n",
       " 'm': 260,\n",
       " 'soft': 394,\n",
       " 'smile': 390,\n",
       " 'then': 430,\n",
       " 'tuh': 457,\n",
       " 'top': 451,\n",
       " 'influential': 216,\n",
       " 'twitter': 460,\n",
       " 'accounts': 4,\n",
       " 'within': 509,\n",
       " 'space': 402,\n",
       " 'past': 319,\n",
       " 'week': 494,\n",
       " 'who': 503,\n",
       " 'or': 310,\n",
       " 'also': 19,\n",
       " 'collection': 81,\n",
       " 'about': 1,\n",
       " 'v': 475,\n",
       " 'time': 445,\n",
       " 'through': 440,\n",
       " 'right': 358,\n",
       " 'that': 426,\n",
       " 'us': 472,\n",
       " 'ready': 349,\n",
       " 'friend': 167,\n",
       " 'am': 21,\n",
       " 'there': 431,\n",
       " 'chase': 74,\n",
       " 'dan': 108,\n",
       " 'giveaway': 177,\n",
       " 'account': 3,\n",
       " 'followers': 159,\n",
       " 'sure': 414,\n",
       " 'follow': 158,\n",
       " 'share': 381,\n",
       " 'friends': 168,\n",
       " 'confirmed': 96,\n",
       " 'contestant': 98,\n",
       " 'commission': 86,\n",
       " 'still': 407,\n",
       " 'very': 478,\n",
       " 'open': 308,\n",
       " 'commissions': 87,\n",
       " 'urgent': 471,\n",
       " 'excellent': 141,\n",
       " 'read': 347,\n",
       " 'return': 355,\n",
       " 'alexander': 15,\n",
       " 'tig': 443,\n",
       " 'trager': 453,\n",
       " 'brilliant': 65,\n",
       " 'hello': 196,\n",
       " 'po': 333,\n",
       " 'miss': 274,\n",
       " 'ria': 357,\n",
       " 'ako': 13,\n",
       " 'maghohost': 262,\n",
       " 'sent': 378,\n",
       " 'dm': 120,\n",
       " 'ur': 470,\n",
       " 'interest': 219,\n",
       " 'cover': 100,\n",
       " 'confident': 95,\n",
       " 'but': 67,\n",
       " 'everything': 139,\n",
       " 'own': 315,\n",
       " 'collections': 82,\n",
       " 'piece': 327,\n",
       " 'price': 340,\n",
       " 'story': 410,\n",
       " 'was': 490,\n",
       " 'stan': 405,\n",
       " 'accs': 5,\n",
       " 'made': 261,\n",
       " 'many': 266,\n",
       " 'means': 269,\n",
       " 'off': 302,\n",
       " 'french': 166,\n",
       " 'speaking': 403,\n",
       " 'feminists': 151,\n",
       " 'activists': 7,\n",
       " 'helping': 198,\n",
       " 'create': 102,\n",
       " 'language': 241,\n",
       " 'inclusivity': 213,\n",
       " 'anyone': 31,\n",
       " 'young': 529,\n",
       " 'lady': 240,\n",
       " 'emergency': 131,\n",
       " 'men': 270,\n",
       " 'please': 330,\n",
       " 'judge': 231,\n",
       " 'book': 59,\n",
       " 'vote': 480,\n",
       " 'exit': 143,\n",
       " 'strategy': 411,\n",
       " 'first': 156,\n",
       " 'comp': 90,\n",
       " 'look': 254,\n",
       " 'really': 350,\n",
       " 'feel': 149,\n",
       " 'tickets': 442,\n",
       " 'long': 253,\n",
       " 'live': 251,\n",
       " 'available': 44,\n",
       " 'go': 180,\n",
       " 'can': 70,\n",
       " 'stop': 408,\n",
       " 'reading': 348,\n",
       " 'her': 199,\n",
       " 'l': 239,\n",
       " 'trying': 456,\n",
       " 'hard': 189,\n",
       " 'get': 175,\n",
       " 'bit': 58,\n",
       " 'art': 36,\n",
       " 'big': 57,\n",
       " 'lt': 259,\n",
       " 'good': 183,\n",
       " 'morning': 278,\n",
       " 'around': 34,\n",
       " 'than': 423,\n",
       " 'people': 322,\n",
       " 'happens': 187,\n",
       " 'autistic': 43,\n",
       " 'y': 519,\n",
       " 'paps': 317,\n",
       " 'karan': 236,\n",
       " 'bhai': 56,\n",
       " 'solo': 395,\n",
       " 'dono': 123,\n",
       " 'p': 316,\n",
       " 'individuality': 214,\n",
       " 'corner': 99,\n",
       " 'coming': 84,\n",
       " 'yours': 531,\n",
       " 'why': 505,\n",
       " 'tweet': 458,\n",
       " 'c': 69,\n",
       " 'something': 398,\n",
       " 'stories': 409,\n",
       " 'sign': 388,\n",
       " 'written': 516,\n",
       " 'which': 501,\n",
       " 'translates': 454,\n",
       " 'thank': 424,\n",
       " 'saving': 367,\n",
       " 'hongjoong': 204,\n",
       " 'saw': 368,\n",
       " 'album': 14,\n",
       " 'woman': 510,\n",
       " 'chance': 72,\n",
       " 'opportunity': 309,\n",
       " 'thirbea': 436,\n",
       " 'kalinaw': 235,\n",
       " 'use': 473,\n",
       " 'hashtag': 191,\n",
       " 'easily': 129,\n",
       " 'find': 154,\n",
       " 'questions': 345,\n",
       " 'need': 290,\n",
       " 'come': 83,\n",
       " 'st': 404,\n",
       " 'slot': 389,\n",
       " 'j': 225,\n",
       " 've': 477,\n",
       " 'month': 276,\n",
       " 'hope': 205,\n",
       " 're': 346,\n",
       " 'show': 385,\n",
       " 'full': 170,\n",
       " 'video': 479,\n",
       " 'life': 248,\n",
       " 'project': 341,\n",
       " 'itu': 223,\n",
       " 'haven': 194,\n",
       " 'yet': 525,\n",
       " 'announce': 27,\n",
       " 'link': 250,\n",
       " 'complete': 91,\n",
       " 'morons': 279,\n",
       " 'present': 338,\n",
       " 'him': 202,\n",
       " 'last': 242,\n",
       " 'year': 521,\n",
       " 'did': 115,\n",
       " 'app': 32,\n",
       " 'only': 307,\n",
       " 'welcome': 495,\n",
       " 'lets': 245,\n",
       " 'congratulations': 97,\n",
       " 'best': 54,\n",
       " 'boy': 64,\n",
       " 'alldayclearfreshness': 18,\n",
       " 'withdonny': 508,\n",
       " 'iiiit': 210,\n",
       " 'due': 128,\n",
       " 'pretty': 339,\n",
       " 'love': 257,\n",
       " 'set': 379,\n",
       " 'pob': 334,\n",
       " 'php': 326,\n",
       " 'same': 365,\n",
       " 'payo': 320,\n",
       " 'sa': 362,\n",
       " 'tell': 421,\n",
       " 'regarding': 353,\n",
       " 'valorant': 476,\n",
       " 'social': 393,\n",
       " 'pcs': 321,\n",
       " 'pf': 323,\n",
       " 'lsf': 258,\n",
       " 'selling': 376,\n",
       " 'until': 466,\n",
       " 'days': 111,\n",
       " 'o': 300,\n",
       " 'new': 293,\n",
       " 'side': 387,\n",
       " 'ed': 130,\n",
       " 'th': 422,\n",
       " 'writers': 515,\n",
       " 'jangkku': 226,\n",
       " 'commmunity': 88,\n",
       " 'cranky': 101,\n",
       " 'mirror': 273,\n",
       " 'nawww': 289,\n",
       " 'diddums': 116,\n",
       " 'wante': 488,\n",
       " 'after': 10,\n",
       " 'ni': 294,\n",
       " 'didn': 117,\n",
       " 'way': 492,\n",
       " 'atinys': 42,\n",
       " 'huge': 207,\n",
       " 'following': 160,\n",
       " 'never': 292,\n",
       " 'voting': 481,\n",
       " 'plan': 328,\n",
       " 'start': 406,\n",
       " 'moving': 282,\n",
       " 'add': 9,\n",
       " 'data': 109,\n",
       " 'dick': 114,\n",
       " 'd': 107,\n",
       " 'finally': 153,\n",
       " 'monday': 275,\n",
       " 'rm': 359,\n",
       " 'n': 286,\n",
       " 'update': 469,\n",
       " 'become': 50,\n",
       " 'begins': 53,\n",
       " 'actually': 8,\n",
       " 'end': 133,\n",
       " 'were': 498,\n",
       " 'ama': 22,\n",
       " 'freezing': 165,\n",
       " 'things': 434,\n",
       " 'season': 372,\n",
       " 'no': 296,\n",
       " 'ppl': 337,\n",
       " 'muchhhhhh': 284,\n",
       " 'again': 11,\n",
       " 'song': 400,\n",
       " 'should': 384,\n",
       " 'found': 163,\n",
       " 'yes': 523,\n",
       " 'movie': 281,\n",
       " 'watch': 491,\n",
       " 'down': 124,\n",
       " 'man': 265,\n",
       " 'exactly': 140,\n",
       " 'jisung': 228,\n",
       " 'yoongi': 527,\n",
       " 'went': 497,\n",
       " 'take': 419,\n",
       " 'announcement': 28,\n",
       " 'might': 271,\n",
       " 'eyes': 144,\n",
       " 'uarmyhope': 462,\n",
       " 'instagram': 218,\n",
       " 'thursday': 441,\n",
       " 'engagement': 136,\n",
       " 'booster': 61,\n",
       " 'another': 29,\n",
       " 'youtube': 532,\n",
       " 'boost': 60,\n",
       " 'engageme': 135,\n",
       " 'snapchat': 391,\n",
       " 'wants': 489,\n",
       " 'bully': 66,\n",
       " 'tiktok': 444,\n",
       " 'play': 329,\n",
       " 'fella': 150,\n",
       " 'part': 318,\n",
       " 'wait': 483,\n",
       " 'saying': 369,\n",
       " 'deserved': 113,\n",
       " 'confession': 94,\n",
       " 'neha': 291,\n",
       " 'strong': 412,\n",
       " 'reason': 351,\n",
       " 'minutes': 272,\n",
       " 'work': 511,\n",
       " 'wtt': 518,\n",
       " 'lft': 247,\n",
       " 'k': 234,\n",
       " 'orang': 311,\n",
       " 'dr': 126,\n",
       " 'join': 230,\n",
       " 'pm': 332,\n",
       " 'drop': 127,\n",
       " 'send': 377,\n",
       " 'cause': 71,\n",
       " 'asked': 40,\n",
       " 'upcoming': 468,\n",
       " 'put': 344,\n",
       " 'undies': 464,\n",
       " 'back': 46,\n",
       " 'while': 502,\n",
       " 'yesterday': 524,\n",
       " 'giving': 179,\n",
       " 'se': 371,\n",
       " 'algu': 16,\n",
       " 'tiver': 446,\n",
       " 'descontos': 112,\n",
       " 'flixbus': 157,\n",
       " 'aceito': 6,\n",
       " 'forever': 162,\n",
       " 'shut': 386,\n",
       " 'en': 132,\n",
       " 'retweet': 356,\n",
       " 'gives': 178,\n",
       " 'teaser': 420,\n",
       " 'malaysian': 264,\n",
       " 'conditions': 93,\n",
       " 'secon': 373}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demo the preprocessor:\n",
    "X_example = preprocessor.fit_transform(df['text'].iloc[:1000])\n",
    "print(f'Preprocessing output shape: {X_example.shape}')\n",
    "\n",
    "# Show the process for the first datapoint\n",
    "first_datapoint = df['text'].iloc[0]\n",
    "print(f'First datapoint: {first_datapoint[:100]}')\n",
    "\n",
    "first_tokens = nltk.word_tokenize(first_datapoint)\n",
    "print(f'First datapoint tokens: {first_tokens[:10]}')\n",
    "\n",
    "first_bow = preprocessor.transform([first_datapoint])\n",
    "first_bow.maxprint = 5  # Change how many of the non-zero elements are printing to not clutter the notebook\n",
    "print(f'First datapoint Binary Bag of Words (sparse) representation:\\n{first_bow}')\n",
    "print(f'First datapoint Binary Bag of Words (dense) representation:\\n{first_bow.todense()}')\n",
    "\n",
    "preprocessor.vocabulary_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB # Bernoulli because we have binary features\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([('preprocessing', preprocessor), \n",
    "                     ('model', BernoulliNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['mood'], test_size=0.30, random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:\t0.9011406844106464\n",
      "Test accuracy:\t0.8596491228070176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickokwir/miniconda/envs/DeepLearning/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)\n",
    "train_accuracy = pipeline.score(X_train, y_train)\n",
    "test_accuracy = pipeline.score(X_test, y_test)\n",
    "\n",
    "print(f'Train accuracy:\\t{train_accuracy}')\n",
    "print(f'Test accuracy:\\t{test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions to text\n",
    "\n",
    "text = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the TF-IDF vectorizer\n",
    "# Keep words that appear in atleast 2 documents, keeps 5000 most common words\n",
    "preprocessor = TfidfVectorizer(min_df=2, tokenizer=nltk.word_tokenize, max_features=1000, ngram_range=(1,2), stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickokwir/miniconda/envs/DeepLearning/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# fit and transform data\n",
    "X = preprocessor.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.toarray(), df['mood'], test_size=0.20, random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:\t0.840531561461794\n",
      "Test accuracy:\t0.7368421052631579\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "train_accuracy = model.score(X_train, y_train)\n",
    "test_accuracy = model.score(X_test, y_test)\n",
    "\n",
    "print(f'Train accuracy:\\t{train_accuracy}')\n",
    "print(f'Test accuracy:\\t{test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter search using grid CV\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
