{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RNN**\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Overview`\\\n",
    "Choose a language model that will best represent the input text.\\\n",
    "Clean and prepare the data for training.\\\n",
    "Build a basic Keras sequential neural network model.\\\n",
    "Apply recurrent neural network (RNN) to process character sequences.\\\n",
    "Generate 3 channel RGB color outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.api._v2.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will start with the import of the required packages:\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.python.keras import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>red</th>\n",
       "      <th>green</th>\n",
       "      <th>blue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18th Century Green</td>\n",
       "      <td>165</td>\n",
       "      <td>147</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1975 Earth Red</td>\n",
       "      <td>123</td>\n",
       "      <td>70</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1989 Miami Hotline</td>\n",
       "      <td>221</td>\n",
       "      <td>51</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20000 Leagues Under the Sea</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3AM in Shibuya</td>\n",
       "      <td>34</td>\n",
       "      <td>85</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          name  red  green  blue\n",
       "0           18th Century Green  165    147    68\n",
       "1               1975 Earth Red  123     70    59\n",
       "2           1989 Miami Hotline  221     51   102\n",
       "3  20000 Leagues Under the Sea   25     25   112\n",
       "4               3AM in Shibuya   34     85   119"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"colors.csv\")\n",
    "names = data[\"name\"]\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Preparation**\n",
    "\n",
    "We mentioned that we’re limiting our color names to 25 characters. To arrive at this number, we checked the distribution of the length of color names across all training samples and visualize it to make sure the length limit we pick makes sense.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Rectangle.set() got an unexpected keyword argument 'normed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/patrickokwir/Desktop/Lighthouse-data-notes/Unit_9/Day_1/RNN.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/patrickokwir/Desktop/Lighthouse-data-notes/Unit_9/Day_1/RNN.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m fit \u001b[39m=\u001b[39m stats\u001b[39m.\u001b[39mnorm\u001b[39m.\u001b[39mpdf(h, np\u001b[39m.\u001b[39mmean(h), np\u001b[39m.\u001b[39mstd(h))  \u001b[39m#this is a fitting indeed\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/patrickokwir/Desktop/Lighthouse-data-notes/Unit_9/Day_1/RNN.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(h,fit,\u001b[39m'\u001b[39m\u001b[39m-o\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/patrickokwir/Desktop/Lighthouse-data-notes/Unit_9/Day_1/RNN.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m plt\u001b[39m.\u001b[39;49mhist(h,normed\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)      \u001b[39m#use this to draw histogram of your data\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/patrickokwir/Desktop/Lighthouse-data-notes/Unit_9/Day_1/RNN.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m'\u001b[39m\u001b[39mChars\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/patrickokwir/Desktop/Lighthouse-data-notes/Unit_9/Day_1/RNN.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m'\u001b[39m\u001b[39mProbability density\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda/envs/DeepLearning/lib/python3.9/site-packages/matplotlib/pyplot.py:2645\u001b[0m, in \u001b[0;36mhist\u001b[0;34m(x, bins, range, density, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, data, **kwargs)\u001b[0m\n\u001b[1;32m   2639\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mhist)\n\u001b[1;32m   2640\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhist\u001b[39m(\n\u001b[1;32m   2641\u001b[0m         x, bins\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39mrange\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, density\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, weights\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2642\u001b[0m         cumulative\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, bottom\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, histtype\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbar\u001b[39m\u001b[39m'\u001b[39m, align\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmid\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   2643\u001b[0m         orientation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mvertical\u001b[39m\u001b[39m'\u001b[39m, rwidth\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, log\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, color\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2644\u001b[0m         label\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, stacked\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 2645\u001b[0m     \u001b[39mreturn\u001b[39;00m gca()\u001b[39m.\u001b[39;49mhist(\n\u001b[1;32m   2646\u001b[0m         x, bins\u001b[39m=\u001b[39;49mbins, \u001b[39mrange\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mrange\u001b[39;49m, density\u001b[39m=\u001b[39;49mdensity, weights\u001b[39m=\u001b[39;49mweights,\n\u001b[1;32m   2647\u001b[0m         cumulative\u001b[39m=\u001b[39;49mcumulative, bottom\u001b[39m=\u001b[39;49mbottom, histtype\u001b[39m=\u001b[39;49mhisttype,\n\u001b[1;32m   2648\u001b[0m         align\u001b[39m=\u001b[39;49malign, orientation\u001b[39m=\u001b[39;49morientation, rwidth\u001b[39m=\u001b[39;49mrwidth, log\u001b[39m=\u001b[39;49mlog,\n\u001b[1;32m   2649\u001b[0m         color\u001b[39m=\u001b[39;49mcolor, label\u001b[39m=\u001b[39;49mlabel, stacked\u001b[39m=\u001b[39;49mstacked,\n\u001b[1;32m   2650\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m({\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m: data} \u001b[39mif\u001b[39;49;00m data \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m {}), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda/envs/DeepLearning/lib/python3.9/site-packages/matplotlib/__init__.py:1442\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1439\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m   1440\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(ax, \u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1441\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1442\u001b[0m         \u001b[39mreturn\u001b[39;00m func(ax, \u001b[39m*\u001b[39;49m\u001b[39mmap\u001b[39;49m(sanitize_sequence, args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1444\u001b[0m     bound \u001b[39m=\u001b[39m new_sig\u001b[39m.\u001b[39mbind(ax, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1445\u001b[0m     auto_label \u001b[39m=\u001b[39m (bound\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mget(label_namer)\n\u001b[1;32m   1446\u001b[0m                   \u001b[39mor\u001b[39;00m bound\u001b[39m.\u001b[39mkwargs\u001b[39m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m~/miniconda/envs/DeepLearning/lib/python3.9/site-packages/matplotlib/axes/_axes.py:6943\u001b[0m, in \u001b[0;36mAxes.hist\u001b[0;34m(self, x, bins, range, density, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, **kwargs)\u001b[0m\n\u001b[1;32m   6941\u001b[0m \u001b[39mif\u001b[39;00m patch:\n\u001b[1;32m   6942\u001b[0m     p \u001b[39m=\u001b[39m patch[\u001b[39m0\u001b[39m]\n\u001b[0;32m-> 6943\u001b[0m     p\u001b[39m.\u001b[39;49m_internal_update(kwargs)\n\u001b[1;32m   6944\u001b[0m     \u001b[39mif\u001b[39;00m lbl \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   6945\u001b[0m         p\u001b[39m.\u001b[39mset_label(lbl)\n",
      "File \u001b[0;32m~/miniconda/envs/DeepLearning/lib/python3.9/site-packages/matplotlib/artist.py:1223\u001b[0m, in \u001b[0;36mArtist._internal_update\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m   1216\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_internal_update\u001b[39m(\u001b[39mself\u001b[39m, kwargs):\n\u001b[1;32m   1217\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1218\u001b[0m \u001b[39m    Update artist properties without prenormalizing them, but generating\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m \u001b[39m    errors as if calling `set`.\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m \n\u001b[1;32m   1221\u001b[0m \u001b[39m    The lack of prenormalization is to maintain backcompatibility.\u001b[39;00m\n\u001b[1;32m   1222\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1223\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_props(\n\u001b[1;32m   1224\u001b[0m         kwargs, \u001b[39m\"\u001b[39;49m\u001b[39m{cls.__name__}\u001b[39;49;00m\u001b[39m.set() got an unexpected keyword argument \u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m   1225\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m{prop_name!r}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda/envs/DeepLearning/lib/python3.9/site-packages/matplotlib/artist.py:1197\u001b[0m, in \u001b[0;36mArtist._update_props\u001b[0;34m(self, props, errfmt)\u001b[0m\n\u001b[1;32m   1195\u001b[0m             func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mset_\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m   1196\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m callable(func):\n\u001b[0;32m-> 1197\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m   1198\u001b[0m                     errfmt\u001b[39m.\u001b[39mformat(\u001b[39mcls\u001b[39m\u001b[39m=\u001b[39m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m), prop_name\u001b[39m=\u001b[39mk))\n\u001b[1;32m   1199\u001b[0m             ret\u001b[39m.\u001b[39mappend(func(v))\n\u001b[1;32m   1200\u001b[0m \u001b[39mif\u001b[39;00m ret:\n",
      "\u001b[0;31mAttributeError\u001b[0m: Rectangle.set() got an unexpected keyword argument 'normed'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "h = sorted(names.str.len().values)\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pylab as plt\n",
    "\n",
    "fit = stats.norm.pdf(h, np.mean(h), np.std(h))  #this is a fitting indeed\n",
    "plt.plot(h,fit,'-o')\n",
    "plt.hist(h,normed=True)      #use this to draw histogram of your data\n",
    "plt.xlabel('Chars')\n",
    "plt.ylabel('Probability density')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`That gives us this plot, and you can clearly see that the majority of the color name strings have lengths less or equal to 25, even though the max length goes up to 30.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Tonenizer from keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "# import pad_sequences from keras\n",
    "from keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18606, 25)\n"
     ]
    }
   ],
   "source": [
    "maxlen = 25\n",
    "t = Tokenizer(char_level=True)\n",
    "t.fit_on_texts(names)\n",
    "tokenized = t.texts_to_sequences(names)\n",
    "padded_names = pad_sequences(tokenized, maxlen=maxlen)\n",
    "print(padded_names.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Right now, padded_names will have the shape of (18606, 25), with 18,606 is the number of total training samples and 25 being the max sequence length. If a string has less than 25 characters, it will be padded with the value 0 from the beginning of the sequence.`\n",
    "\n",
    "You might be thinking, all inputs are now in the form of integers, and our model should be able to process it. But there is one more step we can take to make later model training more effective."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **One-hot Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'e': 1, 'a': 2, ' ': 3, 'r': 4, 'i': 5, 'l': 6, 'n': 7, 'o': 8, 's': 9, 't': 10, 'c': 11, 'u': 12, 'd': 13, 'g': 14, 'p': 15, 'h': 16, 'b': 17, 'm': 18, 'y': 19, 'w': 20, 'k': 21, 'f': 22, 'v': 23, 'z': 24, 'j': 25, 'q': 26, 'x': 27, \"'\": 28, '-': 29, 'ō': 30, 'è': 31, 'é': 32, 'ā': 33, 'á': 34, 'ó': 35, 'ū': 36, '0': 37, '8': 38, '.': 39, 'ē': 40, 'ī': 41, 'ǎ': 42, '!': 43, 'í': 44, '&': 45, 'ǜ': 46, '9': 47, '2': 48, 'à': 49, 'ǐ': 50, '’': 51, '6': 52, 'ú': 53, '1': 54, '3': 55, 'â': 56, '4': 57, 'ǔ': 58, 'ì': 59, '7': 60, '5': 61, 'ê': 62, 'ö': 63, 'ł': 64, 'š': 65, 'ü': 66, '₂': 67, 'ò': 68, 'ñ': 69, 'ě': 70, 'ń': 71, 'ä': 72, 'œ': 73, 'ß': 74, '%': 75, 'ı': 76, 'ż': 77, '/': 78, 'î': 79, 'ë': 80, '(': 81, ')': 82, 'å': 83, '$': 84, 'я': 85, 'ő': 86, 'ğ': 87, 'ç': 88, 'ù': 89}\n"
     ]
    }
   ],
   "source": [
    "# We can view the character to integer mapping by inspecting the t.word_index property of the instance of Keras’ Tokenizer.\n",
    "print(t.word_index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`We can see that all of the most frequent characters (letters) are in the top 25. Therefore, this number should be sufficient.`\n",
    "\n",
    "`However, the integer values have no natural ordered relationship between each other and our model may not be able to harness any benefit from it. What’s worse, our model will initially assume such an ordering relationship among those characters (i.e. “a” is 2 and “e” is 1 but that should not signify a relationship), which can lead to an unwanted result. We will use one-hot encoding to represent the input sequence.`\n",
    "\n",
    "`Each integer will be represented by a boolean array where only one element in the array will have a value of 1. The max integer value will determine the length of the boolean array in the character dictionary.`\n",
    "\n",
    "`In our case, the max integer value is 'ù': 89, so the length of a one-hot boolean array will be 28 (considering the lowest value starts with 0, which is the padding).`\n",
    "\n",
    "`For example, instead of using the integer value 2 to represent character ‘a’, we’re going to use one-hot array [0, 0, 1, 0 …….. 0].`\n",
    "\n",
    "`One-hot encoding is also accessible in Keras.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.utils import np_utils\n",
    "one_hot_names = np_utils.to_categorical(padded_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18606, 25, 90)\n"
     ]
    }
   ],
   "source": [
    "print (one_hot_names.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`The resulting onehotnames has the shape (18606, 25, 90), which stands for (# of training samples, max sequence length, # of unique tokens).`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Normalization**\n",
    "\n",
    "`Remember we’re predicting 3 color channel values, each value ranging between 0–255. There is no golden rule for data normalization. Data normalization is purely practical because in practice it could take a model forever to converge if the training data values are spread out too much. A common normalization technique is to scale values to [-1, 1]. In our model, we’re using a ReLu activation function in the last layer. Since ReLu outputs non-negative numbers, we’ll normalize the values to [0, 1].`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The RGB values are between 0 - 255\n",
    "# scale them to be between 0 - 1\n",
    "def norm(value):\n",
    "    return value / 255.0\n",
    "\n",
    "normalized_values = np.column_stack([norm(data[\"red\"]), norm(data[\"green\"]), norm(data[\"blue\"])])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Building the Model**\n",
    "\n",
    "**To build our model we’re going to use two types of neural networks: a feed-forward neural network and a recurrent neural network. The feed-forward neural network is by far the most common type of neural network. In this neural network, the information comes into the input units and flows in one direction through hidden layers until each reaches the output units.**\n",
    "\n",
    "`In recurrent neural networks, information can flow around in cycles. These networks can remember information for a long time. Recurrent networks are a very natural way to model sequential data. In our specific model, we’re using one of the most powerful recurrent networks named long short term memory (LSTM).`\n",
    "\n",
    "**The easiest way to build a deep learning model in Keras is to use its sequential API, and we simply connect each of the neural network layers by calling its model.add() function like connecting LEGO bricks.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, return_sequences=True, input_shape=(maxlen, 90)))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 23:52:47.793367: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466/466 [==============================] - 16s 27ms/step - loss: 0.0685 - acc: 0.5761 - val_loss: 0.0643 - val_acc: 0.5975\n",
      "Epoch 2/40\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0636 - acc: 0.6212 - val_loss: 0.0611 - val_acc: 0.6174\n",
      "Epoch 3/40\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0609 - acc: 0.6393 - val_loss: 0.0607 - val_acc: 0.6037\n",
      "Epoch 4/40\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0583 - acc: 0.6344 - val_loss: 0.0593 - val_acc: 0.6096\n",
      "Epoch 5/40\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0559 - acc: 0.6377 - val_loss: 0.0567 - val_acc: 0.6265\n",
      "Epoch 6/40\n",
      "466/466 [==============================] - 11s 24ms/step - loss: 0.0537 - acc: 0.6429 - val_loss: 0.0554 - val_acc: 0.6255\n",
      "Epoch 7/40\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0516 - acc: 0.6433 - val_loss: 0.0554 - val_acc: 0.6161\n",
      "Epoch 8/40\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0496 - acc: 0.6495 - val_loss: 0.0565 - val_acc: 0.6274\n",
      "Epoch 9/40\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0474 - acc: 0.6484 - val_loss: 0.0560 - val_acc: 0.6214\n",
      "Epoch 10/40\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0452 - acc: 0.6561 - val_loss: 0.0564 - val_acc: 0.6128\n",
      "Epoch 11/40\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0424 - acc: 0.6631 - val_loss: 0.0574 - val_acc: 0.6171\n",
      "Epoch 12/40\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0398 - acc: 0.6604 - val_loss: 0.0578 - val_acc: 0.6037\n",
      "Epoch 13/40\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0367 - acc: 0.6748 - val_loss: 0.0593 - val_acc: 0.6016\n",
      "Epoch 14/40\n",
      "466/466 [==============================] - 11s 24ms/step - loss: 0.0334 - acc: 0.6760 - val_loss: 0.0611 - val_acc: 0.5854\n",
      "Epoch 15/40\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0297 - acc: 0.6819 - val_loss: 0.0630 - val_acc: 0.5943\n",
      "Epoch 16/40\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0262 - acc: 0.6930 - val_loss: 0.0639 - val_acc: 0.6059\n",
      "Epoch 17/40\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0229 - acc: 0.6996 - val_loss: 0.0658 - val_acc: 0.5830\n",
      "Epoch 18/40\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0196 - acc: 0.7052 - val_loss: 0.0657 - val_acc: 0.5817\n",
      "Epoch 19/40\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0169 - acc: 0.7139 - val_loss: 0.0671 - val_acc: 0.6018\n",
      "Epoch 20/40\n",
      "466/466 [==============================] - 11s 24ms/step - loss: 0.0146 - acc: 0.7244 - val_loss: 0.0681 - val_acc: 0.5836\n",
      "Epoch 21/40\n",
      "466/466 [==============================] - 11s 24ms/step - loss: 0.0129 - acc: 0.7303 - val_loss: 0.0688 - val_acc: 0.6037\n",
      "Epoch 22/40\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0113 - acc: 0.7400 - val_loss: 0.0705 - val_acc: 0.5905\n",
      "Epoch 23/40\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0099 - acc: 0.7454 - val_loss: 0.0705 - val_acc: 0.5841\n",
      "Epoch 24/40\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0088 - acc: 0.7565 - val_loss: 0.0729 - val_acc: 0.5752\n",
      "Epoch 25/40\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0079 - acc: 0.7671 - val_loss: 0.0717 - val_acc: 0.5766\n",
      "Epoch 26/40\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0072 - acc: 0.7741 - val_loss: 0.0709 - val_acc: 0.5704\n",
      "Epoch 27/40\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0065 - acc: 0.7779 - val_loss: 0.0703 - val_acc: 0.5750\n",
      "Epoch 28/40\n",
      "466/466 [==============================] - 11s 24ms/step - loss: 0.0060 - acc: 0.7853 - val_loss: 0.0711 - val_acc: 0.5645\n",
      "Epoch 29/40\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0058 - acc: 0.7930 - val_loss: 0.0710 - val_acc: 0.5707\n",
      "Epoch 30/40\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0054 - acc: 0.7968 - val_loss: 0.0720 - val_acc: 0.5742\n",
      "Epoch 31/40\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0050 - acc: 0.7999 - val_loss: 0.0718 - val_acc: 0.5881\n",
      "Epoch 32/40\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0047 - acc: 0.8037 - val_loss: 0.0709 - val_acc: 0.5763\n",
      "Epoch 33/40\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0043 - acc: 0.8139 - val_loss: 0.0702 - val_acc: 0.5642\n",
      "Epoch 34/40\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0042 - acc: 0.8195 - val_loss: 0.0709 - val_acc: 0.5806\n",
      "Epoch 35/40\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0038 - acc: 0.8216 - val_loss: 0.0699 - val_acc: 0.5774\n",
      "Epoch 36/40\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0038 - acc: 0.8230 - val_loss: 0.0705 - val_acc: 0.5567\n",
      "Epoch 37/40\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0037 - acc: 0.8291 - val_loss: 0.0702 - val_acc: 0.5728\n",
      "Epoch 38/40\n",
      "466/466 [==============================] - 11s 24ms/step - loss: 0.0038 - acc: 0.8278 - val_loss: 0.0712 - val_acc: 0.5725\n",
      "Epoch 39/40\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0035 - acc: 0.8293 - val_loss: 0.0685 - val_acc: 0.5776\n",
      "Epoch 40/40\n",
      "466/466 [==============================] - 11s 23ms/step - loss: 0.0032 - acc: 0.8390 - val_loss: 0.0700 - val_acc: 0.5758\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(one_hot_names, normalized_values,\n",
    "                    epochs=40,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Generate Colors**\n",
    "\n",
    "Let’s define some functions to generate and show the color predicted.\n",
    "\n",
    "`For a color name input, we need to transform it into the same one-hot representation. To achieve this, we tokenize characters to integers with the same tokenizer with which we processed the training data, pad it to the max sequence length of 25, then apply the one-hot encoding to the integer sequence.`\n",
    "\n",
    "And for the output RGB values, we need to scale it back to 0–255, so we can display them correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a color image\n",
    "def plot_rgb(rgb):\n",
    "    data = [[rgb]]\n",
    "    plt.figure(figsize=(2,2))\n",
    "    plt.imshow(data, interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "def scale(n):\n",
    "    return int(n * 255) \n",
    "\n",
    "def predict(name):\n",
    "    name = name.lower()\n",
    "    tokenized = t.texts_to_sequences([name])\n",
    "    padded = pad_sequences(tokenized, maxlen=maxlen)\n",
    "    one_hot = np_utils.to_categorical(padded, num_classes=90)\n",
    "    pred = model.predict(np.array(one_hot))[0]\n",
    "    r, g, b = scale(pred[0]), scale(pred[1]), scale(pred[2])\n",
    "    print(name + ',', 'R,G,B:', r,g,b)\n",
    "    plot_rgb(pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Let’s give the predict() function a try.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step\n",
      "forest, R,G,B: 24 75 24\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step\n",
      "ocean, R,G,B: 13 95 154\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict(\"forest\")\n",
    "predict(\"ocean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
