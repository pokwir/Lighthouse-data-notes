{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementing Word2Vec with Gensim Library in Python**\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we could summarize Wikipedia articles, we need to fetch them. To do so we will use a couple of libraries. The first library that we need to download is the Beautiful Soup library, which is a very useful Python utility for web scraping. Execute the following command at command prompt to download the Beautiful Soup utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "scrapped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Artificial_intelligence')\n",
    "article = scrapped_data .read()\n",
    "\n",
    "parsed_article = bs.BeautifulSoup(article,'lxml')\n",
    "\n",
    "paragraphs = parsed_article.find_all('p')\n",
    "\n",
    "article_text = \"\"\n",
    "\n",
    "for p in paragraphs:\n",
    "    article_text += p.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaing the text\n",
    "processed_article = article_text.lower()\n",
    "processed_article = re.sub('[^a-zA-Z]', ' ', processed_article )\n",
    "processed_article = re.sub(r'\\s+', ' ', processed_article)\n",
    "\n",
    "# Preparing the dataset\n",
    "all_sentences = nltk.sent_tokenize(processed_article)\n",
    "\n",
    "all_words = [nltk.word_tokenize(sent) for sent in all_sentences]\n",
    "\n",
    "# Removing Stop Words\n",
    "from nltk.corpus import stopwords\n",
    "for i in range(len(all_words)):\n",
    "    all_words[i] = [w for w in all_words[i] if w not in stopwords.words('english')]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`In the script above, we convert all the text to lowercase and then remove all the digits, special characters, and extra spaces from the text. After preprocessing, we are only left with the words.`\n",
    "\n",
    "`The Word2Vec model is trained on a collection of words. First, we need to convert our article into sentences. We use nltk.sent_tokenize utility to convert our article into sentences. To convert sentences into words, we use nltk.word_tokenize utility. As a last preprocessing step, we remove all the stop words from the text.`\n",
    "\n",
    "`After the script completes its execution, the all_words object contains the list of all the words in the article. We will use this list to create our Word2Vec model with the Gensim library.`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec(all_words, min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ai, intelligence, artificial, learning, human, machine, problems, many, networks, research, used, search, knowledge, use, also, neural, symbolic, may, researchers, systems, logic, field, computer, general, machines, problem, would, algorithms, reasoning, data, mind, intelligent, applications, tools, solve, could, humans, since, include, example, computing, specific, system, however, based, developed, ability, decision, mathematical, optimization, number, goals, information, one, approaches, two, well, level, including, recognition, world, risk, program, theory, difficult, agent, algorithm, even, using, deep, neurons, u, term, others, several, known, input, like, widely, make, sub, first, goal, rather, whether, language, fiction, form, inputs, related, issue, behavior, successful, particular, processing, perception, objects, techniques, methods, described, turing, new, question, approach, model, people, increase, solutions, tasks, robotics, called, time, formal, future, logics, natural, often, solving, long, works, large, published, popular, programs, bias, argues, software, become, facts, statistical, representation, classifiers, inspired, commonsense, examples, content, brain, vision, things, us, beings, processes, philosophy, considered, laws, patents, patent, science, know, technology, google, modern, among, thus, calculus, jobs, classification, creating, layers, outputs, argue, simulate, rights, understanding, planning, possible, ethics, spam, text, r, states, process, began, e, economics, simple, application, computational, g, advanced, russell, robots, step, easy, allow, making, learn, became, soft, generation, space, part, far, capable, years, go, high, technique, speech, feel, true, multi, lead, policy, risks, reduced, analysis, simulated, market, early, forms, issues, uses, antiquity, size, guess, able, especially, projects, choices, heuristics, solution, similar, events, accuracy, common, categories, marvin, included, designed, experience, k, patterns, domains, minsky, founder, classify, function, person, achieve, set, real, viewed, due, classifier, work, computers, digital, sufficiently, definition, logical, led, much, equity, followed, analyze, percent, ethical, class, terms, self, performance, neuron, training, together, thinking, different, created, superintelligence, layer, image, identify, fields, introduced, various, industry, internet, traffic, microsoft, still, highly, smart, previous, network, funding, defined, made, bayesian, sense, default, principles, prevent, finding, hard, life, identified, probability, late, facebook, typically, narrow, produce, connectionist, truth, need, area, friendly, simon, amazon, breadth, regression, better, requires, potential, web, around, fuzzy, improved, n, improve, morality, genetic, reason, areas, discovery, evolutionary, programming, swarm, express, statements, failed, ontologies, rarely, white, strategies, word, culture, increased, name, less, view, without, usage, middle, agents, concern, largest, within, companies, game, defendants, decisions, reported, ensure, animal, games, proposed, benchmarks, properties, relations, algorithmic, ontology, functions, philosophical, playing, questions, test, asimov, legal, century, concepts, st, champion, philosopher, gained, show, translation, experts, raised, determine, governments, capabilities, agree, filed, critics, services, pattern, devices, symbol, interaction, scientific, makes, likely, engineering, steps, something, driving, emerging, tend, moravec, trained, winter, match, recommendation, humanity, demonstrated, economists, statistics, central, path, matching, point, employment, unemployment, reach, development, social, believe, longer, position, subjective, study, public, total, move, stephen, consciousness, required, activation, philosophers, searching, considers, founded, consider, way, classifies, generally, design, superintelligent, idea, begin, chess, sources, relationship, require, another, norvig, directly, care, success, frequently, higher, gradient, commercial, might, computation, effect, academic, straightforward, displayed, twenty, later, next, predicted, recognize, inferring, simulating, agreed, remaining, criticism, writing, revived, done, opposed, british, substantially, difficulty, waves, newell, eventually, established, cars, deduction, act, alan, discussed, apek, karel, frankenstein, shelley, mary, alexa, storytelling, wide, range, siri, technological, netflix, redundancies, c, beneficial, existential, upon, assumption, suggested, precisely, scientists, explored, youtube, arguments, cybernetics, creative, engines, manyika, funded, theorems, databases, algebra, experienced, produced, leaders, prominence, decades, intellectual, period, phenomenon, languages, competing, emerged, james, connections, dominated, ways, sought, art, second, proved, helping, herbert, proponents, create, gofai, appeared, textbooks, reached, personal, becoming, significant, ibm, jeopardy, beat, blue, medical, deepfakes, thousands, gpt, face, apple, autonomous, mainstream, includes, professional, superhuman, deepmind, installed, inventions, four, papers, technologies, cities, control, professor, hours, security, inappropriate, images, detector, widespread, realism, hardware, cases, back, neighbor, perform, depends, scalability, bayes, naive, svm, nearest, weighted, classified, observation, certain, observations, therefore, actions, extremely, fire, rnn, short, creates, reduce, convolutional, important, relevant, perceptrons, recurrent, wire, signal, feedforward, competitive, descent, spectrum, continuous, refer, wrote, billion, thinks, misinformation, surveillance, provides, taking, worth, collar, automation, benefit, transhumanism, singularity, institute, centuries, robot, hypothetical, suffer, countries, researching, unfair, copyright, series, regulation, three, becomes, developing, jurisdictions, said, compas, openai, prominent, musk, shut, resources, hawking, searle, body, computationalism, pigeons, adopted, discussion, defines, simulation, mccarthy, john, exactly, conscious, define, must, matter, observe, measures, machinery, history, physical, red, stuart, learned, explain, right, assuming, named, peter, provably, necessary, neats, attempts, impossible, away, mistakes, presented, controllers, diamond, types, divided, dick, truly, domain, foundation, attempt, interpret, exists, intelligently, fast, exponentially, explosion, incomplete, uncertain, deductions, puzzles, knows, represented, description, concept, change, belongs, main, supervised, finds, j, fundamental, represent, support, retrieval, enormous, changing, told, assume, attention, received, expect, survive, access, although, mathematics, verifiable, david, interest, focused, started, brooks, rodney, lisp, beginning, fifth, japan, hungry, according, traits, agi, answer, think, concludes, lanier, scientist, institutions, involves, attributed, current, fully, concerned, numerous, incorporated, survey, learners, good, responses, searches, blind, came, supply, tree, result, sufficient, moving, landscape, means, target, find, inference, critical, features, random, top, architecture, dynamic, simplest, valuable, measure, models, markov, filtering, devised, guesses, belief, situation, handle, degree, order, ant, hopes, cognitive, ones, understand, useful, structure, translate, answering, texts, powerful, allows, appears, nlp, sample, complexity, assess, applied, strategy, occurrence, sensors, individual, otherwise, hans, sentiment, successes, actually, existing, users, programmed, signals, assistants, virtual, affective, visual, object, facial, average, "
     ]
    }
   ],
   "source": [
    "# Vocabulary using .key_to_index\n",
    "vocabulary = word2vec.wv.key_to_index\n",
    "\n",
    "for word in vocabulary:\n",
    "    print(word, end=\", \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Analysis\n",
    "We successfully created our Word2Vec model in the last section. Now is the time to explore what we created."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Vectors for a Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = word2vec.wv['artificial']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector `v1` contains the vector representation for the word \"artificial\". By default, a hundred dimensional vector is created by Gensim Word2Vec. This is a much, much smaller vector as compared to what would have been produced by bag of words. If we use the bag of words approach for embedding the article, the length of the vector for each will be 1206 since there are 1206 unique words with a minimum frequency of 2. If the minimum frequency of occurrence is set to 1, the size of the bag of words vector will further increase. On the other hand, vectors generated through Word2Vec are not affected by the size of the vocabulary."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Similar Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_words = word2vec.wv.most_similar('intelligence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('internet', 0.40523967146873474),\n",
       " ('ai', 0.34835532307624817),\n",
       " ('objects', 0.33982163667678833),\n",
       " ('researchers', 0.3379882872104645),\n",
       " ('difficult', 0.3354819416999817),\n",
       " ('algorithms', 0.33525350689888),\n",
       " ('science', 0.3136553466320038),\n",
       " ('goals', 0.3053082823753357),\n",
       " ('bias', 0.29881370067596436),\n",
       " ('may', 0.296377956867218)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
