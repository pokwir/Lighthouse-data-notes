{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Representations**\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the text Data with scikit-learn — Feature Extraction\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Bag of Words Model**\n",
    "\n",
    "` — Find the unique words i.e., vocabulary from the list of documents. Parse each document word with the vocabulary, if present ‘1’ else ‘0’. This makes each document vector maintain the same length that of vocabulary length. We use this vocabulary for the new document vectorization.`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary:  ['PHONE', 'IN', 'phone', 'SUPERB,', 'THIS', 'LOVE', 'I', 'hate', 'this', 'AM']\n",
      "vectors:  [[0, 0, 1, 0, 0, 0, 0, 0, 1, 0], [0, 0, 1, 0, 0, 0, 0, 1, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "# define a new document\n",
    "docs = [\"SUPERB, I AM IN LOVE IN THIS PHONE\", \"I hate this phone\"]\n",
    "\n",
    "words = list(set([word for doc in docs for word in doc.split()]))\n",
    "\n",
    "# create a dictionary\n",
    "vectors = []\n",
    "\n",
    "for doc in docs:\n",
    "    vectors.append([1 if word in doc.lower().split() else 0 for word in words])\n",
    "print(\"vocabulary: \", words)   \n",
    "print(\"vectors: \", vectors)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Word Counts with CountVectorizer(scikit-learn)**\n",
    "\n",
    " `— Tokenize the collection of documents and form a vocabulary with it and use this vocabulary to encode new documents. We can use CountVectorizer of the scikit-learn library. It by default remove punctuation and lower the documents.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary:  {'superb': 5, 'am': 0, 'in': 2, 'love': 3, 'this': 6, 'phone': 4, 'hate': 1}\n",
      "shape:  (2, 7)\n",
      "vectors:  [[1 0 2 1 1 1 1]\n",
      " [0 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of documents\n",
    "docs = ['SUPERB, I AM IN LOVE IN THIS PHONE', 'I hate this phone']\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(docs)\n",
    "print('vocabulary: ', vectorizer.vocabulary_)\n",
    "# encode document\n",
    "vector = vectorizer.transform(docs)\n",
    "# summarize encoded vector\n",
    "print('shape: ', vector.shape)\n",
    "print('vectors: ', vector.toarray())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Word Frequencies with TfidfVectorizer (scikit-learn)**\n",
    "\n",
    "` — Word counts are pretty basic. In the first document, the word “in” has repeated and with that word we can’t draw any meaning. Stop words can repeat several times in a document and word count prioritize with the occurrence of the word. From word counts, we lose the interesting words and we mostly give priority to stopping words/less meaning carrying words.`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF is a popular method. Acronym is “Term Frequency and Inverse Document Frequency”. TF-IDF is word frequency scores that try to highlight words that are more interesting, e.g. frequent in a document but not across documents.\n",
    "\n",
    "There are a few types of weighting schemes for tf-idf in general. Let's see how scikit-learn calculates tf*idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary:  {'superb': 5, 'am': 0, 'in': 2, 'love': 3, 'this': 6, 'phone': 4, 'hate': 1}\n",
      "idfs:  [1.40546511 1.40546511 1.40546511 1.40546511 1.         1.40546511\n",
      " 1.        ]\n",
      "[[0.35327777 0.         0.70655553 0.35327777 0.25136004 0.35327777\n",
      "  0.25136004]\n",
      " [0.         0.70490949 0.         0.         0.50154891 0.\n",
      "  0.50154891]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# list of documents\n",
    "docs = ['SUPERB, I AM IN LOVE IN THIS PHONE', 'I hate this phone']\n",
    "\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(docs)\n",
    "\n",
    "# summarize encoded vector\n",
    "print('vocabulary: ', vectorizer.vocabulary_)\n",
    "print('idfs: ', vectorizer.idf_)\n",
    "\n",
    "# encode document\n",
    "vector = vectorizer.transform(docs)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
