{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/Users/patrickokwir/Desktop/Lighthouse-data-notes/Test/Process.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 81>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/patrickokwir/Desktop/Lighthouse-data-notes/Test/Process.ipynb#W0sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrmsprop\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/patrickokwir/Desktop/Lighthouse-data-notes/Test/Process.ipynb#W0sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/patrickokwir/Desktop/Lighthouse-data-notes/Test/Process.ipynb#W0sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/patrickokwir/Desktop/Lighthouse-data-notes/Test/Process.ipynb#W0sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m     [encoder_input_data, decoder_input_data],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/patrickokwir/Desktop/Lighthouse-data-notes/Test/Process.ipynb#W0sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m     decoder_target_data,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/patrickokwir/Desktop/Lighthouse-data-notes/Test/Process.ipynb#W0sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,  \u001b[39m# Try reducing the batch size\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/patrickokwir/Desktop/Lighthouse-data-notes/Test/Process.ipynb#W0sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/patrickokwir/Desktop/Lighthouse-data-notes/Test/Process.ipynb#W0sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m     validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/patrickokwir/Desktop/Lighthouse-data-notes/Test/Process.ipynb#W0sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/patrickokwir/Desktop/Lighthouse-data-notes/Test/Process.ipynb#W0sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m \u001b[39m# Save the trained model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/patrickokwir/Desktop/Lighthouse-data-notes/Test/Process.ipynb#W0sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m model\u001b[39m.\u001b[39msave(\u001b[39m'\u001b[39m\u001b[39mthe_office_chatbot_model.h5\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda/envs/DeepLearning/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda/envs/DeepLearning/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m    102\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, TimeDistributed, Attention\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" \n",
    "\n",
    "\n",
    "# Read the Twitter dataset from 'twitter_data.txt' file\n",
    "data_file = 'schrute.txt'\n",
    "with open(data_file, 'r', encoding='utf-8') as f:\n",
    "    tweet_lines = f.readlines()\n",
    "\n",
    "# Split the dataset into input and target texts\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "for tweet in tweet_lines:\n",
    "    input_texts.append(tweet)\n",
    "    target_texts.append('\\t' + tweet + '\\n')\n",
    "\n",
    "# Define the input and target characters based on your dataset\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "for input_text, target_text in zip(input_texts, target_texts):\n",
    "    input_characters.update(input_text)\n",
    "    target_characters.update(target_text)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "\n",
    "# Rest of the code remains the same as before (tokenization, vectorization, model definition, training)\n",
    "\n",
    "# Creating dictionaries to map characters to integers\n",
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "max_encoder_seq_length = max([len(text) for text in input_texts])\n",
    "max_decoder_seq_length = max([len(text) for text in target_texts])\n",
    "\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "\n",
    "# Creating empty arrays to store encoder and decoder data\n",
    "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
    "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "\n",
    "# Vectorizing the input and target texts\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
    "        if t > 0:\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
    "\n",
    "# Define the model architecture with attention\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=[state_h, state_c])\n",
    "\n",
    "attention = Attention()\n",
    "attention_outputs = attention([decoder_outputs, encoder_outputs])\n",
    "\n",
    "decoder_concat_input = keras.layers.concatenate([decoder_outputs, attention_outputs])\n",
    "decoder_dense = TimeDistributed(Dense(num_decoder_tokens, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    [encoder_input_data, decoder_input_data],\n",
    "    decoder_target_data,\n",
    "    batch_size=32,  # Try reducing the batch size\n",
    "    epochs=50,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save('the_office_chatbot_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
