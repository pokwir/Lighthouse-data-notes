{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Duplicate Questions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over 100 million people visit Quora every month, so it's no surprise that many people ask similar (or the same) questions. Various questions with the same intent can cause people to spend extra time searching for the best answer to their question, and results in members answering multiple versions of the same question. Quora uses random forest to identify duplicated questions to provide a better experience to active seekers and writers, and offer more value to both of these groups in the long term.\n",
    "Follow the steps outlined below to build the appropriate classifier model. \n",
    "\n",
    "\n",
    "Steps:\n",
    "- Download data\n",
    "- Exploration\n",
    "- Cleaning\n",
    "- Feature Engineering\n",
    "- Modeling\n",
    "\n",
    "By the end of this project you should have **a presentation that describes the model you built** and its **performance**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "There is no designated test.csv file. The train.csv file is the entire dataset. Part of the data in the train.csv file should be set aside to act as the final testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exploration and Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div id=\"atARxG\"></div>\n",
       "            <script type=\"text/javascript\" data-lets-plot-script=\"library\">\n",
       "                if(!window.letsPlotCallQueue) {\n",
       "                    window.letsPlotCallQueue = [];\n",
       "                }; \n",
       "                window.letsPlotCall = function(f) {\n",
       "                    window.letsPlotCallQueue.push(f);\n",
       "                };\n",
       "                (function() {\n",
       "                    var script = document.createElement(\"script\");\n",
       "                    script.type = \"text/javascript\";\n",
       "                    script.src = \"https://cdn.jsdelivr.net/gh/JetBrains/lets-plot@v3.2.0/js-package/distr/lets-plot.min.js\";\n",
       "                    script.onload = function() {\n",
       "                        window.letsPlotCall = function(f) {f();};\n",
       "                        window.letsPlotCallQueue.forEach(function(f) {f();});\n",
       "                        window.letsPlotCallQueue = [];\n",
       "                        \n",
       "                    };\n",
       "                    script.onerror = function(event) {\n",
       "                        window.letsPlotCall = function(f) {};    // noop\n",
       "                        window.letsPlotCallQueue = [];\n",
       "                        var div = document.createElement(\"div\");\n",
       "                        div.style.color = 'darkred';\n",
       "                        div.textContent = 'Error loading Lets-Plot JS';\n",
       "                        document.getElementById(\"atARxG\").appendChild(div);\n",
       "                    };\n",
       "                    var e = document.getElementById(\"atARxG\");\n",
       "                    e.appendChild(script);\n",
       "                })()\n",
       "            </script>\n",
       "            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import lets plot\n",
    "import numpy as np\n",
    "from lets_plot import *\n",
    "LetsPlot.setup_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404290, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How big is this dataset?\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    255027\n",
       "1    149263\n",
       "Name: is_duplicate, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What portion of our questions are actually duplicate?\n",
    "df['is_duplicate'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "   <div id=\"MNlV0E\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
       "       (function() {\n",
       "           var plotSpec={\n",
       "\"data\":{\n",
       "},\n",
       "\"mapping\":{\n",
       "\"x\":\"is_duplicate\",\n",
       "\"fill\":\"is_duplicate\"\n",
       "},\n",
       "\"data_meta\":{\n",
       "},\n",
       "\"ggtitle\":{\n",
       "\"text\":\" \"\n",
       "},\n",
       "\"theme\":{\n",
       "\"name\":\"classic\",\n",
       "\"flavor\":\"high_contrast_dark\"\n",
       "},\n",
       "\"kind\":\"plot\",\n",
       "\"scales\":[{\n",
       "\"name\":\"Class\",\n",
       "\"aesthetic\":\"x\"\n",
       "},{\n",
       "\"name\":\"Count\",\n",
       "\"aesthetic\":\"y\"\n",
       "},{\n",
       "\"aesthetic\":\"fill\",\n",
       "\"guide\":\"none\",\n",
       "\"discrete\":true\n",
       "}],\n",
       "\"layers\":[{\n",
       "\"geom\":\"bar\",\n",
       "\"mapping\":{\n",
       "},\n",
       "\"data_meta\":{\n",
       "},\n",
       "\"data\":{\n",
       "\"..count..\":[255027.0,149263.0],\n",
       "\"is_duplicate\":[0.0,1.0]\n",
       "}\n",
       "}],\n",
       "\"metainfo_list\":[]\n",
       "};\n",
       "           var plotContainer = document.getElementById(\"MNlV0E\");\n",
       "           window.letsPlotCall(function() {{\n",
       "               LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);\n",
       "           }});\n",
       "       })();    \n",
       "   </script>"
      ],
      "text/plain": [
       "<lets_plot.plot.core.PlotSpec at 0x172bf29a0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot class distribution for each class using ggplot2\n",
    "ggplot(df, aes(x='is_duplicate', fill = 'is_duplicate')) + geom_bar() + ggtitle(\" \") + labs(x=\"Class\", y=\"Count\") +\\\n",
    "scale_fill_discrete(guide='none') + \\\n",
    "theme_classic() + \\\n",
    "flavor_high_contrast_dark() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nulls in label: 0\n",
      "Number of nulls in text: 1\n",
      "Number of nulls in text: 2\n"
     ]
    }
   ],
   "source": [
    "# Are we missing any data?\n",
    "print('Number of nulls in label: {}'.format(df['is_duplicate'].isnull().sum()))\n",
    "print('Number of nulls in text: {}'.format(df['question1'].isnull().sum()))\n",
    "print('Number of nulls in text: {}'.format(df['question2'].isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of question pairs for training: 404290\n"
     ]
    }
   ],
   "source": [
    "# How many unique questions are there?\n",
    "print('Total number of question pairs for training: {}'.format(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many times a question appears in the df\n",
    "qids = pd.Series(df[df['qid1'].notnull()]['qid1'].tolist() + df[df['qid2'].notnull()]['qid2'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "   <div id=\"xjjXb7\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
       "       (function() {\n",
       "           var plotSpec={\n",
       "\"data\":{\n",
       "},\n",
       "\"mapping\":{\n",
       "\"x\":\"x\"\n",
       "},\n",
       "\"data_meta\":{\n",
       "},\n",
       "\"ggtitle\":{\n",
       "\"text\":\" \"\n",
       "},\n",
       "\"kind\":\"plot\",\n",
       "\"scales\":[{\n",
       "\"name\":\"Question ID\",\n",
       "\"aesthetic\":\"x\"\n",
       "},{\n",
       "\"name\":\"Count\",\n",
       "\"aesthetic\":\"y\"\n",
       "}],\n",
       "\"layers\":[{\n",
       "\"geom\":\"density\",\n",
       "\"mapping\":{\n",
       "},\n",
       "\"data_meta\":{\n",
       "},\n",
       "\"binwidth\":10.0,\n",
       "\"method\":\"histodot\",\n",
       "\"data\":{\n",
       "\"..density..\":[2.1492823508388794E-6,2.3311515735926243E-6,2.5083518806303457E-6,2.6784754540070888E-6,2.839336854995643E-6,2.9890499239927307E-6,3.126086633498013E-6,3.2493159768550513E-6,3.358020535116754E-6,3.4518915970981926E-6,3.531004255301969E-6,3.595776045659485E-6,3.6469130748370917E-6,3.685348161883932E-6,3.7121759786297543E-6,3.7285892725529383E-6,3.7358204211398495E-6,3.735090469255272E-6,3.7275681220184593E-6,3.7143393552880898E-6,3.6963874959339457E-6,3.6745830820776634E-6,3.649682070685421E-6,3.622330819673608E-6,3.5930757916137185E-6,3.562376151540749E-6,3.5306179351364948E-6,3.498127828831717E-6,3.465186151718636E-6,3.432037819786293E-6,3.3989009283878307E-6,3.3659731314595023E-6,3.3334355087134903E-6,3.301454247746231E-6,3.2701809398501784E-6,3.239750956412565E-6,3.2102814727382126E-6,3.1818689720000433E-6,3.154586508271918E-6,3.1284815959200684E-6,3.103574743682197E-6,3.0798588063164822E-6,3.0572994408001145E-6,3.0358367493176903E-6,3.0153878359581107E-6,2.9958502772552016E-6,2.9771068727963666E-6,2.959031281607552E-6,2.9414919365038585E-6,2.9243580739649986E-6,2.9075053169864087E-6,2.8908190634026326E-6,2.874199478825158E-6,2.857563767487338E-6,2.8408480520179616E-6,2.8240083387394847E-6,2.8070206114849644E-6,2.7898795048051386E-6,2.7725964028870733E-6,2.7551976828664543E-6,2.7377218155302516E-6,2.720216129983086E-6,2.702734200044155E-6,2.6853331716229073E-6,2.6680708956608673E-6,2.651003575452782E-6,2.6341839207630274E-6,2.617659809435598E-6,2.6014725456058164E-6,2.5856565712157132E-6,2.570238643245615E-6,2.5552379847672156E-6,2.540666245268486E-6,2.526528225213702E-6,2.512822498015168E-6,2.512822498015168E-6,2.499542394715484E-6,2.486676677883166E-6,2.4742108313789833E-6,2.4621281447287212E-6,2.450410274433089E-6,2.4390380446070073E-6,2.427992014555268E-6,2.41725239573111E-6,2.406799298305458E-6,2.3966126487767807E-6,2.3866715443980403E-6,2.376954366037888E-6,2.3674381922660095E-6,2.358098812089009E-6,2.348910818106669E-6,2.3398476892400797E-6,2.3308824320939662E-6,2.3219881263303012E-6,2.3131387845821636E-6,2.3043101280911077E-6,2.295480473165759E-6,2.286631552593728E-6,2.2777488972071856E-6,2.268822445605075E-6,2.259846662493793E-6,2.2508204428703627E-6,2.241747061018746E-6,2.2326334902611324E-6,2.2234902350678827E-6,2.2143307064631935E-6,2.2051705435197246E-6,2.196027087162313E-6,2.186919152732617E-6,2.177866185711606E-6,2.1688881493972887E-6,2.160004880826989E-6,2.151235698385379E-6,2.142599024940476E-6,2.1341117242435506E-6,2.125788734675707E-6,2.117642369091058E-6,2.1096819202926765E-6,2.1019131110962708E-6,2.0943377599615068E-6,2.0869537887353976E-6,2.0797551454428917E-6,2.0727323245018967E-6,2.0658729692162264E-6,2.0591625964187853E-6,2.0525860183836716E-6,2.046128356433338E-6,2.039776303460615E-6,2.0335196020549414E-6,2.027351873202286E-6,2.0212715094720675E-6,2.0152820836900366E-6,2.0093924726484583E-6,2.0036163721680825E-6,1.9979716582081826E-6,1.992479222670091E-6,1.9871615813850024E-6,1.9820413525065066E-6,1.977139722580483E-6,1.9724749412313215E-6,1.968061088450519E-6,1.9639070764022883E-6,1.960015835041186E-6,1.956384079260903E-6,1.95300239724557E-6,1.9498554711961746E-6,1.9469226269659814E-6,1.9441786756049456E-6,1.9415948451503894E-6,1.9391395472381024E-6,1.9367793112659263E-6,1.9344797677102327E-6,1.932206344310143E-6,1.9299249577757887E-6,1.9276027159568383E-6,1.925208615591845E-6,1.9227140144570404E-6,1.92009334804017E-6,1.917324561747582E-6,1.914389789642805E-6,1.9112757570577743E-6,1.9079743093514586E-6,1.9044827438937097E-6,1.900803860857044E-6,1.8969460204152652E-6,1.8929228367960678E-6,1.88875279129933E-6,1.8844583848183123E-6,1.8800652740883953E-6,1.8756011306189805E-6,1.8710944294411582E-6,1.866573118033846E-6,1.8620634885105229E-6,1.8575888991647059E-6,1.8531689118784482E-6,1.8488185661113012E-6,1.8445478941028626E-6,1.8403618542714297E-6,1.8362605438587235E-6,1.8322395820403565E-6,1.8282908582894557E-6,1.8244033477134214E-6,1.8205641257562426E-6,1.816759179392654E-6,1.8129744868652677E-6,1.8091967037376208E-6,1.8054137467252597E-6,1.8016153874003584E-6,1.7977934493180797E-6,1.7977934493180797E-6,1.7939421035575629E-6,1.7900578049289155E-6,1.7861394159303777E-6,1.7821881376950531E-6,1.7782075365131406E-6,1.7742034797999933E-6,1.7701841022491748E-6,1.766159828577447E-6,1.762143174504579E-6,1.7581485097382666E-6,1.7541916856241863E-6,1.750289519872051E-6,1.7464590619724095E-6,1.7427167630624234E-6,1.7390775451626172E-6,1.7355539732024122E-6,1.7321553457465757E-6,1.7288871254925228E-6,1.7257505692674725E-6,1.7227426177287147E-6,1.7198561589053868E-6,1.717080622522438E-6,1.714402808341842E-6,1.7118079073538443E-6,1.7092806809751713E-6,1.706806464665722E-6,1.7043720881948433E-6,1.701966607582613E-6,1.699581597585944E-6,1.697211235135889E-6,1.6948520631765449E-6,1.692502295307138E-6,1.6901611331922846E-6,1.6878279351937122E-6,1.6855013220499194E-6,1.6831785234606215E-6,1.680854804994683E-6,1.6785233522590124E-6,1.6761752769239304E-6,1.6738000571573263E-6,1.6713861675887537E-6,1.6689219315588432E-6,1.6663965051007672E-6,1.6638008443917544E-6,1.6611285626210968E-6,1.6583767601030876E-6,1.6555464324674596E-6,1.6526427502866778E-6,1.6496749481515336E-6,1.6466559744125997E-6,1.6436018857069201E-6,1.640530963878433E-6,1.6374628135713105E-6,1.6344172890459002E-6,1.6314135013126167E-6,1.6284688758116643E-6,1.6255983322862027E-6,1.6228136714543625E-6,1.6201231980171391E-6,1.6175314737050858E-6,1.6150393463830027E-6,1.612644112937929E-6,1.6103398616244568E-6,1.6081179649774314E-6,1.605967560575857E-6,1.603876155159765E-6,1.6018301737956648E-6,1.599815580426572E-6,1.5978183243938353E-6,1.595824938016587E-6,1.5938228645687346E-6,1.5918009329679744E-6,1.589749536593244E-6,1.5876610170273878E-6,1.5855297219148358E-6,1.5833522356651902E-6,1.5811273305822365E-6,1.5788560019337289E-6,1.576541345692019E-6,1.5741884328222217E-6,1.5718040443695854E-6,1.5693964080055386E-6,1.5669748502549635E-6,1.5645493906292187E-6,1.5621303752086236E-6,1.559728029325003E-6,1.5573520730509071E-6,1.5550113308978146E-6,1.5527134184937882E-6,1.5504644092847665E-6,1.5482687510557796E-6,1.546129076975049E-6,1.5440462271052223E-6,1.542019331019389E-6,1.540045952314938E-6,1.5381223845089264E-6,1.5362438350548506E-6,1.53440482948942E-6,1.5325995082513471E-6,1.5308219492215194E-6,1.5290664679371919E-6,1.5273278496739167E-6,1.5256015432861293E-6,1.5238837293804805E-6,1.5221713952429192E-6,1.520462318052899E-6,1.5187549489989445E-6,1.5170483082320306E-6,1.5153418159079622E-6,1.5136351130350843E-6,1.5119279001287108E-6,1.5102197340197945E-6,1.508509968515476E-6,1.5067975469369886E-6,1.505081069998989E-6,1.5033587137693201E-6,1.5016283433327928E-6,1.499887583473745E-6,1.4981340081863443E-6,1.496365259829679E-6,1.4945792869502444E-6,1.492774541652802E-6,1.4909501534208902E-6,1.4891061026526984E-6,1.4872433363419705E-6,1.4853638659845313E-6,1.4834707660583867E-6,1.4815681198769434E-6,1.4796609292787613E-6,1.4777549323359085E-6,1.475856364893083E-6,1.4739717091184846E-6,1.4721073205947663E-6,1.47026914889671E-6,1.4684623496715503E-6,1.4666910034257206E-6,1.46495778843651E-6,1.4632637739247455E-6,1.461608282573428E-6,1.4599888175286055E-6,1.4584011216019493E-6,1.4568393775260854E-6,1.4552964646966636E-6,1.4537644043443354E-6,1.4522347919991566E-6,1.4522347919991566E-6,1.4506993832179548E-6,1.4491506629891157E-6,1.4475823913159044E-6,1.4459901135084633E-6,1.4443715193233635E-6,1.442726736163438E-6,1.4410583983598534E-6,1.439371541768672E-6,1.4376733069531087E-6,1.4359725383816864E-6,1.4342790854742608E-6,1.4326031856411528E-6,1.4309546405299192E-6,1.4293421266250654E-6,1.427772508981643E-6,1.4262503027192957E-6,1.4247773262474304E-6,1.423352527501451E-6,1.4219720886436085E-6,1.4206296598401367E-6,1.4193168990779439E-6,1.4180240794694178E-6,1.4167408263406858E-6,1.4154569220223022E-6,1.4141630063733099E-6,1.4128513408969574E-6,1.4115162858950718E-6,1.4101546885031685E-6,1.4087660886476568E-6,1.4073526552381676E-6,1.4059190487932936E-6,1.4044719836678285E-6,1.4030197739063528E-6,1.4015717674443635E-6,1.40013771840394E-6,1.3987271958470367E-6,1.3973490286289068E-6,1.3960108515282215E-6,1.394718733472018E-6,1.3934769104921807E-6,1.3922877386445692E-6,1.3911515855426065E-6,1.390066983463458E-6,1.3890307551062116E-6,1.388038269890671E-6,1.387083641885334E-6,1.3861600260231748E-6,1.3852598628866658E-6,1.384375104139943E-6,1.3834973847303275E-6,1.3826182246701167E-6,1.3817291766524326E-6,1.3808219015702483E-6,1.3798883376824592E-6,1.3789208255759037E-6,1.3779122373318914E-6,1.3768561063156472E-6,1.375746860425873E-6,1.374579980049208E-6,1.3733521983237261E-6,1.3720617225116554E-6,1.3707083547054246E-6,1.3692936421872987E-6,1.3678209337637892E-6,1.3662953595866814E-6,1.3647237338089453E-6,1.3631143871628388E-6,1.3614768393979382E-6,1.3598215191778582E-6,1.3581593121387302E-6,1.3565011362947558E-6,1.3548574886823974E-6,1.3532379883469332E-6,1.3516510197110537E-6,1.350103377670493E-6,1.3486000096260355E-6,1.3471439059875082E-6,1.3457360441106894E-6,1.3443754256850496E-6,1.343059238187059E-6,1.3417830927157798E-6,1.3405413168526599E-6,1.3393272965256343E-6,1.3381338058555315E-6,1.3369533964575918E-6,1.3357787150580478E-6,1.3346028140123819E-6,1.3334194112084095E-6,1.3322231259649832E-6,1.3310095941924892E-6,1.329775605707876E-6,1.3285191183660177E-6,1.3272392624725818E-6,1.3259362953484996E-6,1.3246114962122516E-6,1.32326704311809E-6,1.3219058637961307E-6,1.3205314846420943E-6,1.3191478617724444E-6,1.3177592137547863E-6,1.316369886683292E-6,1.3149842055971037E-6,1.3136063643631017E-6,1.3122403347544297E-6,1.3108897988758614E-6,1.309558080701893E-6,1.308248123603568E-6,1.3069624551534256E-6,1.3057031791410916E-6,1.3044719709801435E-6,1.3032700580600276E-6,1.3020982571769807E-6,1.3009569616265125E-6,1.2998461528010415E-6,1.2987654244409991E-6,1.2977140010638717E-6,1.2966907669658778E-6,1.295694273348854E-6,1.2947227832937866E-6,1.29377427180091E-6,1.2928464710151338E-6,1.2919368788506043E-6,1.2910427818208667E-6,1.2901612831267902E-6,1.2892893456168955E-6,1.2884238171719338E-6,1.2875614998781955E-6,1.2866991958704647E-6,1.2858337888797892E-6,1.2849623087779604E-6,1.2840820138090687E-6,1.2831904628287785E-6,1.282285579498728E-6,1.2813656988041299E-6,1.280429425718577E-6,1.2794759363778906E-6,1.2785048516113738E-6,1.2775160703045633E-6,1.2765097485118575E-6,1.2754861800021334E-6,1.2744456292626997E-6,1.2733881486518968E-6,1.2723132978710494E-6,1.271219824421392E-6,1.2701052346513254E-6,1.2689652624742885E-6,1.267793130895833E-6,1.2665786725028996E-6,1.2653071688239457E-6,1.2639579170226912E-6,1.262502519756656E-6,1.26090278079273E-6,1.2591083333277842E-6,1.2570539486973332E-6,1.2546566564516017E-6,1.2518127763054977E-6,1.24839508585661E-6,1.244250333153454E-6,1.2391974356970434E-6,1.2330266979027762E-6,1.2255004054212473E-6,1.2163552000441002E-6,1.2053065054608415E-6,1.192055251363679E-6,1.1762970224661234E-6,1.1577334692108323E-6,1.1360857898835774E-6,1.1111096889876303E-6,1.0826111302601648E-6,1.0504619096156485E-6,1.014614052479971E-6,9.751118388269332E-7,9.321004463524777E-7,8.858302038530943E-7,8.366557909917043E-7,7.850300056640776E-7,7.314920784421102E-7,6.766510165084457E-7,6.211647797574757E-7],\n",
       "\"..quantile..\":[0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0],\n",
       "\"x\":[1.0,1053.7045009784736,2106.409001956947,3159.113502935421,4211.818003913894,5264.522504892368,6317.227005870842,7369.931506849315,8422.636007827788,9475.340508806263,10528.045009784735,11580.74951076321,12633.454011741684,13686.158512720156,14738.86301369863,15791.567514677103,16844.272015655577,17896.97651663405,18949.681017612525,20002.385518590996,21055.09001956947,22107.794520547945,23160.49902152642,24213.203522504893,25265.908023483367,26318.612524461838,27371.317025440312,28424.021526418786,29476.72602739726,30529.430528375735,31582.135029354205,32634.83953033268,33687.544031311154,34740.248532289625,35792.9530332681,36845.65753424657,37898.36203522505,38951.06653620352,40003.77103718199,41056.47553816047,42109.18003913894,43161.88454011742,44214.58904109589,45267.29354207436,46319.99804305284,47372.70254403131,48425.407045009786,49478.11154598826,50530.816046966735,51583.520547945205,52636.225048923676,53688.929549902154,54741.634050880624,55794.3385518591,56847.04305283757,57899.74755381604,58952.45205479452,60005.15655577299,61057.86105675147,62110.56555772994,63163.27005870841,64215.97455968689,65268.67906066536,66321.38356164383,67374.08806262231,68426.79256360079,69479.49706457925,70532.20156555773,71584.9060665362,72637.61056751468,73690.31506849315,74743.01956947162,75795.7240704501,76848.42857142857,77901.13307240704,77901.13307240704,78953.83757338552,80006.54207436398,81059.24657534246,82111.95107632094,83164.65557729942,84217.36007827788,85270.06457925636,86322.76908023484,87375.4735812133,88428.17808219178,89480.88258317026,90533.58708414872,91586.2915851272,92638.99608610568,93691.70058708415,94744.40508806262,95797.1095890411,96849.81409001957,97902.51859099804,98955.22309197651,100007.92759295499,101060.63209393347,102113.33659491193,103166.04109589041,104218.74559686889,105271.45009784735,106324.15459882583,107376.85909980431,108429.56360078277,109482.26810176125,110534.97260273973,111587.6771037182,112640.38160469667,113693.08610567515,114745.79060665362,115798.49510763209,116851.19960861056,117903.90410958904,118956.6086105675,120009.31311154598,121062.01761252446,122114.72211350294,123167.4266144814,124220.13111545988,125272.83561643836,126325.54011741682,127378.2446183953,128430.94911937378,129483.65362035224,130536.35812133072,131589.0626223092,132641.76712328766,133694.47162426615,134747.17612524462,135799.88062622308,136852.58512720157,137905.28962818003,138957.9941291585,140010.698630137,141063.40313111545,142116.10763209392,143168.8121330724,144221.51663405087,145274.22113502937,146326.92563600783,147379.6301369863,148432.33463796478,149485.03913894325,150537.7436399217,151590.4481409002,152643.15264187867,153695.85714285713,154748.56164383562,155801.2661448141,156853.97064579255,157906.67514677104,158959.3796477495,160012.08414872797,161064.78864970646,162117.49315068492,163170.1976516634,164222.90215264188,165275.60665362034,166328.31115459884,167381.0156555773,168433.72015655576,169486.42465753425,170539.12915851272,171591.83365949118,172644.53816046967,173697.24266144814,174749.9471624266,175802.6516634051,176855.35616438356,177908.06066536202,178960.7651663405,180013.46966731898,181066.17416829744,182118.87866927593,183171.5831702544,184224.2876712329,185276.99217221135,186329.6966731898,187382.4011741683,188435.10567514677,189487.81017612523,190540.51467710373,191593.2191780822,192645.92367906065,193698.62818003914,194751.3326810176,195804.03718199607,196856.74168297456,196856.74168297456,197909.44618395303,198962.1506849315,200014.85518590998,201067.55968688845,202120.26418786694,203172.9686888454,204225.67318982386,205278.37769080236,206331.08219178082,207383.78669275928,208436.49119373778,209489.19569471624,210541.9001956947,211594.6046966732,212647.30919765166,213700.01369863012,214752.71819960861,215805.42270058708,216858.12720156554,217910.83170254403,218963.5362035225,220016.24070450096,221068.94520547945,222121.64970645792,223174.3542074364,224227.05870841487,225279.76320939334,226332.46771037183,227385.1722113503,228437.87671232875,229490.58121330725,230543.2857142857,231595.99021526417,232648.69471624267,233701.39921722113,234754.1037181996,235806.80821917808,236859.51272015655,237912.217221135,238964.9217221135,240017.62622309197,241070.33072407046,242123.03522504892,243175.7397260274,244228.44422700588,245281.14872798434,246333.8532289628,247386.5577299413,248439.26223091976,249491.96673189822,250544.67123287672,251597.37573385518,252650.08023483364,253702.78473581214,254755.4892367906,255808.19373776906,256860.89823874756,257913.60273972602,258966.30724070448,260019.01174168297,261071.71624266144,262124.42074363993,263177.1252446184,264229.82974559686,265282.5342465753,266335.2387475538,267387.9432485323,268440.64774951077,269493.35225048923,270546.0567514677,271598.76125244616,272651.4657534247,273704.17025440314,274756.8747553816,275809.57925636007,276862.28375733853,277914.988258317,278967.6927592955,280020.397260274,281073.10176125244,282125.8062622309,283178.5107632094,284231.21526418783,285283.91976516636,286336.6242661448,287389.3287671233,288442.03326810175,289494.7377690802,290547.44227005873,291600.1467710372,292652.85127201566,293705.5557729941,294758.2602739726,295810.96477495105,296863.66927592957,297916.37377690803,298969.0782778865,300021.78277886496,301074.4872798434,302127.1917808219,303179.8962818004,304232.60078277887,305285.30528375733,306338.0097847358,307390.71428571426,308443.4187866928,309496.12328767125,310548.8277886497,311601.5322896282,312654.23679060664,313706.9412915851,314759.6457925636,315812.3502935421,316865.05479452055,317917.759295499,318970.4637964775,320023.16829745594,321075.87279843446,322128.5772994129,323181.2818003914,324233.98630136985,325286.6908023483,326339.3953033268,327392.0998043053,328444.80430528376,329497.5088062622,330550.2133072407,331602.91780821915,332655.6223091977,333708.32681017614,334761.0313111546,335813.73581213306,336866.4403131115,337919.14481409,338971.8493150685,340024.553816047,341077.25831702544,342129.9628180039,343182.66731898236,344235.3718199608,345288.07632093935,346340.7808219178,347393.4853228963,348446.18982387474,349498.8943248532,349498.8943248532,350551.5988258317,351604.3033268102,352657.00782778865,353709.7123287671,354762.4168297456,355815.12133072404,356867.82583170256,357920.530332681,358973.2348336595,360025.93933463795,361078.6438356164,362131.3483365949,363184.0528375734,364236.75733855186,365289.4618395303,366342.1663405088,367394.87084148725,368447.5753424658,369500.27984344424,370552.9843444227,371605.68884540116,372658.3933463796,373711.0978473581,374763.8023483366,375816.5068493151,376869.21135029354,377921.915851272,378974.62035225047,380027.3248532289,381080.02935420745,382132.7338551859,383185.4383561644,384238.14285714284,385290.8473581213,386343.5518590998,387396.2563600783,388448.96086105675,389501.6653620352,390554.3698630137,391607.07436399214,392659.77886497066,393712.4833659491,394765.1878669276,395817.89236790605,396870.5968688845,397923.301369863,398976.0058708415,400028.71037181997,401081.4148727984,402134.1193737769,403186.82387475536,404239.5283757339,405292.23287671234,406344.9373776908,407397.64187866927,408450.34637964773,409503.0508806262,410555.7553816047,411608.4598825832,412661.16438356164,413713.8688845401,414766.57338551857,415819.27788649703,416871.98238747555,417924.686888454,418977.3913894325,420030.09589041094,421082.8003913894,422135.50489236787,423188.2093933464,424240.91389432485,425293.6183953033,426346.3228962818,427399.02739726024,428451.73189823877,429504.43639921723,430557.1409001957,431609.84540117416,432662.5499021526,433715.2544031311,434767.9589041096,435820.66340508807,436873.36790606653,437926.072407045,438978.77690802346,440031.4814090019,441084.18590998044,442136.8904109589,443189.59491193737,444242.29941291583,445295.0039138943,446347.7084148728,447400.4129158513,448453.11741682974,449505.8219178082,450558.52641878667,451611.23091976513,452663.93542074366,453716.6399217221,454769.3444227006,455822.04892367905,456874.7534246575,457927.457925636,458980.1624266145,460032.86692759296,461085.5714285714,462138.2759295499,463190.98043052835,464243.68493150687,465296.38943248533,466349.0939334638,467401.79843444226,468454.5029354207,469507.2074363992,470559.9119373777,471612.61643835617,472665.32093933463,473718.0254403131,474770.72994129156,475823.43444227,476876.13894324854,477928.843444227,478981.5479452055,480034.25244618393,481086.9569471624,482139.6614481409,483192.3659491194,484245.07045009785,485297.7749510763,486350.4794520548,487403.18395303324,488455.88845401176,489508.5929549902,490561.2974559687,491614.00195694715,492666.7064579256,493719.4109589041,494772.1154598826,495824.81996086106,496877.5244618395,497930.228962818,498982.93346379645,500035.6379647749,501088.34246575343,502141.0469667319,503193.75146771036,504246.4559686888,505299.1604696673,506351.8649706458,507404.5694716243,508457.27397260274,509509.9784735812,510562.68297455966,511615.3874755381,512668.09197651665,513720.7964774951,514773.5009784736,515826.20547945204,516878.9099804305,517931.61448140896,518984.3189823875,520037.02348336595,521089.7279843444,522142.4324853229,523195.13698630134,524247.84148727986,525300.5459882583,526353.2504892368,527405.9549902153,528458.6594911937,529511.3639921722,530564.0684931506,531616.7729941291,532669.4774951076,533722.1819960861,534774.8864970646,535827.5909980431,536880.2954990215,537933.0]\n",
       "}\n",
       "}],\n",
       "\"metainfo_list\":[]\n",
       "};\n",
       "           var plotContainer = document.getElementById(\"xjjXb7\");\n",
       "           window.letsPlotCall(function() {{\n",
       "               LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);\n",
       "           }});\n",
       "       })();    \n",
       "   </script>"
      ],
      "text/plain": [
       "<lets_plot.plot.core.PlotSpec at 0x107b5bdc0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot distribution of of qids using ggplot2\n",
    "ggplot(qids, aes(x=qids)) + geom_density(binwidth=10, method='histodot') + ggtitle(\" \") + labs(x=\"Question ID\", y=\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat question1 and question2 into a single string\n",
    "questions = df['question1'].astype(str) + \" \" + df['question2'].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = questions.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get min, max, mean, and standard deviation of a list of numbers\n",
    "def min_max_mean_std(numbers):\n",
    "    mean = sum(numbers) / len(numbers)\n",
    "    std = (sum([(x - mean) ** 2 for x in numbers]) / len(numbers)) ** 0.5\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.6450963417349 55.00734920633935\n"
     ]
    }
   ],
   "source": [
    "# call function on questions list\n",
    "mean, std = min_max_mean_std(questions)\n",
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 1319, 120.6450963417349, 55.00734920633935)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(questions), max(questions), mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "   <div id=\"tSdq15\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
       "       (function() {\n",
       "           var plotSpec={\n",
       "\"data\":{\n",
       "},\n",
       "\"mapping\":{\n",
       "\"x\":\"x\"\n",
       "},\n",
       "\"data_meta\":{\n",
       "},\n",
       "\"ggtitle\":{\n",
       "\"text\":\" \"\n",
       "},\n",
       "\"theme\":{\n",
       "\"name\":\"classic\",\n",
       "\"flavor\":\"high_contrast_dark\"\n",
       "},\n",
       "\"kind\":\"plot\",\n",
       "\"scales\":[{\n",
       "\"name\":\"Question Length\",\n",
       "\"aesthetic\":\"x\"\n",
       "},{\n",
       "\"name\":\"Count\",\n",
       "\"aesthetic\":\"y\"\n",
       "}],\n",
       "\"layers\":[{\n",
       "\"geom\":\"histogram\",\n",
       "\"mapping\":{\n",
       "},\n",
       "\"data_meta\":{\n",
       "},\n",
       "\"bins\":100.0,\n",
       "\"fill\":\"white\",\n",
       "\"data\":{\n",
       "\"..count..\":[2.0,13.0,920.0,8142.0,22829.0,42172.0,58809.0,55477.0,46665.0,36233.0,29694.0,21131.0,17477.0,15191.0,11139.0,8878.0,7642.0,5167.0,4393.0,3111.0,2281.0,1922.0,1474.0,674.0,570.0,484.0,396.0,272.0,237.0,174.0,167.0,147.0,122.0,73.0,52.0,37.0,16.0,14.0,16.0,12.0,11.0,2.0,9.0,1.0,4.0,2.0,1.0,1.0,1.0,0.0,3.0,3.0,1.0,1.0,0.0,0.0,1.0,1.0,2.0,0.0,0.0,2.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,3.0,1.0,9.0,1.0,1.0],\n",
       "\"x\":[3.465909999999999,16.77973,30.09355,43.40737,56.72119,70.03500999999999,83.34882999999999,96.66265,109.97646999999999,123.29028999999998,136.60411,149.91793,163.23175,176.54557,189.85939000000002,203.17321,216.48703,229.80085,243.11467,256.42849,269.74231,283.05613,296.36995,309.68377,322.99759,336.31141,349.62523,362.93905,376.25287000000003,389.56669,402.88051,416.19433,429.50815,442.82197,456.13579,469.44961,482.76342999999997,496.07725,509.39107,522.70489,536.0187099999999,549.33253,562.64635,575.96017,589.27399,602.58781,615.90163,629.21545,642.52927,655.84309,669.15691,682.47073,695.78455,709.09837,722.41219,735.72601,749.03983,762.35365,775.66747,788.98129,802.29511,815.60893,828.92275,842.23657,855.55039,868.86421,882.17803,895.49185,908.80567,922.11949,935.43331,948.74713,962.0609499999999,975.37477,988.68859,1002.0024099999999,1015.31623,1028.63005,1041.9438699999998,1055.25769,1068.5715099999998,1081.8853299999998,1095.19915,1108.5129699999998,1121.8267899999998,1135.14061,1148.4544299999998,1161.7682499999999,1175.08207,1188.3958899999998,1201.7097099999999,1215.02353,1228.3373499999998,1241.6511699999999,1254.96499,1268.2788099999998,1281.5926299999999,1294.90645,1308.2202699999998,1321.5340899999999]\n",
       "}\n",
       "}],\n",
       "\"metainfo_list\":[]\n",
       "};\n",
       "           var plotContainer = document.getElementById(\"tSdq15\");\n",
       "           window.letsPlotCall(function() {{\n",
       "               LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);\n",
       "           }});\n",
       "       })();    \n",
       "   </script>"
      ],
      "text/plain": [
       "<lets_plot.plot.core.PlotSpec at 0x2844b10a0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot distribution of questions using ggplot2\n",
    "ggplot(questions, aes(x=questions)) + \\\n",
    "    geom_histogram(bins = 100, fill = 'white') +\\\n",
    "          ggtitle(\" \") + labs(x=\"Question Length\", y=\"Count\") + \\\n",
    "          theme_classic() + \\\n",
    "            flavor_high_contrast_dark() \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new column to the dataframe with the length of each question\n",
    "df['characters'] = questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word count for each question\n",
    "questions = df['question1'].astype(str) + \" \" + df['question2'].astype(str).tolist()\n",
    "\n",
    "# split questions into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = questions.apply(lambda x: x.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = questions.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = min_max_mean_std(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 270, 22.124200450171905, 10.074891656619457)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(questions), max(questions), mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "   <div id=\"Aya171\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
       "       (function() {\n",
       "           var plotSpec={\n",
       "\"data\":{\n",
       "},\n",
       "\"mapping\":{\n",
       "\"x\":\"x\"\n",
       "},\n",
       "\"data_meta\":{\n",
       "},\n",
       "\"ggtitle\":{\n",
       "\"text\":\" \"\n",
       "},\n",
       "\"theme\":{\n",
       "\"name\":\"classic\",\n",
       "\"flavor\":\"high_contrast_dark\"\n",
       "},\n",
       "\"kind\":\"plot\",\n",
       "\"scales\":[{\n",
       "\"name\":\"Question Word Length\",\n",
       "\"aesthetic\":\"x\"\n",
       "},{\n",
       "\"name\":\"Count\",\n",
       "\"aesthetic\":\"y\"\n",
       "}],\n",
       "\"layers\":[{\n",
       "\"geom\":\"histogram\",\n",
       "\"mapping\":{\n",
       "},\n",
       "\"data_meta\":{\n",
       "},\n",
       "\"bins\":100.0,\n",
       "\"fill\":\"white\",\n",
       "\"data\":{\n",
       "\"..count..\":[3.0,18.0,5528.0,11487.0,39398.0,69725.0,72230.0,40311.0,45339.0,30901.0,24648.0,12128.0,14016.0,10863.0,5441.0,6554.0,4563.0,3016.0,1525.0,1763.0,1206.0,648.0,716.0,526.0,404.0,204.0,220.0,201.0,107.0,131.0,120.0,97.0,55.0,53.0,33.0,17.0,17.0,14.0,9.0,5.0,9.0,1.0,3.0,0.0,1.0,2.0,2.0,2.0,3.0,1.0,0.0,2.0,1.0,0.0,0.0,2.0,1.0,2.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,4.0,1.0,3.0,2.0,3.0,1.0],\n",
       "\"x\":[1.48276,4.20028,6.9178,9.63532,12.35284,15.07036,17.787879999999998,20.505399999999998,23.22292,25.94044,28.65796,31.37548,34.092999999999996,36.81052,39.52804,42.24556,44.96308,47.6806,50.39812,53.11564,55.83316,58.55068,61.2682,63.98572,66.70324,69.42076,72.13828,74.8558,77.57332,80.29084,83.00836,85.72588,88.4434,91.16091999999999,93.87844,96.59595999999999,99.31348,102.03099999999999,104.74852,107.46603999999999,110.18356,112.90108,115.6186,118.33612,121.05364,123.77116,126.48868,129.2062,131.92372,134.64124,137.35876000000002,140.07628,142.7938,145.51132,148.22884000000002,150.94636,153.66388,156.3814,159.09892000000002,161.81644,164.53396,167.25148000000002,169.96900000000002,172.68652,175.40404,178.12156000000002,180.83908,183.5566,186.27412,188.99164000000002,191.70916,194.42668,197.1442,199.86172000000002,202.57924,205.29676,208.01428,210.73180000000002,213.44932,216.16684,218.88436000000002,221.60188,224.3194,227.03692,229.75444000000002,232.47196,235.18948,237.907,240.62452000000002,243.34204,246.05956,248.77708,251.49460000000002,254.21212,256.92964,259.64716,262.36467999999996,265.0822,267.79972,270.51723999999996]\n",
       "}\n",
       "}],\n",
       "\"metainfo_list\":[]\n",
       "};\n",
       "           var plotContainer = document.getElementById(\"Aya171\");\n",
       "           window.letsPlotCall(function() {{\n",
       "               LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);\n",
       "           }});\n",
       "       })();    \n",
       "   </script>"
      ],
      "text/plain": [
       "<lets_plot.plot.core.PlotSpec at 0x2844b9520>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot distribution of questions using ggplot2\n",
    "ggplot(questions, aes(x=questions)) + \\\n",
    "    geom_histogram(bins = 100, fill = 'white') +\\\n",
    "          ggtitle(\" \") + labs(x=\"Question Word Length\", y=\"Count\") + \\\n",
    "          theme_classic() + \\\n",
    "            flavor_high_contrast_dark()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, I will drop those questions that have 99 words and less than 15 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['words'] = questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove id and qid1 and qid2\n",
    "df.drop(['id', 'qid1', 'qid2'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>characters</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>124</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "      <td>116</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "      <td>116</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question1  \\\n",
       "0  What is the step by step guide to invest in sh...   \n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2  How can I increase the speed of my internet co...   \n",
       "3  Why am I mentally very lonely? How can I solve...   \n",
       "4  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \\\n",
       "0  What is the step by step guide to invest in sh...             0   \n",
       "1  What would happen if the Indian government sto...             0   \n",
       "2  How can Internet speed be increased by hacking...             0   \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0   \n",
       "4            Which fish would survive in salt water?             0   \n",
       "\n",
       "   characters  words  \n",
       "0         124     26  \n",
       "1         140     21  \n",
       "2         133     24  \n",
       "3         116     20  \n",
       "4         116     20  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert is_duplicate as yes or no\n",
    "df['is_duplicate'] = df['is_duplicate'].apply(lambda x: 'yes' if x == 1 else 'no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "   <div id=\"YhPun1\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
       "       (function() {\n",
       "           var plotSpec={\n",
       "\"data\":{\n",
       "},\n",
       "\"mapping\":{\n",
       "\"x\":\"words\",\n",
       "\"fill\":\"is_duplicate\"\n",
       "},\n",
       "\"data_meta\":{\n",
       "},\n",
       "\"ggsize\":{\n",
       "\"width\":700.0,\n",
       "\"height\":400.0\n",
       "},\n",
       "\"theme\":{\n",
       "\"name\":\"classic\",\n",
       "\"flavor\":\"high_contrast_dark\"\n",
       "},\n",
       "\"kind\":\"plot\",\n",
       "\"scales\":[{\n",
       "\"aesthetic\":\"fill\",\n",
       "\"guide\":\"none\",\n",
       "\"discrete\":true\n",
       "}],\n",
       "\"layers\":[{\n",
       "\"geom\":\"density\",\n",
       "\"mapping\":{\n",
       "},\n",
       "\"data_meta\":{\n",
       "},\n",
       "\"color\":\"dark_green\",\n",
       "\"alpha\":0.7,\n",
       "\"data\":{\n",
       "\"..density..\":[4.81461700564927E-6,4.475934054764298E-6,5.874278870164383E-6,1.3183807143889063E-5,4.376938191748027E-5,2.0908433433496746E-4,7.632202938178302E-4,0.0017700078537089344,0.0029130565460651946,0.004129702062113542,0.005660901599823483,0.007479762482482627,0.009191982764025193,0.010897649744418026,0.012998000875877857,0.015471435647776671,0.017981726201431635,0.020860783040289155,0.024447177246450034,0.02816479005981475,0.031271357230162895,0.034683643682027636,0.03900863440974743,0.0429549996427251,0.04544990919017953,0.04787112838387386,0.050778985144142696,0.050778985144142696,0.052676883712414656,0.05297652127733125,0.053387793418819975,0.05423719798632933,0.054027517569388867,0.05269467791144478,0.05169486808436643,0.05081143323734414,0.04855562139659093,0.04855562139659093,0.04564510535171975,0.043780622193994485,0.042494408768453146,0.04035062814931663,0.038033505827554,0.036486139297602235,0.035159137086507086,0.033184733239102364,0.03116010878913129,0.02952290028474241,0.028027899151103132,0.026521357994705532,0.02565322732172388,0.025320512147552838,0.025320512147552838,0.024427861012250387,0.022890801475218908,0.021949624681204316,0.021544378108944023,0.020450334213890427,0.01888393059871831,0.018028800943797576,0.017635905799272673,0.016712171418134945,0.015665480202241976,0.01523738940782688,0.014898018879917567,0.013893856109840844,0.012868250346624421,0.012416865620703708,0.012071637200985526,0.011398818691832537,0.010840287790003844,0.010570197770608008,0.010141440648375267,0.009433414993978368,0.008970142314737055,0.008785847079254028,0.008365259322452686,0.007692878443613999,0.007218065641680657,0.006950605937296672,0.006596739476408326,0.006222210386449102,0.006016219328061864,0.005804192235425839,0.005311831197957192,0.004744456392671787,0.004360791052533717,0.004116338780398903,0.0038853374694305933,0.003674251568754166,0.0034921975646263616,0.003328594589783418,0.0031700793059735895,0.003041242592987658,0.0029219356033779566,0.0027573562222610363,0.002569085572507985,0.0024579416603682305,0.0023942428726701236,0.0022488955095918564,0.0020311618212656714,0.001841144907662155,0.0017241605611749541,0.0016708998087361408,0.0016229938169530181,0.001520517628038001,0.0014020672823239397,0.0013178437430546902,0.0012773111542107763,0.0012553655773295755,0.0012058397258656237,0.0011082634288330172,0.001013718501791996,9.556542787788285E-4,9.319703645230658E-4,9.163462036311114E-4,8.704657823577689E-4,7.983847747758821E-4,7.435732383975062E-4,7.107174405149904E-4,6.743512192147724E-4,6.328248813552602E-4,5.994182602276975E-4,5.790838862152864E-4,5.653989031810914E-4,5.440382786475345E-4,5.041523328711305E-4,4.684872208802586E-4,4.56311559627857E-4,4.563238976801315E-4,4.4821484839345776E-4,4.20733483673016E-4,3.7793503357703675E-4,3.4987788772119615E-4,3.416992235129035E-4,3.237112485190395E-4,2.863120311881455E-4,2.5056509200127326E-4,2.2875086922524452E-4,2.227945536016931E-4,2.3021474527210506E-4,2.4564881102185645E-4,2.609357883067108E-4,2.647190019004952E-4,2.529155758703759E-4,2.2743694485798455E-4,2.07297463071109E-4,2.0757977270868973E-4,2.1546030507023944E-4,2.133854913142878E-4,1.9926732446404386E-4,1.7700318298852845E-4,1.5767454228889205E-4,1.4939504858620383E-4,1.552468809163096E-4,1.6793359472363483E-4,1.6911284410195675E-4,1.559977309278477E-4,1.4326137737947437E-4,1.3782682844852386E-4,1.3766867694674494E-4,1.3694666423574495E-4,1.2945286274648827E-4,1.1962850921157461E-4,1.1559499653215334E-4,1.1672914403819898E-4,1.1732209349125819E-4,1.126462254457152E-4,1.0078268886844114E-4,8.89837891293524E-5,8.223302715432925E-5,7.65685421829558E-5,6.903614709381423E-5,6.383133157276116E-5,6.15321098217682E-5,5.594128954734491E-5,4.5707843193338266E-5,3.758939483101861E-5,3.5416364394942214E-5,3.896017513295464E-5,4.473400494217922E-5,4.695450431779478E-5,4.3194740046701166E-5,3.390053381337653E-5,2.4457888226703644E-5,2.0822989056918463E-5,2.0677083497296637E-5,1.981436033080281E-5,1.9765820398470297E-5,2.3143532975972128E-5,2.8873300964442007E-5,3.3279274604024914E-5,3.0444804987222803E-5,1.954581340982883E-5,1.0275014183083162E-5,8.030534871034668E-6,9.927241723831211E-6,1.2517767653071925E-5,1.3356040328266684E-5,1.1924270361110766E-5,9.205739495184081E-6,6.7390950510836245E-6,6.244903823414227E-6,7.5199783649746875E-6,9.19577375867837E-6,1.0141854544572972E-5,9.676948485766478E-6,8.676351320100076E-6,8.413293671313317E-6,9.50040416540245E-6,1.138555052278783E-5,1.1483789037928944E-5,8.387290665985624E-6,4.52285681046293E-6,1.8620529824037453E-6,5.304540968707111E-7,1.1092825544552404E-7,2.2347759577536816E-7,1.2539619232140548E-6,3.954206853725451E-6,6.738507141519707E-6,6.203551359360596E-6,3.0852330804783236E-6,8.289082928010788E-7,1.2030838525240977E-7,9.433138679713351E-9,4.723262091298664E-10,1.9092844441147667E-9,2.7166559612642006E-8,2.0977407285844148E-7,8.910381034514556E-7,2.1591599452149286E-6,3.262231682770057E-6,3.5107135743574666E-6,2.8135161313973686E-6,1.5064073929111726E-6,4.785978567636107E-7,9.879051248366074E-8,1.5921187486051877E-7,8.683589471740531E-7,2.6954363005025608E-6,4.541016054189003E-6,4.312169254276602E-6,3.126574041376668E-6,3.6419232005895295E-6,4.778478098985724E-6,4.226724933118354E-6,3.7727665432393736E-6,5.652895461821286E-6,7.068532751184988E-6,5.059644759966043E-6,2.0012726534774365E-6,6.666110269593656E-7,1.0217967308261533E-6,2.027256664834927E-6,2.2709326345345986E-6,1.3761014050953566E-6,4.504906086545687E-7,7.970087261445422E-8,8.6747317611024E-9,1.984382679324904E-8,1.9223944002597592E-7,1.026277464297101E-6,2.9599388939249597E-6,4.6118568837759815E-6,3.883059972855787E-6,1.7847165910764527E-6,5.969971198712617E-7,7.919128120041572E-7,1.7869012987115835E-6,2.3381655342883944E-6,1.6563996214769434E-6,6.339508211637963E-7,1.3107411163047627E-7,1.4640273336696829E-8,8.833897492568748E-10,2.8795620001008878E-11,0.0,0.0,1.7347189937707148E-11,5.780672510394992E-10,1.0406351873727613E-8,1.0120210720753985E-7,5.317164181753662E-7,1.5100217606351577E-6,2.3302679250415922E-6,2.06080661408875E-6,1.5343255691590035E-6,1.921488111952583E-6,2.3937311795285974E-6,1.932090800948847E-6,1.5329790054080528E-6,2.0568623702273295E-6,2.4150682048244483E-6,2.079222264320055E-6,2.599333275567351E-6,4.239910732359269E-6,4.503152660500629E-6,2.6367352537235186E-6,8.356825283920102E-7,1.4312844631665192E-7,1.386789797291413E-8,1.1791886595439336E-8,1.0653240894623769E-7,5.506628535354375E-7,1.537919336590304E-6,2.3203432866775795E-6,1.8912190533950765E-6,8.327264036256186E-7,1.9807686053628512E-7,2.5452829053396462E-8,1.766889075314681E-9,6.626030312773628E-11,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,3.00227214495069E-11,9.147077722875745E-10,1.5055167432374438E-8,1.3386274249071396E-7,6.429907938425314E-7,1.6684788256902077E-6,2.3388779102032073E-6,1.7711906579791726E-6,7.245927455395401E-7,1.6013768963414454E-7,1.9118950261342548E-8,1.2331202687641425E-9,4.2965288992185866E-11,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,8.20146574622884E-11,2.108052519552804E-9,2.927131853868418E-8,2.195705315057532E-7,8.897678893267126E-7,1.9478266877315406E-6,2.3035349367176984E-6,1.4716692071857402E-6,5.082173937399356E-7,1.0076962493002923E-7,7.706431399315581E-8,4.1380813951851847E-7,1.443257706530268E-6,3.149008052679883E-6,4.961521587997554E-6,6.142524055713411E-6,5.932878968028912E-6,4.873251476744034E-6,4.0297866130097735E-6,3.2759642702819784E-6,2.064515011223593E-6,8.414858844397393E-7,3.8395939642430843E-7,9.108085442740325E-7,2.3475475918894544E-6,4.19139100988485E-6,5.761612337999414E-6,6.202158571514522E-6,5.36223687775246E-6,4.4112265639596436E-6,4.004254540034852E-6,3.860184816387803E-6,3.516663031911126E-6,2.6958521957875113E-6,2.1409757599565786E-6,3.1575513424769693E-6,4.633459370378774E-6,4.118538919757495E-6,2.50683035276554E-6,2.2461523326776197E-6,3.0834187478148465E-6,3.542375337065608E-6,3.1049471966910626E-6,6.094960170026755E-6,6.4842540808452146E-6,5.955128269746003E-6,3.402835411231778E-6,7.985057009925795E-6,3.840417755748206E-5,1.4790717118267117E-4,4.776059873227094E-4,0.001204822173098739,0.0027372406584308248,0.00409540196386054,0.005802936322362513,0.007920009686725812,0.011354541619227875,0.014021370068421529,0.017804531017411914,0.021333753755964826,0.02737800618665406,0.033582286831099935,0.042632916318626174,0.04677614118236662,0.053996081998528546,0.06053644405629941,0.06922772397686511,0.06922772397686511,0.06924036127099886,0.07273910787194804,0.07516374506019606,0.07831267317732657,0.07503813482696109,0.07458873507556828,0.07419287778516691,0.07419287778516691,0.0718399122299808,0.06619990902302926,0.06170464333099507,0.05889091874403197,0.05422104696521244,0.050582159514114156,0.04542247022806529,0.042368929423682165,0.0376748419016225,0.0376748419016225,0.03528031031767009,0.031168987779774556,0.02922010220327494,0.0257643019348752,0.024283840564536377,0.021886171171237596,0.020909176755048886,0.018919083636113387,0.018348732083393045,0.017945054898605487,0.017443341750151102,0.015785073521418284,0.014450769877696154,0.013129617390401565,0.011973364352027597,0.011112613901576541,0.010184896635463256,0.009683623268564066,0.008762805582117011,0.008286321120896742,0.0075989297515580995,0.007386607680139571,0.00652167523791965,0.006089476469213879,0.005491394208373072,0.005266732190631273,0.004692809239163157,0.004592363228728672,0.004695125964687081,0.004632117095150613,0.00384767951055905,0.0033257225865486154,0.002897402073704176,0.002689982346642347,0.0027098597306565377,0.0031864126912023233,0.004081166132378942,0.0036953527868935007,0.0029080481284663754,0.002524727631521944,0.0024464762242601,0.0020320884101405565,0.0016951195405021163,0.0013706835848894896,0.001233596621152914,0.00119693301313323,0.0012001552371345644,9.917470749546864E-4,8.839447712293532E-4,8.398121307145649E-4,8.113954432009085E-4,7.155906866589892E-4,6.304957906738173E-4,5.293438771847663E-4,5.122508103553644E-4,5.616283823285328E-4,5.179238262863087E-4,4.4211123692867695E-4,4.2215765286650466E-4,4.4075501169374724E-4,3.437324821193151E-4,2.4485592630121253E-4,2.26373936678535E-4,2.5014282217529E-4,2.545105013875198E-4,2.6737755020238003E-4,2.5170708117598986E-4,2.4200721679771894E-4,2.0637815748934717E-4,1.8549424657958666E-4,1.6283021305671554E-4,1.4816311816295223E-4,1.2837832421929378E-4,1.1252065442283935E-4,9.675015910377445E-5,9.982274930139015E-5,1.1951302010934769E-4,1.179188672816165E-4,1.078146926310139E-4,8.203922524213071E-5,5.9555740884779404E-5,6.758859053717189E-5,8.402580014146739E-5,6.203123039255409E-5,4.356967910366204E-5,5.115076475510945E-5,5.6647523021729104E-5,3.0101612424522043E-5,1.5026952060016195E-5,2.403708651961426E-5,2.970257862017381E-5,2.426616717503774E-5,2.383116248676434E-5,2.896482060586345E-5,2.3655648225409368E-5,8.178530671203112E-6,7.2239328336407884E-6,1.6219179673502854E-5,1.701528569492536E-5,1.3675652865197017E-5,7.731519024127045E-6,1.737188328005403E-6,2.723381600312814E-7,2.105403363551545E-6,1.0234538339964238E-5,1.6950236955033206E-5,1.2204807067498419E-5,8.742642693420278E-6,1.0583418091976066E-5,1.0705273211647775E-5,4.354898150973802E-6,6.023815944133539E-7,2.176732284286574E-7,1.6798138296544495E-6,4.8266197532535174E-6,4.53377846894774E-6,1.3919115942933039E-6,1.3966807782211455E-7,5.5997829579694195E-9,5.532542462040492E-8,9.805707313502238E-7,5.714193797862139E-6,1.1355478730808924E-5,9.912717729155301E-6,8.213292442287248E-6,1.0053492925785567E-5,1.1247316428116974E-5,5.4065206367511985E-6,8.829307038400683E-7,4.734475845601323E-8,8.303547096579985E-10,2.944727873644526E-10,2.3541844495475535E-8,6.151343128944703E-7,5.253316498084108E-6,1.4663286738121953E-5,1.337713884652301E-5,3.9890552846618746E-6,4.1060324879007084E-7,4.306530947583696E-7,2.612340250643967E-6,5.3320243297687895E-6,3.5572147022049695E-6,7.756435436908665E-7,5.5277504949608407E-8,1.2875640120153833E-9,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.7565327165142173E-10,1.2287445655287418E-8,2.809319436739072E-7,2.0993013490588583E-6,5.127227557761568E-6,4.092837932091652E-6,1.0678263903943038E-6,9.10564886605537E-8,3.514611725953653E-9,4.50428672307546E-8,6.781472298100548E-7,3.3387127151265056E-6,5.372398970514394E-6,2.8254759197743244E-6,4.856787594288414E-7,2.7286055476014558E-8,5.010332548333769E-10,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,3.545985866052533E-11,3.5513245585230907E-9,1.1624596206878383E-7,1.243653895782632E-6,4.348658863707427E-6,4.969873021325939E-6,1.8563907357441835E-6,2.266353438113634E-7,9.043156209179156E-9,1.1793604879611747E-10,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\n",
       "\"..quantile..\":[0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0],\n",
       "\"words\":[2.0,2.524461839530333,3.0489236790606653,3.5733855185909977,4.097847358121331,4.6223091976516635,5.1467710371819955,5.671232876712328,6.195694716242661,6.720156555772994,7.244618395303327,7.769080234833659,8.293542074363991,8.818003913894325,9.342465753424657,9.86692759295499,10.391389432485322,10.915851272015654,11.440313111545988,11.96477495107632,12.489236790606654,13.013698630136986,13.538160469667318,14.062622309197652,14.587084148727984,15.111545988258316,15.63600782778865,15.63600782778865,16.16046966731898,16.684931506849313,17.20939334637965,17.73385518590998,18.258317025440313,18.782778864970645,19.307240704500977,19.83170254403131,20.356164383561644,20.356164383561644,20.880626223091976,21.40508806262231,21.92954990215264,22.454011741682972,22.978473581213308,23.50293542074364,24.027397260273972,24.551859099804304,25.076320939334636,25.600782778864968,26.125244618395303,26.649706457925635,27.174168297455967,27.6986301369863,27.6986301369863,28.22309197651663,28.747553816046967,29.2720156555773,29.79647749510763,30.320939334637963,30.845401174168295,31.369863013698627,31.894324853228962,32.4187866927593,32.94324853228963,33.46771037181996,33.992172211350294,34.516634050880626,35.04109589041096,35.56555772994129,36.09001956947162,36.614481409001954,37.138943248532286,37.66340508806262,38.18786692759295,38.71232876712329,39.23679060665362,39.76125244618395,40.285714285714285,40.81017612524462,41.33463796477495,41.85909980430528,42.38356164383561,42.908023483365945,43.43248532289628,43.956947162426616,44.48140900195695,45.00587084148728,45.53033268101761,46.054794520547944,46.579256360078276,47.10371819960861,47.62818003913894,48.15264187866927,48.677103718199604,49.201565557729936,49.726027397260275,50.25048923679061,50.77495107632094,51.29941291585127,51.8238747553816,52.348336594911935,52.87279843444227,53.3972602739726,53.92172211350293,54.44618395303326,54.970645792563595,55.495107632093934,56.019569471624266,56.5440313111546,57.06849315068493,57.59295499021526,58.11741682974559,58.641878669275926,59.16634050880626,59.69080234833659,60.21526418786692,60.73972602739725,61.26418786692759,61.788649706457925,62.31311154598826,62.83757338551859,63.36203522504892,63.88649706457925,64.41095890410958,64.93542074363992,65.45988258317024,65.98434442270059,66.50880626223092,67.03326810176125,67.55772994129158,68.08219178082192,68.60665362035225,69.13111545988258,69.65557729941291,70.18003913894324,70.70450097847358,71.22896281800391,71.75342465753424,72.27788649706457,72.8023483365949,73.32681017612524,73.85127201565557,74.3757338551859,74.90019569471625,75.42465753424658,75.94911937377691,76.47358121330724,76.99804305283757,77.5225048923679,78.04696673189824,78.57142857142857,79.0958904109589,79.62035225048923,80.14481409001957,80.6692759295499,81.19373776908023,81.71819960861056,82.2426614481409,82.76712328767123,83.29158512720156,83.81604696673189,84.34050880626222,84.86497064579255,85.38943248532289,85.91389432485323,86.43835616438356,86.9628180039139,87.48727984344423,88.01174168297456,88.53620352250489,89.06066536203522,89.58512720156556,90.10958904109589,90.63405088062622,91.15851272015655,91.68297455968688,92.20743639921722,92.73189823874755,93.25636007827788,93.78082191780821,94.30528375733854,94.82974559686888,95.35420743639921,95.87866927592954,96.40313111545987,96.9275929549902,97.45205479452055,97.97651663405088,98.50097847358121,99.02544031311155,99.54990215264188,100.07436399217221,100.59882583170254,101.12328767123287,101.6477495107632,102.17221135029354,102.69667318982387,103.2211350293542,103.74559686888453,104.27005870841487,104.7945205479452,105.31898238747553,105.84344422700586,106.3679060665362,106.89236790606653,107.41682974559686,107.94129158512719,108.46575342465754,108.99021526418787,109.5146771037182,110.03913894324853,110.56360078277886,111.0880626223092,111.61252446183953,112.13698630136986,112.66144814090019,113.18590998043052,113.71037181996086,114.23483365949119,114.75929549902152,115.28375733855185,115.80821917808218,116.33268101761252,116.85714285714285,117.38160469667318,117.90606653620351,118.43052837573384,118.95499021526417,119.4794520547945,120.00391389432485,120.52837573385519,121.05283757338552,121.57729941291585,122.10176125244618,122.62622309197651,123.15068493150685,123.67514677103718,124.19960861056751,124.72407045009784,125.24853228962817,125.7729941291585,126.29745596868884,126.82191780821917,127.3463796477495,127.87084148727983,128.39530332681016,128.91976516634048,129.44422700587083,129.96868884540118,130.4931506849315,131.01761252446184,131.54207436399216,132.0665362035225,132.59099804305282,133.11545988258317,133.63992172211348,134.16438356164383,134.68884540117415,135.2133072407045,135.7377690802348,136.26223091976516,136.7866927592955,137.31115459882582,137.83561643835617,138.3600782778865,138.88454011741683,139.40900195694715,139.9334637964775,140.45792563600781,140.98238747553816,141.50684931506848,142.03131115459882,142.55577299412914,143.0802348336595,143.6046966731898,144.12915851272015,144.65362035225047,145.17808219178082,145.70254403131113,146.22700587084148,146.7514677103718,147.27592954990214,147.8003913894325,148.3248532289628,148.84931506849315,149.37377690802347,149.89823874755382,150.42270058708414,150.94716242661448,151.4716242661448,151.99608610567515,152.52054794520546,153.0450097847358,153.56947162426613,154.09393346379647,154.6183953033268,155.14285714285714,155.66731898238746,156.1917808219178,156.71624266144812,157.24070450097847,157.76516634050878,158.28962818003913,158.81409001956948,159.3385518590998,159.86301369863014,160.38747553816046,160.9119373776908,161.43639921722112,161.96086105675147,162.4853228962818,163.00978473581213,163.53424657534245,164.0587084148728,164.58317025440311,165.10763209393346,165.63209393346378,166.15655577299412,166.68101761252444,167.2054794520548,167.7299412915851,168.25440313111545,168.77886497064577,169.30332681017612,169.82778864970646,170.35225048923678,170.87671232876713,171.40117416829744,171.9256360078278,172.4500978473581,172.97455968688845,173.49902152641877,174.02348336594912,174.54794520547944,175.07240704500978,175.5968688845401,176.12133072407045,176.64579256360076,177.1702544031311,177.69471624266143,178.21917808219177,178.7436399217221,179.26810176125244,179.79256360078276,180.3170254403131,180.84148727984342,181.36594911937377,181.8904109589041,182.41487279843443,182.93933463796478,183.4637964774951,183.98825831702544,184.51272015655576,185.0371819960861,185.56164383561642,186.08610567514677,186.6105675146771,187.13502935420743,187.65949119373775,188.1839530332681,188.70841487279841,189.23287671232876,189.75733855185908,190.28180039138942,190.80626223091974,191.3307240704501,191.8551859099804,192.37964774951075,192.9041095890411,193.42857142857142,193.95303326810176,194.47749510763208,195.00195694716243,195.52641878669274,196.0508806262231,196.5753424657534,197.09980430528375,197.62426614481407,198.14872798434442,198.67318982387474,199.19765166340508,199.7221135029354,200.24657534246575,200.77103718199606,201.2954990215264,201.81996086105673,202.34442270058707,202.8688845401174,203.39334637964774,203.91780821917808,204.4422700587084,204.96673189823875,205.49119373776907,206.0156555772994,206.54011741682973,207.06457925636008,207.5890410958904,208.11350293542074,208.63796477495106,209.1624266144814,209.68688845401172,210.21135029354207,210.7358121330724,211.26027397260273,211.78473581213305,212.3091976516634,212.8336594911937,213.35812133072406,213.88258317025438,214.40704500978472,214.93150684931507,215.4559686888454,215.98043052837573,216.50489236790605,217.0293542074364,217.55381604696672,218.07827788649706,218.60273972602738,219.12720156555773,219.65166340508804,220.1761252446184,220.7005870841487,221.22504892367905,221.74951076320937,222.27397260273972,222.79843444227004,223.32289628180038,223.8473581213307,224.37181996086105,224.89628180039136,225.4207436399217,225.94520547945206,226.46966731898237,226.99412915851272,227.51859099804304,228.04305283757338,228.5675146771037,229.09197651663405,229.61643835616437,230.1409001956947,230.66536203522503,231.18982387475538,231.7142857142857,232.23874755381604,232.76320939334636,233.2876712328767,233.81213307240702,234.33659491193737,234.8610567514677,235.38551859099803,235.90998043052835,236.4344422700587,236.958904109589,237.48336594911936,238.0078277886497,238.53228962818002,239.05675146771037,239.5812133072407,240.10567514677103,240.63013698630135,241.1545988258317,241.67906066536202,242.20352250489236,242.72798434442268,243.25244618395303,243.77690802348334,244.3013698630137,244.825831702544,245.35029354207435,245.87475538160467,246.39921722113502,246.92367906066534,247.44814090019568,247.972602739726,248.49706457925635,249.0215264187867,249.545988258317,250.07045009784736,250.59491193737767,251.11937377690802,251.64383561643834,252.16829745596868,252.692759295499,253.21722113502935,253.74168297455967,254.26614481409,254.79060665362033,255.31506849315068,255.839530332681,256.36399217221134,256.88845401174166,257.41291585127203,257.93737769080235,258.46183953033267,258.986301369863,259.51076320939336,260.0352250489237,260.559686888454,261.0841487279843,261.6086105675147,262.133072407045,262.6575342465753,263.18199608610564,263.706457925636,264.23091976516633,264.75538160469665,265.27984344422697,265.80430528375734,266.32876712328766,266.853228962818,267.3776908023483,267.9021526418787,268.426614481409,268.9510763209393,269.4755381604696,270.0,2.0,2.524461839530333,3.0489236790606653,3.5733855185909977,4.097847358121331,4.6223091976516635,5.1467710371819955,5.671232876712328,6.195694716242661,6.720156555772994,7.244618395303327,7.769080234833659,8.293542074363991,8.818003913894325,9.342465753424657,9.86692759295499,10.391389432485322,10.915851272015654,11.440313111545988,11.96477495107632,12.489236790606654,13.013698630136986,13.538160469667318,14.062622309197652,14.062622309197652,14.587084148727984,15.111545988258316,15.63600782778865,16.16046966731898,16.684931506849313,17.20939334637965,17.73385518590998,17.73385518590998,18.258317025440313,18.782778864970645,19.307240704500977,19.83170254403131,20.356164383561644,20.880626223091976,21.40508806262231,21.92954990215264,22.454011741682972,22.454011741682972,22.978473581213308,23.50293542074364,24.027397260273972,24.551859099804304,25.076320939334636,25.600782778864968,26.125244618395303,26.649706457925635,27.174168297455967,27.6986301369863,28.22309197651663,28.747553816046967,29.2720156555773,29.79647749510763,30.320939334637963,30.845401174168295,31.369863013698627,31.894324853228962,32.4187866927593,32.94324853228963,33.46771037181996,33.992172211350294,34.516634050880626,35.04109589041096,35.56555772994129,36.09001956947162,36.614481409001954,37.138943248532286,37.66340508806262,38.18786692759295,38.71232876712329,39.23679060665362,39.76125244618395,40.285714285714285,40.81017612524462,41.33463796477495,41.85909980430528,42.38356164383561,42.908023483365945,43.43248532289628,43.956947162426616,44.48140900195695,45.00587084148728,45.53033268101761,46.054794520547944,46.579256360078276,47.10371819960861,47.62818003913894,48.15264187866927,48.677103718199604,49.201565557729936,49.726027397260275,50.25048923679061,50.77495107632094,51.29941291585127,51.8238747553816,52.348336594911935,52.87279843444227,53.3972602739726,53.92172211350293,54.44618395303326,54.970645792563595,55.495107632093934,56.019569471624266,56.5440313111546,57.06849315068493,57.59295499021526,58.11741682974559,58.641878669275926,59.16634050880626,59.69080234833659,60.21526418786692,60.73972602739725,61.26418786692759,61.788649706457925,62.31311154598826,62.83757338551859,63.36203522504892,63.88649706457925,64.41095890410958,64.93542074363992,65.45988258317024,65.98434442270059,66.50880626223092,67.03326810176125,67.55772994129158,68.08219178082192,68.60665362035225,69.13111545988258,69.65557729941291,70.18003913894324,70.70450097847358,71.22896281800391,71.75342465753424,72.27788649706457,72.8023483365949,73.32681017612524,73.85127201565557,74.3757338551859,74.90019569471625,75.42465753424658,75.94911937377691,76.47358121330724,76.99804305283757,77.5225048923679,78.04696673189824,78.57142857142857,79.0958904109589,79.62035225048923,80.14481409001957,80.6692759295499,81.19373776908023,81.71819960861056,82.2426614481409,82.76712328767123,83.29158512720156,83.81604696673189,84.34050880626222,84.86497064579255,85.38943248532289,85.91389432485323,86.43835616438356,86.9628180039139,87.48727984344423,88.01174168297456,88.53620352250489,89.06066536203522,89.58512720156556,90.10958904109589,90.63405088062622,91.15851272015655,91.68297455968688,92.20743639921722,92.73189823874755,93.25636007827788,93.78082191780821,94.30528375733854,94.82974559686888,95.35420743639921,95.87866927592954,96.40313111545987,96.9275929549902,97.45205479452055,97.97651663405088,98.50097847358121,99.02544031311155,99.54990215264188,100.07436399217221,100.59882583170254,101.12328767123287,101.6477495107632,102.17221135029354,102.69667318982387,103.2211350293542,103.74559686888453,104.27005870841487,104.7945205479452,105.31898238747553,105.84344422700586,106.3679060665362,106.89236790606653,107.41682974559686,107.94129158512719,108.46575342465754,108.99021526418787,109.5146771037182,110.03913894324853,110.56360078277886,111.0880626223092,111.61252446183953,112.13698630136986,112.66144814090019,113.18590998043052,113.71037181996086,114.23483365949119,114.75929549902152,115.28375733855185,115.80821917808218,116.33268101761252,116.85714285714285,117.38160469667318,117.90606653620351,118.43052837573384,118.95499021526417,119.4794520547945,120.00391389432485,120.52837573385519,121.05283757338552,121.57729941291585,122.10176125244618,122.62622309197651,123.15068493150685,123.67514677103718,124.19960861056751,124.72407045009784,125.24853228962817,125.7729941291585,126.29745596868884,126.82191780821917,127.3463796477495,127.87084148727983,128.39530332681016,128.91976516634048,129.44422700587083,129.96868884540118,130.4931506849315,131.01761252446184,131.54207436399216,132.0665362035225,132.59099804305282,133.11545988258317,133.63992172211348,134.16438356164383,134.68884540117415,135.2133072407045,135.7377690802348,136.26223091976516,136.7866927592955,137.31115459882582,137.83561643835617,138.3600782778865,138.88454011741683,139.40900195694715,139.9334637964775,140.45792563600781,140.98238747553816,141.50684931506848,142.03131115459882,142.55577299412914,143.0802348336595,143.6046966731898,144.12915851272015,144.65362035225047,145.17808219178082,145.70254403131113,146.22700587084148,146.7514677103718,147.27592954990214,147.8003913894325,148.3248532289628,148.84931506849315,149.37377690802347,149.89823874755382,150.42270058708414,150.94716242661448,151.4716242661448,151.99608610567515,152.52054794520546,153.0450097847358,153.56947162426613,154.09393346379647,154.6183953033268,155.14285714285714,155.66731898238746,156.1917808219178,156.71624266144812,157.24070450097847,157.76516634050878,158.28962818003913,158.81409001956948,159.3385518590998,159.86301369863014,160.38747553816046,160.9119373776908,161.43639921722112,161.96086105675147,162.4853228962818,163.00978473581213,163.53424657534245,164.0587084148728,164.58317025440311,165.10763209393346,165.63209393346378,166.15655577299412,166.68101761252444,167.2054794520548,167.7299412915851,168.25440313111545,168.77886497064577,169.30332681017612,169.82778864970646,170.35225048923678,170.87671232876713,171.40117416829744,171.9256360078278,172.4500978473581,172.97455968688845,173.49902152641877,174.02348336594912,174.54794520547944,175.07240704500978,175.5968688845401,176.12133072407045,176.64579256360076,177.1702544031311,177.69471624266143,178.21917808219177,178.7436399217221,179.26810176125244,179.79256360078276,180.3170254403131,180.84148727984342,181.36594911937377,181.8904109589041,182.41487279843443,182.93933463796478,183.4637964774951,183.98825831702544,184.51272015655576,185.0371819960861,185.56164383561642,186.08610567514677,186.6105675146771,187.13502935420743,187.65949119373775,188.1839530332681,188.70841487279841,189.23287671232876,189.75733855185908,190.28180039138942,190.80626223091974,191.3307240704501,191.8551859099804,192.37964774951075,192.9041095890411,193.42857142857142,193.95303326810176,194.47749510763208,195.00195694716243,195.52641878669274,196.0508806262231,196.5753424657534,197.09980430528375,197.62426614481407,198.14872798434442,198.67318982387474,199.19765166340508,199.7221135029354,200.24657534246575,200.77103718199606,201.2954990215264,201.81996086105673,202.34442270058707,202.8688845401174,203.39334637964774,203.91780821917808,204.4422700587084,204.96673189823875,205.49119373776907,206.0156555772994,206.54011741682973,207.06457925636008,207.5890410958904,208.11350293542074,208.63796477495106,209.1624266144814,209.68688845401172,210.21135029354207,210.7358121330724,211.26027397260273,211.78473581213305,212.3091976516634,212.8336594911937,213.35812133072406,213.88258317025438,214.40704500978472,214.93150684931507,215.4559686888454,215.98043052837573,216.50489236790605,217.0293542074364,217.55381604696672,218.07827788649706,218.60273972602738,219.12720156555773,219.65166340508804,220.1761252446184,220.7005870841487,221.22504892367905,221.74951076320937,222.27397260273972,222.79843444227004,223.32289628180038,223.8473581213307,224.37181996086105,224.89628180039136,225.4207436399217,225.94520547945206,226.46966731898237,226.99412915851272,227.51859099804304,228.04305283757338,228.5675146771037,229.09197651663405,229.61643835616437,230.1409001956947,230.66536203522503,231.18982387475538,231.7142857142857,232.23874755381604,232.76320939334636,233.2876712328767,233.81213307240702,234.33659491193737,234.8610567514677,235.38551859099803,235.90998043052835,236.4344422700587,236.958904109589,237.48336594911936,238.0078277886497,238.53228962818002,239.05675146771037,239.5812133072407,240.10567514677103,240.63013698630135,241.1545988258317,241.67906066536202,242.20352250489236,242.72798434442268,243.25244618395303,243.77690802348334,244.3013698630137,244.825831702544,245.35029354207435,245.87475538160467,246.39921722113502,246.92367906066534,247.44814090019568,247.972602739726,248.49706457925635,249.0215264187867,249.545988258317,250.07045009784736,250.59491193737767,251.11937377690802,251.64383561643834,252.16829745596868,252.692759295499,253.21722113502935,253.74168297455967,254.26614481409,254.79060665362033,255.31506849315068,255.839530332681,256.36399217221134,256.88845401174166,257.41291585127203,257.93737769080235,258.46183953033267,258.986301369863,259.51076320939336,260.0352250489237,260.559686888454,261.0841487279843,261.6086105675147,262.133072407045,262.6575342465753,263.18199608610564,263.706457925636,264.23091976516633,264.75538160469665,265.27984344422697,265.80430528375734,266.32876712328766,266.853228962818,267.3776908023483,267.9021526418787,268.426614481409,268.9510763209393,269.4755381604696,270.0],\n",
       "\"is_duplicate\":[\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\"]\n",
       "}\n",
       "}],\n",
       "\"metainfo_list\":[]\n",
       "};\n",
       "           var plotContainer = document.getElementById(\"YhPun1\");\n",
       "           window.letsPlotCall(function() {{\n",
       "               LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);\n",
       "           }});\n",
       "       })();    \n",
       "   </script>"
      ],
      "text/plain": [
       "<lets_plot.plot.core.PlotSpec at 0x2844b9a30>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = df['words']\n",
    "ggplot(df, aes(x='words', fill='is_duplicate')) + ggsize(700, 400) + \\\n",
    "geom_density(color='dark_green', alpha=.7) + scale_fill_discrete(guide='none') + \\\n",
    "theme_classic() + \\\n",
    "flavor_high_contrast_dark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(402691, 5)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from here i will drop rows where words are less than 10 and greater than 67 words. \n",
    "\n",
    "df = df[df['words'] > 5]\n",
    "df = df[df['words'] < 67]\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "   <div id=\"TiG8E1\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
       "       (function() {\n",
       "           var plotSpec={\n",
       "\"data\":{\n",
       "},\n",
       "\"mapping\":{\n",
       "\"x\":\"words\",\n",
       "\"fill\":\"is_duplicate\"\n",
       "},\n",
       "\"data_meta\":{\n",
       "},\n",
       "\"ggsize\":{\n",
       "\"width\":700.0,\n",
       "\"height\":400.0\n",
       "},\n",
       "\"theme\":{\n",
       "\"name\":\"classic\",\n",
       "\"flavor\":\"high_contrast_dark\"\n",
       "},\n",
       "\"kind\":\"plot\",\n",
       "\"scales\":[{\n",
       "\"aesthetic\":\"fill\",\n",
       "\"guide\":\"none\",\n",
       "\"discrete\":true\n",
       "}],\n",
       "\"layers\":[{\n",
       "\"geom\":\"density\",\n",
       "\"mapping\":{\n",
       "},\n",
       "\"data_meta\":{\n",
       "},\n",
       "\"color\":\"dark_green\",\n",
       "\"alpha\":0.7,\n",
       "\"data\":{\n",
       "\"..density..\":[0.002497585409005234,0.0027559574678912,0.0030148612596101547,0.0032768011086728647,0.003545071727749991,0.0038232611486588336,0.004114905174433358,0.004423044686793348,0.004750141209637644,0.005097665534807299,0.0054657392584187,0.005852760690683623,0.006255202210351816,0.006667765196489571,0.007084001501888716,0.007497451947547175,0.007902603739666085,0.008296327710053751,0.008678309080737776,0.00905106309211742,0.009419301116260721,0.00978893918124083,0.010166092765841855,0.010556433744168987,0.010964576605334715,0.011394303931675896,0.011848337822322646,0.012328159748532895,0.012833591728140551,0.013362359171283584,0.013909985750469752,0.014470242273854734,0.015036481979996017,0.015602793273953494,0.016166097081778064,0.0167268164283631,0.01728897900009858,0.017859318878944493,0.018445801580616945,0.01905614768639817,0.01969688495061435,0.020372568602842915,0.021086222980655458,0.021839066897047652,0.022630179255014156,0.023455674824438288,0.024307793807119404,0.025174629842247945,0.02604086376846152,0.02689026732412491,0.027708281155910248,0.028486025455369882,0.02922217415236792,0.02992353879927804,0.030603618178559222,0.03127974383802316,0.03196988748050938,0.032690078834460026,0.033452486914355144,0.03426540096688612,0.03513277183458663,0.036053861890235704,0.0370222985882914,0.03802495026210281,0.03904166686741905,0.040046354556272326,0.04101070302305274,0.04190840551020715,0.04272117056342049,0.043442120519218176,0.04407705995930756,0.044642588296004516,0.04516186195803947,0.045659649184333656,0.04615808516518634,0.04667393329447572,0.04721851053267823,0.04779781981867609,0.04841295526014752,0.04905961435957337,0.04972687431757038,0.05039649464787832,0.0510431630044793,0.051637988935213534,0.052152898019964526,0.05256735613887945,0.05256735613887945,0.05287280582779088,0.05307480040671622,0.05319147997613728,0.05324904325489109,0.053276092305752996,0.053298346164306926,0.053335362929437774,0.05340020763637975,0.05349996132244169,0.05363677342293598,0.05380800333638241,0.05400527522298538,0.05421373663786559,0.05441153111639091,0.054572690239910744,0.05467095229756174,0.0546860534337794,0.05460833165062578,0.05444123725886871,0.05420031056256044,0.05390905151466595,0.053593535157450585,0.053276978402034686,0.052976533632989514,0.052702772946058705,0.05246034721339658,0.052249287916819355,0.05206548143938518,0.0518999575652504,0.05173827147174755,0.05155960046167505,0.05133932674476828,0.051052502193260386,0.05067983675078904,0.05021251454150624,0.049655167847256605,0.04902548263411444,0.04835056074371139,0.04766167892670045,0.04698835652451208,0.046354408070511784,0.046354408070511784,0.04577630174653949,0.04526302394022354,0.044817023611087733,0.044434938283174,0.04410758841111757,0.043820221674280097,0.043552135659873516,0.04327930092771592,0.04297733655708331,0.04262647551492618,0.04221550908064804,0.041744205871408556,0.041222987765454744,0.04066997384984694,0.04010682480784574,0.03955394612047163,0.039027568546916254,0.03853864487782841,0.038093034999550766,0.03769249564419831,0.03733534789474403,0.037016334122843574,0.036726549521697655,0.03645253767366245,0.03617796654664658,0.035885525812791454,0.03556057877546711,0.035194176944379034,0.034784999304199565,0.03433921062386732,0.03386829097351755,0.03338600035562103,0.03290486292572768,0.032434278563136784,0.031980157054512466,0.0315455663588631,0.03113195006848298,0.03073988809826604,0.03036892694001185,0.03001720437040243,0.02968009470634244,0.0293508830195779,0.029021593291280116,0.028685420804349656,0.028338905550761656,0.02798352336838292,0.02762581860399333,0.027275996373342798,0.02694578936991708,0.02664572131421179,0.026383473425351075,0.026163207866134318,0.025985531522934417,0.02584789118800849,0.025744746513778703,0.025667309773336974,0.02560358532597334,0.02553805300749827,0.02545344531895867,0.025332838681182288,0.025163002600909378,0.02493722660944909,0.02465713313117916,0.02465713313117916,0.024332656694254683,0.023980147445451065,0.0236194329344454,0.02327013766154403,0.022948976874882402,0.02266806278702009,0.022434135290837733,0.022248568654871202,0.02210757923469621,0.022002370982645767,0.021919717420827448,0.02184235776334765,0.02175114299892158,0.021627422614976705,0.021456390376960875,0.021229872165822315,0.020948095056369895,0.020619723926240195,0.020260122722115095,0.019888563245955403,0.019524699744057523,0.019185826616417895,0.01888504888547719,0.018630362197030452,0.018424550660911376,0.018265408572130912,0.018146008438137982,0.01805540012242703,0.017979049195776753,0.01790074959511892,0.017804612045405274,0.017677799439646348,0.017512787673284822,0.017308794745896344,0.017071814144314086,0.016813221838601698,0.016547555188209988,0.016289651892708652,0.016052625268648195,0.015846114914282873,0.015675823965340632,0.015543254313655976,0.015445841527529225,0.015377080617273168,0.0153270416578888,0.015282679062710053,0.01522947846083759,0.015153212485704224,0.01504238023120919,0.014890264975221088,0.014696267137791623,0.01446599907540027,0.014210092862487473,0.013942227821339047,0.01367656277438944,0.0134258655687606,0.013199918464448339,0.013005079853857784,0.01284410082057997,0.012716334558582251,0.012617945168983657,0.012542392334530169,0.012480642023842456,0.012422386069849717,0.012357263446614558,0.012276606142083408,0.012174903553944209,0.012050754866207302,0.011906930382270718,0.01174951404485379,0.01158653584175429,0.011426185952711238,0.011275646432150654,0.011140106713598873,0.011022642834408897,0.010924181785620234,0.010843653346694335,0.010778023016955301,0.010722493429340663,0.010670435815467182,0.010614229077326915,0.010546163378923966,0.01045987157473635,0.010351608115527789,0.010221139955907678,0.010071888719930724,0.009910257292588806,0.009744458225951276,0.009582912498900975,0.009433110083814044,0.009300617041994294,0.00918880316402521,0.009098716493613835,0.009029178754268853,0.008976857604441906,0.00893653824320188,0.00890121347555681,0.008862940694382718,0.008813751907725221,0.008746976114752898,0.008658400977999494,0.008547065099916686,0.008415379791151851,0.008268529336733816,0.008113435417486672,0.007957346728503232,0.007806848321248644,0.007667022696075052,0.007541261840173856,0.0074312359675901,0.00733705542186829,0.007257407555164262,0.007189651979975219,0.007130034513311875,0.0070739729870522685,0.007016698074025499,0.0069540702492659565,0.0068833421946474915,0.006803702202046461,0.006716378240632926,0.006624256621152712,0.00653122465115785,0.006441236888328929,0.006357714115920995,0.006283027766486435,0.006218411411617455,0.006163944490115108,0.006118618559887922,0.006080352781872665,0.006045981915610349,0.006011387848318924,0.005971749974563989,0.0059221657457605645,0.005858461328755144,0.005777976536929481,0.005680140991759832,0.005566621536040529,0.005440979533989666,0.005308015020333699,0.005172819552863405,0.005040096221762746,0.004913590737496743,0.004795952522888257,0.0046887537108274745,0.004592648752096777,0.0045075167054532995,0.004432540973558218,0.004366308439823038,0.0043068819615913675,0.004252058225791959,0.0041997169376909475,0.004148160729969801,0.00409637595603842,0.0040440959259535275,0.003991640174067873,0.003939658845587035,0.0038887373304368434,0.003839255455772623,0.0037912866874395094,0.003744748855875184,0.003699548978476977,0.003655710312101618,0.0036133752750671065,0.0035726849853017963,0.003533648011091353,0.003495987685136595,0.0034591857566912327,0.0034226426831421727,0.0033858877301912726,0.0033487708735981136,0.0033115294064175087,0.0032746934599533796,0.0032389199570963295,0.0032047124263175544,0.0031723328506677376,0.003141733195444062,0.0031126777583895715,0.003084849078705845,0.003057935715482332,0.0030316236170000337,0.0030055021436329343,0.0029789798549489472,0.002951219394811376,0.0029212472141981133,0.002888179693833951,0.0028514887512073868,0.002811231614543226,0.00276813873049373,0.002723516958521119,0.0026790390772564736,0.002636389185831076,0.0025970363132192032,0.0025620242229457946,0.0025319270974484055,0.0025068444558375793,0.002486433219665966,0.0024699214543035877,0.0024561055039392854,0.0024433946116373243,0.0024298904064382273,0.0024136096053616937,0.0023927835327721145,0.002366155733320844,0.0023332093407512932,0.0022942365477808305,0.0022502227194509746,0.002202619512852632,0.002152997204095762,0.0021028240745340327,0.0020532904114357376,0.0020053049652032687,0.0019595523118401993,0.0019165879524806106,0.0018768978569619595,0.0018408952650846248,0.0018088893072130885,0.0017810058735738391,0.0017571744909077606,0.001737148935350411,0.001720548743681351,0.001706905088963595,0.0016956731108916246,0.001686201619291603,0.0016777115934235312,0.0016692477632031305,0.0016597624919418305,0.0016482183576545044,0.001633779532536701,0.0016159694565142512,0.0015947722165824826,0.0015706255623023416,0.0015443060564961054,0.0015167659739947542,0.0014889326212580117,0.0014615939362893522,0.0014353522414998405,0.001410641401241473,0.0013877827522127755,0.0013670312107148441,0.0013485827304296771,0.0013325664811834831,0.0013189795462709912,0.0013076933673468844,0.001298449445105421,0.0012909132946656461,0.0012847156301077892,0.001279479054116015,0.0012748057804392512,0.0012702343350695215,0.001265207955441195,0.0012590572205693539,0.001251064144239218,0.0012405793034723947,0.0012271560182255067,0.0012106655710687547,0.001191343994862246,0.0011697491773547373,0.0011466630592329617,0.001122919917624876,0.0010992996221941354,0.0010764331016331883,0.001054792872159371,0.0010347147165951714,0.001016440545096992,0.0010001468128316281,9.859451578128529E-4,9.738727395036694E-4,9.638618395921923E-4,9.557451022174613E-4,9.492786307516484E-4,9.441732041193569E-4,9.401229619841022E-4,9.36810487293539E-4,9.338837044013623E-4,9.309352463677548E-4,9.274653940001446E-4,9.229215899184563E-4,9.167548657890822E-4,9.085283128915119E-4,8.980153212954676E-4,8.852674003873185E-4,8.706197918563576E-4,8.546303580471455E-4,8.379820354193027E-4,8.213539807411547E-4,8.05333781053748E-4,7.903640172386457E-4,7.767272845609754E-4,7.645601640905086E-4,7.538726807184519E-4,7.44558558692398E-4,7.364094034479083E-4,7.291071908331904E-4,7.222661234655896E-4,7.154736145669665E-4,7.083653533687266E-4,7.006883942383298E-4,6.923445508190556E-4,6.833932702930786E-4,6.740131209389658E-4,6.644449543315562E-4,6.549188604318299E-4,6.456198262899711E-4,6.366739713589872E-4,6.281639836817599E-4,6.20154686318525E-4,6.127102008615423E-4,6.058892609224502E-4,5.997321595059339E-4,5.942143052071739E-4,5.89234527385519E-4,5.846008048075044E-4,5.800346439834704E-4,5.751799749906807E-4,5.696049885860542E-4,5.627917602636004E-4,5.541194962432877E-4,5.428658817960088E-4,5.282300657430616E-4,5.09418901837203E-4,4.857706806310828E-4,4.569054744860441E-4,8.485394007188908E-4,0.0010458413159850652,0.0012906987964653835,0.0015878493396602443,0.0019322223516338854,0.0023073937727796172,0.0026884397536321137,0.0030497308450023438,0.003374559450894583,0.003662799154077177,0.003932327414935483,0.004212489482401898,0.004531698398288452,0.004904771598444972,0.005326359049739365,0.00577509964935249,0.006227046781010997,0.006672312691941033,0.007125694554443341,0.007624115760562379,0.008209987979574118,0.008907410647068716,0.009703470570214616,0.01054541223546289,0.011359118507370957,0.012081170117754409,0.012690360495322345,0.01322173339764445,0.013753774025105343,0.014372561555861526,0.01512953493388886,0.016014128711450405,0.016957938916025143,0.017870042735636926,0.018687646363568667,0.019415727531273824,0.020133073905176366,0.020958331369857135,0.021990975692981227,0.02325759252808953,0.0246926486331597,0.026169681884657382,0.0275685366414201,0.028846202021423777,0.030071341340530407,0.03139877939816141,0.03299058418405795,0.03492049611027809,0.03711263381219064,0.03934899830591225,0.041355444209784056,0.04292359417234579,0.044013152998622,0.04477922297390362,0.045505804541544605,0.046474041506823674,0.04782982816084711,0.049517692667850936,0.05131984520782182,0.05298093894784141,0.054356279812697354,0.05550346868477206,0.05666397405787462,0.058137270571170296,0.06010824941701415,0.06251891324005372,0.06505326974076536,0.06725921206111607,0.06874698877005138,0.06937234371758208,0.06930962342887628,0.0689745898594961,0.06883187773924376,0.069181936018593,0.07003298604456507,0.07003298604456507,0.07112389206836618,0.07208339272491798,0.07264272753554736,0.07278945879140811,0.07278019175269253,0.07300330455765078,0.07376183338328053,0.07509178443264682,0.07670955942199982,0.07812854014715236,0.07888231547815101,0.07874803316172659,0.07785502720708705,0.0766230583335953,0.07556078329964569,0.07502633306696445,0.07506981476484464,0.07543592782221811,0.0757211730259706,0.07560066548075361,0.07500559510484008,0.07415883640991548,0.07345113901934168,0.07322352299817661,0.07357196185763988,0.07427294666313905,0.07487773703718052,0.07492121935688861,0.07492121935688861,0.0741442935434278,0.0726164221548936,0.07069816210578603,0.06886495366951315,0.06748294157958462,0.06664911229773529,0.06617239346798436,0.06569963885067259,0.06491509868942232,0.06370912499951817,0.06223043166773727,0.0607992178664447,0.05973066074701026,0.05916461659398444,0.0589860443688559,0.05888240315158695,0.05849955320931719,0.05761811366684177,0.05625877793890129,0.05466492208327319,0.053174609168199174,0.05204998077717291,0.05135153121469202,0.05091841686619985,0.050461951753086154,0.04971906098383074,0.048586366307686445,0.04716738670352366,0.04571239532072649,0.044485706329807986,0.04363204008100334,0.04310665547567361,0.04270764739808601,0.04218472354948081,0.041368884962650496,0.04025526084539027,0.03899889080098019,0.0378291483786862,0.03692973654284083,0.03692973654284083,0.03634692970549332,0.035971055012056344,0.0355988683304777,0.03504051638271523,0.03421551663629578,0.033189127765230725,0.032133047163605925,0.031233441273136642,0.03059623794037738,0.03019537715962305,0.029892738759082004,0.02951267035578609,0.028933100641987973,0.028145513417854696,0.02725463338262858,0.026420566583854374,0.02577535985243192,0.025357721692713008,0.02509795013902178,0.024859486169448934,0.024512438150551166,0.024000527468151463,0.023367115590585692,0.02272846028574345,0.022209817354832382,0.021879346299426287,0.021712041354857638,0.021604561273002636,0.021428841159034116,0.021097584081461323,0.020607288500766475,0.020037639263742406,0.019509079181716203,0.019121731401159137,0.018907626461478616,0.018819857316306886,0.018763337218430643,0.018648846773450058,0.018441519432772096,0.018178073732142946,0.01794416612738398,0.017824294153350417,0.017851565196552618,0.017982644050406484,0.018113643340537087,0.01812698582006061,0.017946716926004302,0.01757394785528027,0.017084861343870262,0.016592836024813547,0.016194243938911376,0.015925190352818218,0.01574980826232492,0.01558495304094279,0.015346688649288183,0.014994456703149755,0.014550680934786482,0.014087072906906754,0.013686134422140823,0.013399018632028295,0.013220373306228392,0.013094027081094492,0.012943722723532903,0.012713060558542372,0.012393585952774698,0.012027155306302804,0.011682404987525296,0.011418406285158547,0.011254719487953075,0.011162679567184485,0.011082088286910433,0.010953288085253256,0.010747840202663091,0.010481972457838428,0.010206435699570244,0.009978745630611388,0.009832685318799798,0.009760777289171702,0.009718487924661021,0.009647400231158867,0.009504918740527111,0.009285243028873444,0.009021184624065832,0.008766534999895418,0.00856861058405323,0.008445327272609857,0.008378062079387214,0.008323657786936544,0.008238268097163397,0.00810049322036879,0.007921789266298473,0.007739145077694909,0.007594331457712104,0.007510874412419926,0.007480726659522008,0.00746744740700601,0.007424008247116352,0.007315841557740031,0.007137424592867539,0.006914213090299867,0.00668941285175755,0.00650273539094545,0.006372101208585616,0.006287093030228656,0.006217118139860457,0.006129070451186273,0.0060051498999523336,0.005851635268517669,0.0056944897199979385,0.005564619771693265,0.005480857464443239,0.005439553575589531,0.005416102451610958,0.005377314384297853,0.005298017638168492,0.00517337484635738,0.005020809545480031,0.004870956276957328,0.004752653702707515,0.004679882082192269,0.004647029282792316,0.004634692073148721,0.004622153249991416,0.004599657473888779,0.004573719246289286,0.004562574525930524,0.004584210461429308,0.004643433814392926,0.004725032363150225,0.004797034769246722,0.00482292173270141,0.004776937492268194,0.004655123793479344,0.0044767582556195454,0.004275650710055801,0.004085673086235601,0.00392753889235056,0.003802743131240737,0.003697064570537123,0.0035908829085319217,0.003470742951852758,0.00333632471737401,0.0031997764971523268,0.003078454980376234,0.002985400443144414,0.002922637594318762,0.0028807351047397344,0.0028444358926846074,0.0028011093650510996,0.0027474679302574756,0.0026911175074720265,0.002646406290298269,0.0026270882023002287,0.0026399649804433795,0.0026828556752097996,0.002748040300451218,0.0028288225603820766,0.002925249549978777,0.003045170574920651,0.0031992904407850908,0.003392259100249293,0.0036143929563702865,0.0038388498235517846,0.0040268106629860615,0.004139315832701405,0.004150917792496589,0.004059124401324867,0.003885301664570223,0.0036666023299929376,0.003442591816854662,0.0032425744186678623,0.0030787510898893974,0.002947654648131135,0.0028377723313926053,0.002738936616438584,0.0026485426740184064,0.0025718370098288213,0.0025168478152509245,0.0024873034591609408,0.0024777337627331117,0.00247357054270909,0.002456334677323729,0.002411240523512611,0.0023333995871810858,0.002229563769060236,0.0021146656402995178,0.002004986047562149,0.0019114041681365843,0.0018356631402150676,0.0017714309905975736,0.0017088841637109351,0.0016403995677011672,0.0015644248766366383,0.001485826524015287,0.001412925333083968,0.0013530698369510707,0.0013091650150373773,0.0012788365010670292,0.00125646494388418,0.0012367076927889303,0.001217499497798322,0.0012008779757720423,0.0011912338055840264,0.0011919928146400583,0.0012026262732115131,0.0012175530842364105,0.001227808325591355,0.0012245425166914523,0.0012027511294126977,0.0011633836350906006,0.0011128368255874678,0.0010601378050188945,0.0010132325263721431,9.761573631205901E-4,9.483205176565898E-4,9.260421051265897E-4,9.053215269525538E-4,8.843131941154461E-4,8.642352173165938E-4,8.483432465432113E-4,8.396413484783876E-4,8.386693248888415E-4,8.425460868068927E-4,8.459080556378167E-4,8.431852579329973E-4,8.311019085370865E-4,8.101115914397626E-4,7.840510815843178E-4,7.582066870749223E-4,7.367785949786966E-4,7.20999031787194E-4,7.087820984528507E-4,6.96019361486289E-4,6.787774712292533E-4,6.552884201152196E-4,6.267756160355047E-4,5.968000316349181E-4,5.695621682649761E-4,5.48098573121079E-4,5.332282870157477E-4,5.237701184889866E-4,5.177150966207518E-4,5.136631676992737E-4,5.116818389623686E-4,5.131100555542495E-4,5.19426627506328E-4,5.308274401830963E-4,5.453313558692552E-4,5.589686734156022E-4,5.670655457756545E-4,5.660342354013403E-4,5.548278396529791E-4,5.353573887641501E-4,5.116789125953314E-4,4.883533894203284E-4,4.687790528738803E-4,4.5422230591827986E-4,4.4397499862777024E-4,4.3638089820141786E-4,4.30132154956016E-4,4.25106922859563E-4,4.2231388366922157E-4,4.2300863694249533E-4,4.2749729984472955E-4,4.3431270836533206E-4,4.4025313136374214E-4,4.413424304520745E-4,4.342811077597691E-4,4.177259381682856E-4,3.928149117250312E-4,3.627255689754768E-4,3.31507934416705E-4,3.027572853758617E-4,2.7866553249384984E-4,2.5983696451824305E-4,2.457549676723394E-4,2.3556860845364122E-4,2.2873415522019872E-4,2.252021082781779E-4,2.2512773920565926E-4,2.2835421803008328E-4,2.3403173490465872E-4,2.4064105609501567E-4,2.464608929014493E-4,2.502432378924959E-4,2.517416259304991E-4,2.517905052278335E-4,2.518616341264005E-4,2.532963230839581E-4,2.566008145497089E-4,2.6113748699442145E-4,2.6541706881749564E-4,2.678131650486268E-4,2.673629967775918E-4,2.6424285821986235E-4,2.5967671729574155E-4,2.5532322615928196E-4,2.5245074618966115E-4,2.5131486226916377E-4,2.510457800477182E-4,2.500972994459841E-4,2.4701966573128543E-4,2.411695575312557E-4,2.3300978754534283E-4,2.2386768455266378E-4,2.152936836266507E-4,2.083586901355994E-4,2.0322212335299516E-4,1.9918706280788742E-4,1.9515988812361036E-4,1.9026802095910918E-4,1.8431019129860174E-4,1.7782216102079688E-4,1.7174882756542352E-4,1.6691464096897865E-4,1.6357843390607348E-4,1.6129445536597044E-4,1.591465510075506E-4,1.5621271809181773E-4,1.5201822693484282E-4,1.467468625217583E-4,1.41114185506274E-4,1.359812987561514E-4,1.319206554863163E-4,1.2893873573279242E-4,1.2650958333565546E-4,1.238638159049094E-4,1.2038777119027094E-4,1.1592578163379679E-4,1.1084416422045598E-4,1.0584389047184032E-4,1.0163567905648029E-4,9.865222404061254E-5,9.693613674592091E-5,9.624678509479171E-5,9.629968612002499E-5,9.699183354202717E-5,9.847583963457009E-5,1.0103466030265904E-4,1.0482356340726585E-4,1.0962652657106832E-4,1.147759270269377E-4,1.1930477888769953E-4,1.2228105098996105E-4,1.231697126829986E-4,1.2204611703924908E-4,1.1954836024243044E-4,1.1658177645609735E-4,1.1391023270847633E-4,1.1182455091073983E-4,1.100387472432702E-4,1.0785774760082723E-4,1.0452556306434526E-4,9.958843582908426E-5,9.310928232971968E-5,8.565204924750351E-5,7.806801791169707E-5,7.120303031187905E-5,6.566563408630483E-5,6.174423716420422E-5,5.947730821012434E-5,5.8797600373147023E-5,5.964503411901218E-5,6.197202909194525E-5,6.563639654563069E-5,7.024903569373631E-5,7.508059777402354E-5,7.91046870020621E-5,8.119995088922819E-5,8.044260707811255E-5],\n",
       "\"..quantile..\":[0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0],\n",
       "\"words\":[6.0,6.117416829745597,6.234833659491194,6.35225048923679,6.4696673189823874,6.5870841487279845,6.704500978473581,6.821917808219178,6.939334637964775,7.056751467710372,7.174168297455969,7.291585127201565,7.409001956947162,7.526418786692759,7.643835616438356,7.761252446183953,7.87866927592955,7.996086105675147,8.113502935420744,8.230919765166341,8.348336594911938,8.465753424657535,8.58317025440313,8.700587084148728,8.818003913894325,8.935420743639922,9.052837573385519,9.170254403131114,9.287671232876711,9.405088062622308,9.522504892367905,9.639921722113503,9.7573385518591,9.874755381604697,9.992172211350294,10.10958904109589,10.227005870841488,10.344422700587085,10.46183953033268,10.579256360078277,10.696673189823874,10.814090019569472,10.931506849315069,11.048923679060664,11.166340508806261,11.283757338551858,11.401174168297455,11.518590998043052,11.63600782778865,11.753424657534246,11.870841487279844,11.98825831702544,12.105675146771038,12.223091976516635,12.34050880626223,12.457925636007827,12.575342465753424,12.692759295499021,12.810176125244618,12.927592954990214,13.04500978473581,13.162426614481408,13.279843444227005,13.397260273972602,13.5146771037182,13.632093933463796,13.749510763209393,13.86692759295499,13.984344422700588,14.101761252446183,14.21917808219178,14.336594911937377,14.454011741682974,14.571428571428571,14.688845401174168,14.806262230919765,14.92367906066536,15.041095890410958,15.158512720156555,15.275929549902152,15.393346379647749,15.510763209393346,15.628180039138943,15.74559686888454,15.863013698630137,15.980430528375733,15.980430528375733,16.097847358121328,16.21526418786693,16.332681017612522,16.450097847358123,16.567514677103716,16.684931506849317,16.80234833659491,16.919765166340508,17.037181996086105,17.1545988258317,17.2720156555773,17.389432485322896,17.506849315068493,17.62426614481409,17.741682974559687,17.85909980430528,17.97651663405088,18.093933463796475,18.211350293542075,18.32876712328767,18.44618395303327,18.563600782778863,18.68101761252446,18.798434442270057,18.915851272015654,19.03326810176125,19.15068493150685,19.268101761252446,19.385518590998043,19.50293542074364,19.620352250489237,19.737769080234834,19.855185909980428,19.972602739726028,20.09001956947162,20.207436399217222,20.324853228962816,20.442270058708417,20.55968688845401,20.677103718199607,20.794520547945204,20.794520547945204,20.9119373776908,21.0293542074364,21.146771037181995,21.264187866927593,21.38160469667319,21.499021526418787,21.61643835616438,21.73385518590998,21.851272015655574,21.968688845401175,22.08610567514677,22.203522504892366,22.320939334637963,22.43835616438356,22.555772994129157,22.673189823874754,22.79060665362035,22.908023483365948,23.025440313111545,23.142857142857142,23.26027397260274,23.377690802348337,23.495107632093934,23.61252446183953,23.729941291585128,23.84735812133072,23.96477495107632,24.082191780821915,24.199608610567513,24.31702544031311,24.434442270058707,24.551859099804304,24.6692759295499,24.786692759295498,24.904109589041095,25.021526418786692,25.13894324853229,25.256360078277886,25.373776908023483,25.49119373776908,25.608610567514678,25.726027397260275,25.843444227005868,25.960861056751465,26.078277886497062,26.19569471624266,26.313111545988257,26.430528375733854,26.54794520547945,26.665362035225048,26.782778864970645,26.900195694716242,27.01761252446184,27.135029354207436,27.252446183953033,27.36986301369863,27.487279843444227,27.60469667318982,27.722113502935418,27.839530332681015,27.956947162426612,28.07436399217221,28.191780821917806,28.191780821917806,28.309197651663403,28.426614481409,28.544031311154598,28.661448140900195,28.77886497064579,28.89628180039139,29.013698630136986,29.131115459882583,29.24853228962818,29.365949119373777,29.483365949119374,29.600782778864968,29.718199608610565,29.835616438356162,29.95303326810176,30.070450097847356,30.187866927592953,30.30528375733855,30.422700587084147,30.540117416829744,30.65753424657534,30.77495107632094,30.892367906066536,31.009784735812133,31.12720156555773,31.244618395303327,31.36203522504892,31.479452054794518,31.596868884540115,31.71428571428571,31.83170254403131,31.949119373776906,32.0665362035225,32.1839530332681,32.3013698630137,32.4187866927593,32.53620352250489,32.653620352250485,32.771037181996086,32.888454011741686,33.00587084148728,33.12328767123287,33.240704500978474,33.35812133072407,33.47553816046967,33.59295499021526,33.710371819960855,33.827788649706456,33.945205479452056,34.06262230919765,34.18003913894324,34.297455968688844,34.414872798434445,34.53228962818004,34.64970645792563,34.76712328767123,34.88454011741683,35.00195694716243,35.11937377690802,35.236790606653614,35.354207436399214,35.471624266144815,35.58904109589041,35.706457925636,35.8238747553816,35.9412915851272,36.0587084148728,36.17612524461839,36.29354207436399,36.41095890410959,36.528375733855185,36.64579256360078,36.76320939334638,36.88062622309198,36.99804305283757,37.11545988258317,37.23287671232876,37.35029354207436,37.46771037181996,37.585127201565555,37.70254403131115,37.81996086105675,37.93737769080235,38.054794520547944,38.17221135029354,38.28962818003914,38.40704500978473,38.52446183953033,38.641878669275926,38.759295499021526,38.87671232876712,38.99412915851272,39.111545988258314,39.228962818003914,39.34637964774951,39.46379647749511,39.5812133072407,39.6986301369863,39.816046966731896,39.93346379647749,40.05088062622309,40.168297455968684,40.285714285714285,40.40313111545988,40.52054794520548,40.63796477495107,40.75538160469667,40.87279843444227,40.99021526418787,41.10763209393346,41.22504892367906,41.342465753424655,41.459882583170256,41.57729941291585,41.69471624266144,41.81213307240704,41.92954990215264,42.04696673189824,42.16438356164383,42.28180039138943,42.399217221135025,42.516634050880626,42.63405088062622,42.75146771037182,42.86888454011741,42.986301369863014,43.10371819960861,43.22113502935421,43.3385518590998,43.4559686888454,43.573385518590996,43.69080234833659,43.80821917808219,43.925636007827784,44.043052837573384,44.16046966731898,44.27788649706458,44.39530332681017,44.51272015655577,44.630136986301366,44.74755381604697,44.86497064579256,44.98238747553816,45.099804305283755,45.217221135029355,45.33463796477495,45.45205479452055,45.56947162426614,45.686888454011736,45.80430528375734,45.92172211350293,46.03913894324853,46.156555772994125,46.273972602739725,46.39138943248532,46.50880626223092,46.62622309197651,46.743639921722114,46.86105675146771,46.97847358121331,47.0958904109589,47.2133072407045,47.330724070450096,47.44814090019569,47.56555772994129,47.68297455968688,47.800391389432484,47.91780821917808,48.03522504892368,48.15264187866927,48.27005870841487,48.387475538160466,48.504892367906066,48.62230919765166,48.73972602739726,48.857142857142854,48.974559686888455,49.09197651663405,49.20939334637964,49.32681017612524,49.444227005870836,49.56164383561644,49.67906066536203,49.79647749510763,49.913894324853224,50.031311154598825,50.14872798434442,50.26614481409002,50.38356164383561,50.50097847358121,50.61839530332681,50.73581213307241,50.853228962818,50.9706457925636,51.088062622309195,51.20547945205479,51.32289628180039,51.44031311154598,51.55772994129158,51.67514677103718,51.79256360078278,51.90998043052837,52.02739726027397,52.144814090019565,52.262230919765166,52.37964774951076,52.49706457925636,52.614481409001954,52.731898238747554,52.84931506849315,52.96673189823875,53.08414872798434,53.201565557729936,53.318982387475536,53.43639921722113,53.55381604696673,53.671232876712324,53.788649706457925,53.90606653620352,54.02348336594912,54.14090019569471,54.25831702544031,54.375733855185906,54.49315068493151,54.6105675146771,54.7279843444227,54.845401174168295,54.96281800391389,55.08023483365949,55.19765166340508,55.31506849315068,55.43248532289628,55.54990215264188,55.66731898238747,55.78473581213307,55.902152641878665,56.019569471624266,56.13698630136986,56.25440313111546,56.37181996086105,56.489236790606654,56.60665362035225,56.72407045009784,56.84148727984344,56.958904109589035,57.076320939334636,57.19373776908023,57.31115459882583,57.42857142857142,57.545988258317024,57.66340508806262,57.78082191780822,57.89823874755381,58.01565557729941,58.133072407045006,58.25048923679061,58.3679060665362,58.4853228962818,58.602739726027394,58.72015655577299,58.83757338551859,58.95499021526418,59.07240704500978,59.189823874755376,59.30724070450098,59.42465753424657,59.54207436399217,59.659491193737765,59.776908023483365,59.89432485322896,60.01174168297456,60.12915851272015,60.24657534246575,60.36399217221135,60.48140900195695,60.59882583170254,60.716242661448135,60.833659491193735,60.95107632093933,61.06849315068493,61.18590998043052,61.303326810176124,61.42074363992172,61.53816046966732,61.65557729941291,61.77299412915851,61.890410958904106,62.007827788649706,62.1252446183953,62.2426614481409,62.360078277886494,62.47749510763209,62.59491193737769,62.71232876712328,62.82974559686888,62.947162426614476,63.064579256360076,63.18199608610567,63.29941291585127,63.416829745596864,63.534246575342465,63.65166340508806,63.76908023483366,63.88649706457925,64.00391389432485,64.12133072407045,64.23874755381604,64.35616438356163,64.47358121330723,64.59099804305283,64.70841487279843,64.82583170254404,64.94324853228963,65.06066536203522,65.17808219178082,65.29549902152641,65.412915851272,65.53033268101761,65.6477495107632,65.76516634050881,65.8825831702544,66.0,6.0,6.117416829745597,6.234833659491194,6.35225048923679,6.4696673189823874,6.5870841487279845,6.704500978473581,6.821917808219178,6.939334637964775,7.056751467710372,7.174168297455969,7.291585127201565,7.409001956947162,7.526418786692759,7.643835616438356,7.761252446183953,7.87866927592955,7.996086105675147,8.113502935420744,8.230919765166341,8.348336594911938,8.465753424657535,8.58317025440313,8.700587084148728,8.818003913894325,8.935420743639922,9.052837573385519,9.170254403131114,9.287671232876711,9.405088062622308,9.522504892367905,9.639921722113503,9.7573385518591,9.874755381604697,9.992172211350294,10.10958904109589,10.227005870841488,10.344422700587085,10.46183953033268,10.579256360078277,10.696673189823874,10.814090019569472,10.931506849315069,11.048923679060664,11.166340508806261,11.283757338551858,11.401174168297455,11.518590998043052,11.63600782778865,11.753424657534246,11.870841487279844,11.98825831702544,12.105675146771038,12.223091976516635,12.34050880626223,12.457925636007827,12.575342465753424,12.692759295499021,12.810176125244618,12.927592954990214,13.04500978473581,13.162426614481408,13.279843444227005,13.397260273972602,13.5146771037182,13.632093933463796,13.749510763209393,13.86692759295499,13.984344422700588,14.101761252446183,14.21917808219178,14.336594911937377,14.454011741682974,14.571428571428571,14.688845401174168,14.688845401174168,14.806262230919765,14.92367906066536,15.041095890410958,15.158512720156555,15.275929549902152,15.393346379647749,15.510763209393346,15.628180039138943,15.74559686888454,15.863013698630137,15.980430528375733,16.097847358121328,16.21526418786693,16.332681017612522,16.450097847358123,16.567514677103716,16.684931506849317,16.80234833659491,16.919765166340508,17.037181996086105,17.1545988258317,17.2720156555773,17.389432485322896,17.506849315068493,17.62426614481409,17.741682974559687,17.85909980430528,17.97651663405088,17.97651663405088,18.093933463796475,18.211350293542075,18.32876712328767,18.44618395303327,18.563600782778863,18.68101761252446,18.798434442270057,18.915851272015654,19.03326810176125,19.15068493150685,19.268101761252446,19.385518590998043,19.50293542074364,19.620352250489237,19.737769080234834,19.855185909980428,19.972602739726028,20.09001956947162,20.207436399217222,20.324853228962816,20.442270058708417,20.55968688845401,20.677103718199607,20.794520547945204,20.9119373776908,21.0293542074364,21.146771037181995,21.264187866927593,21.38160469667319,21.499021526418787,21.61643835616438,21.73385518590998,21.851272015655574,21.968688845401175,22.08610567514677,22.203522504892366,22.320939334637963,22.43835616438356,22.555772994129157,22.555772994129157,22.673189823874754,22.79060665362035,22.908023483365948,23.025440313111545,23.142857142857142,23.26027397260274,23.377690802348337,23.495107632093934,23.61252446183953,23.729941291585128,23.84735812133072,23.96477495107632,24.082191780821915,24.199608610567513,24.31702544031311,24.434442270058707,24.551859099804304,24.6692759295499,24.786692759295498,24.904109589041095,25.021526418786692,25.13894324853229,25.256360078277886,25.373776908023483,25.49119373776908,25.608610567514678,25.726027397260275,25.843444227005868,25.960861056751465,26.078277886497062,26.19569471624266,26.313111545988257,26.430528375733854,26.54794520547945,26.665362035225048,26.782778864970645,26.900195694716242,27.01761252446184,27.135029354207436,27.252446183953033,27.36986301369863,27.487279843444227,27.60469667318982,27.722113502935418,27.839530332681015,27.956947162426612,28.07436399217221,28.191780821917806,28.309197651663403,28.426614481409,28.544031311154598,28.661448140900195,28.77886497064579,28.89628180039139,29.013698630136986,29.131115459882583,29.24853228962818,29.365949119373777,29.483365949119374,29.600782778864968,29.718199608610565,29.835616438356162,29.95303326810176,30.070450097847356,30.187866927592953,30.30528375733855,30.422700587084147,30.540117416829744,30.65753424657534,30.77495107632094,30.892367906066536,31.009784735812133,31.12720156555773,31.244618395303327,31.36203522504892,31.479452054794518,31.596868884540115,31.71428571428571,31.83170254403131,31.949119373776906,32.0665362035225,32.1839530332681,32.3013698630137,32.4187866927593,32.53620352250489,32.653620352250485,32.771037181996086,32.888454011741686,33.00587084148728,33.12328767123287,33.240704500978474,33.35812133072407,33.47553816046967,33.59295499021526,33.710371819960855,33.827788649706456,33.945205479452056,34.06262230919765,34.18003913894324,34.297455968688844,34.414872798434445,34.53228962818004,34.64970645792563,34.76712328767123,34.88454011741683,35.00195694716243,35.11937377690802,35.236790606653614,35.354207436399214,35.471624266144815,35.58904109589041,35.706457925636,35.8238747553816,35.9412915851272,36.0587084148728,36.17612524461839,36.29354207436399,36.41095890410959,36.528375733855185,36.64579256360078,36.76320939334638,36.88062622309198,36.99804305283757,37.11545988258317,37.23287671232876,37.35029354207436,37.46771037181996,37.585127201565555,37.70254403131115,37.81996086105675,37.93737769080235,38.054794520547944,38.17221135029354,38.28962818003914,38.40704500978473,38.52446183953033,38.641878669275926,38.759295499021526,38.87671232876712,38.99412915851272,39.111545988258314,39.228962818003914,39.34637964774951,39.46379647749511,39.5812133072407,39.6986301369863,39.816046966731896,39.93346379647749,40.05088062622309,40.168297455968684,40.285714285714285,40.40313111545988,40.52054794520548,40.63796477495107,40.75538160469667,40.87279843444227,40.99021526418787,41.10763209393346,41.22504892367906,41.342465753424655,41.459882583170256,41.57729941291585,41.69471624266144,41.81213307240704,41.92954990215264,42.04696673189824,42.16438356164383,42.28180039138943,42.399217221135025,42.516634050880626,42.63405088062622,42.75146771037182,42.86888454011741,42.986301369863014,43.10371819960861,43.22113502935421,43.3385518590998,43.4559686888454,43.573385518590996,43.69080234833659,43.80821917808219,43.925636007827784,44.043052837573384,44.16046966731898,44.27788649706458,44.39530332681017,44.51272015655577,44.630136986301366,44.74755381604697,44.86497064579256,44.98238747553816,45.099804305283755,45.217221135029355,45.33463796477495,45.45205479452055,45.56947162426614,45.686888454011736,45.80430528375734,45.92172211350293,46.03913894324853,46.156555772994125,46.273972602739725,46.39138943248532,46.50880626223092,46.62622309197651,46.743639921722114,46.86105675146771,46.97847358121331,47.0958904109589,47.2133072407045,47.330724070450096,47.44814090019569,47.56555772994129,47.68297455968688,47.800391389432484,47.91780821917808,48.03522504892368,48.15264187866927,48.27005870841487,48.387475538160466,48.504892367906066,48.62230919765166,48.73972602739726,48.857142857142854,48.974559686888455,49.09197651663405,49.20939334637964,49.32681017612524,49.444227005870836,49.56164383561644,49.67906066536203,49.79647749510763,49.913894324853224,50.031311154598825,50.14872798434442,50.26614481409002,50.38356164383561,50.50097847358121,50.61839530332681,50.73581213307241,50.853228962818,50.9706457925636,51.088062622309195,51.20547945205479,51.32289628180039,51.44031311154598,51.55772994129158,51.67514677103718,51.79256360078278,51.90998043052837,52.02739726027397,52.144814090019565,52.262230919765166,52.37964774951076,52.49706457925636,52.614481409001954,52.731898238747554,52.84931506849315,52.96673189823875,53.08414872798434,53.201565557729936,53.318982387475536,53.43639921722113,53.55381604696673,53.671232876712324,53.788649706457925,53.90606653620352,54.02348336594912,54.14090019569471,54.25831702544031,54.375733855185906,54.49315068493151,54.6105675146771,54.7279843444227,54.845401174168295,54.96281800391389,55.08023483365949,55.19765166340508,55.31506849315068,55.43248532289628,55.54990215264188,55.66731898238747,55.78473581213307,55.902152641878665,56.019569471624266,56.13698630136986,56.25440313111546,56.37181996086105,56.489236790606654,56.60665362035225,56.72407045009784,56.84148727984344,56.958904109589035,57.076320939334636,57.19373776908023,57.31115459882583,57.42857142857142,57.545988258317024,57.66340508806262,57.78082191780822,57.89823874755381,58.01565557729941,58.133072407045006,58.25048923679061,58.3679060665362,58.4853228962818,58.602739726027394,58.72015655577299,58.83757338551859,58.95499021526418,59.07240704500978,59.189823874755376,59.30724070450098,59.42465753424657,59.54207436399217,59.659491193737765,59.776908023483365,59.89432485322896,60.01174168297456,60.12915851272015,60.24657534246575,60.36399217221135,60.48140900195695,60.59882583170254,60.716242661448135,60.833659491193735,60.95107632093933,61.06849315068493,61.18590998043052,61.303326810176124,61.42074363992172,61.53816046966732,61.65557729941291,61.77299412915851,61.890410958904106,62.007827788649706,62.1252446183953,62.2426614481409,62.360078277886494,62.47749510763209,62.59491193737769,62.71232876712328,62.82974559686888,62.947162426614476,63.064579256360076,63.18199608610567,63.29941291585127,63.416829745596864,63.534246575342465,63.65166340508806,63.76908023483366,63.88649706457925,64.00391389432485,64.12133072407045,64.23874755381604,64.35616438356163,64.47358121330723,64.59099804305283,64.70841487279843,64.82583170254404,64.94324853228963,65.06066536203522,65.17808219178082,65.29549902152641,65.412915851272,65.53033268101761,65.6477495107632,65.76516634050881,65.8825831702544,66.0],\n",
       "\"is_duplicate\":[\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"no\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\",\"yes\"]\n",
       "}\n",
       "}],\n",
       "\"metainfo_list\":[]\n",
       "};\n",
       "           var plotContainer = document.getElementById(\"TiG8E1\");\n",
       "           window.letsPlotCall(function() {{\n",
       "               LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);\n",
       "           }});\n",
       "       })();    \n",
       "   </script>"
      ],
      "text/plain": [
       "<lets_plot.plot.core.PlotSpec at 0x298de41c0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot class distribution for each class using ggplot2\n",
    "x = df['words']\n",
    "ggplot(df, aes(x='words', fill='is_duplicate')) + ggsize(700, 400) + \\\n",
    "geom_density(color='dark_green', alpha=.7) + scale_fill_discrete(guide='none') + \\\n",
    "theme_classic() + \\\n",
    "flavor_high_contrast_dark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_duplicate'] = df['is_duplicate'].apply(lambda x: 1 if x == 'yes' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>characters</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>124</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "      <td>116</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "      <td>116</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question1  \\\n",
       "0  What is the step by step guide to invest in sh...   \n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2  How can I increase the speed of my internet co...   \n",
       "3  Why am I mentally very lonely? How can I solve...   \n",
       "4  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \\\n",
       "0  What is the step by step guide to invest in sh...             0   \n",
       "1  What would happen if the Indian government sto...             0   \n",
       "2  How can Internet speed be increased by hacking...             0   \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0   \n",
       "4            Which fish would survive in salt water?             0   \n",
       "\n",
       "   characters  words  \n",
       "0         124     26  \n",
       "1         140     21  \n",
       "2         133     24  \n",
       "3         116     20  \n",
       "4         116     20  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill the missing values with empty strings\n",
    "df = df.fillna(' ')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404285</th>\n",
       "      <td>How many keywords are there in the Racket prog...</td>\n",
       "      <td>How many keywords are there in PERL Programmin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404286</th>\n",
       "      <td>Do you believe there is life after death?</td>\n",
       "      <td>Is it true that there is life after death?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404287</th>\n",
       "      <td>What is one coin?</td>\n",
       "      <td>What's this coin?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404288</th>\n",
       "      <td>What is the approx annual cost of living while...</td>\n",
       "      <td>I am having little hairfall problem but I want...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404289</th>\n",
       "      <td>What is like to have sex with cousin?</td>\n",
       "      <td>What is it like to have sex with your cousin?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>402691 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                question1  \\\n",
       "0       What is the step by step guide to invest in sh...   \n",
       "1       What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2       How can I increase the speed of my internet co...   \n",
       "3       Why am I mentally very lonely? How can I solve...   \n",
       "4       Which one dissolve in water quikly sugar, salt...   \n",
       "...                                                   ...   \n",
       "404285  How many keywords are there in the Racket prog...   \n",
       "404286          Do you believe there is life after death?   \n",
       "404287                                  What is one coin?   \n",
       "404288  What is the approx annual cost of living while...   \n",
       "404289              What is like to have sex with cousin?   \n",
       "\n",
       "                                                question2  is_duplicate  \n",
       "0       What is the step by step guide to invest in sh...             0  \n",
       "1       What would happen if the Indian government sto...             0  \n",
       "2       How can Internet speed be increased by hacking...             0  \n",
       "3       Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4                 Which fish would survive in salt water?             0  \n",
       "...                                                   ...           ...  \n",
       "404285  How many keywords are there in PERL Programmin...             0  \n",
       "404286         Is it true that there is life after death?             1  \n",
       "404287                                  What's this coin?             0  \n",
       "404288  I am having little hairfall problem but I want...             0  \n",
       "404289      What is it like to have sex with your cousin?             0  \n",
       "\n",
       "[402691 rows x 3 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop characters and words columns\n",
    "df.drop(['characters', 'words'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning\n",
    "\n",
    "- Tokenization\n",
    "- Stopwords cleaning\n",
    "- Removing punctuation\n",
    "- Normalizing\n",
    "- Stemming"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #regular expression\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "import string\n",
    "string.punctuation\n",
    "\n",
    "def remove_punct(text):\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['q1_clean'] = df['question1'].apply(lambda x: remove_punct(x))\n",
    "df['q2_clean'] = df['question2'].apply(lambda x: remove_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['question1', 'question2'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Alpha Numeric\n",
    "def remove_alphanumeric(text):\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply function to data frame\n",
    "df['q1_clean'] = df['q1_clean'].apply(lambda x: remove_alphanumeric(x))\n",
    "df['q2_clean'] = df['q2_clean'].apply(lambda x: remove_alphanumeric(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/patrickokwir/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Remove Stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text.split() if word.lower() not in (stop)]\n",
    "    return \" \".join(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>characters</th>\n",
       "      <th>words</th>\n",
       "      <th>q1_clean</th>\n",
       "      <th>q2_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>124</td>\n",
       "      <td>26</td>\n",
       "      <td>step step guide invest share market india</td>\n",
       "      <td>step step guide invest share market</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>21</td>\n",
       "      <td>story Kohinoor KohiNoor Diamond</td>\n",
       "      <td>would happen Indian government stole Kohinoor ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "      <td>24</td>\n",
       "      <td>increase speed internet connection using VPN</td>\n",
       "      <td>Internet speed increased hacking DNS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_duplicate  characters  words  \\\n",
       "0             0         124     26   \n",
       "1             0         140     21   \n",
       "2             0         133     24   \n",
       "\n",
       "                                       q1_clean  \\\n",
       "0     step step guide invest share market india   \n",
       "1               story Kohinoor KohiNoor Diamond   \n",
       "2  increase speed internet connection using VPN   \n",
       "\n",
       "                                            q2_clean  \n",
       "0                step step guide invest share market  \n",
       "1  would happen Indian government stole Kohinoor ...  \n",
       "2               Internet speed increased hacking DNS  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stopwords from questions\n",
    "df['q1_clean'] = df['q1_clean'].apply(lambda x: remove_stopwords(x))\n",
    "df['q2_clean'] = df['q2_clean'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/patrickokwir/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# remove unwanted spaces and convert to lower case\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "# define class to remove unwanted spaces and convert to lower case\n",
    "class CleanText(object):\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        \n",
    "    def clean(self):\n",
    "        # remove unwanted spaces and convert to lower case\n",
    "        self.text = self.text.lower()\n",
    "        self.text = self.text.strip()\n",
    "        return self.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>characters</th>\n",
       "      <th>words</th>\n",
       "      <th>q1_clean</th>\n",
       "      <th>q2_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>124</td>\n",
       "      <td>26</td>\n",
       "      <td>step step guide invest share market india</td>\n",
       "      <td>step step guide invest share market</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>21</td>\n",
       "      <td>story kohinoor kohinoor diamond</td>\n",
       "      <td>would happen indian government stole kohinoor ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "      <td>24</td>\n",
       "      <td>increase speed internet connection using vpn</td>\n",
       "      <td>internet speed increased hacking dns</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_duplicate  characters  words  \\\n",
       "0             0         124     26   \n",
       "1             0         140     21   \n",
       "2             0         133     24   \n",
       "\n",
       "                                       q1_clean  \\\n",
       "0     step step guide invest share market india   \n",
       "1               story kohinoor kohinoor diamond   \n",
       "2  increase speed internet connection using vpn   \n",
       "\n",
       "                                            q2_clean  \n",
       "0                step step guide invest share market  \n",
       "1  would happen indian government stole kohinoor ...  \n",
       "2               internet speed increased hacking dns  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call class to remove unwanted spaces and convert to lower case for q1_clean and q2_clean\n",
    "df['q1_clean'] = df['q1_clean'].apply(lambda x: CleanText(x).clean())\n",
    "df['q2_clean'] = df['q2_clean'].apply(lambda x: CleanText(x).clean())\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    " from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A = deepcopy(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>characters</th>\n",
       "      <th>words</th>\n",
       "      <th>q1_clean</th>\n",
       "      <th>q2_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>124</td>\n",
       "      <td>26</td>\n",
       "      <td>step step guide invest share market india</td>\n",
       "      <td>step step guide invest share market</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>21</td>\n",
       "      <td>story kohinoor kohinoor diamond</td>\n",
       "      <td>would happen indian government stole kohinoor ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "      <td>24</td>\n",
       "      <td>increase speed internet connection using vpn</td>\n",
       "      <td>internet speed increased hacking dns</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_duplicate  characters  words  \\\n",
       "0             0         124     26   \n",
       "1             0         140     21   \n",
       "2             0         133     24   \n",
       "\n",
       "                                       q1_clean  \\\n",
       "0     step step guide invest share market india   \n",
       "1               story kohinoor kohinoor diamond   \n",
       "2  increase speed internet connection using vpn   \n",
       "\n",
       "                                            q2_clean  \n",
       "0                step step guide invest share market  \n",
       "1  would happen indian government stole kohinoor ...  \n",
       "2               internet speed increased hacking dns  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_A.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine q1_clean and q2_clean into a single column called \"combined\" using string concatenation\n",
    "df_A['combined'] = df_A['q1_clean'].astype(str) +' ' + df_A['q2_clean'].astype(str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>characters</th>\n",
       "      <th>words</th>\n",
       "      <th>q1_clean</th>\n",
       "      <th>q2_clean</th>\n",
       "      <th>combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>124</td>\n",
       "      <td>26</td>\n",
       "      <td>step step guide invest share market india</td>\n",
       "      <td>step step guide invest share market</td>\n",
       "      <td>step step guide invest share market india step...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>21</td>\n",
       "      <td>story kohinoor kohinoor diamond</td>\n",
       "      <td>would happen indian government stole kohinoor ...</td>\n",
       "      <td>story kohinoor kohinoor diamond would happen i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "      <td>24</td>\n",
       "      <td>increase speed internet connection using vpn</td>\n",
       "      <td>internet speed increased hacking dns</td>\n",
       "      <td>increase speed internet connection using vpn i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_duplicate  characters  words  \\\n",
       "0             0         124     26   \n",
       "1             0         140     21   \n",
       "2             0         133     24   \n",
       "\n",
       "                                       q1_clean  \\\n",
       "0     step step guide invest share market india   \n",
       "1               story kohinoor kohinoor diamond   \n",
       "2  increase speed internet connection using vpn   \n",
       "\n",
       "                                            q2_clean  \\\n",
       "0                step step guide invest share market   \n",
       "1  would happen indian government stole kohinoor ...   \n",
       "2               internet speed increased hacking dns   \n",
       "\n",
       "                                            combined  \n",
       "0  step step guide invest share market india step...  \n",
       "1  story kohinoor kohinoor diamond would happen i...  \n",
       "2  increase speed internet connection using vpn i...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_A.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>characters</th>\n",
       "      <th>words</th>\n",
       "      <th>combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>124</td>\n",
       "      <td>26</td>\n",
       "      <td>step step guide invest share market india step...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>21</td>\n",
       "      <td>story kohinoor kohinoor diamond would happen i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "      <td>24</td>\n",
       "      <td>increase speed internet connection using vpn i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>116</td>\n",
       "      <td>20</td>\n",
       "      <td>mentally lonely solve find remainder mathmath ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>116</td>\n",
       "      <td>20</td>\n",
       "      <td>one dissolve water quikly sugar salt methane c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_duplicate  characters  words  \\\n",
       "0             0         124     26   \n",
       "1             0         140     21   \n",
       "2             0         133     24   \n",
       "3             0         116     20   \n",
       "4             0         116     20   \n",
       "\n",
       "                                            combined  \n",
       "0  step step guide invest share market india step...  \n",
       "1  story kohinoor kohinoor diamond would happen i...  \n",
       "2  increase speed internet connection using vpn i...  \n",
       "3  mentally lonely solve find remainder mathmath ...  \n",
       "4  one dissolve water quikly sugar salt methane c...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop q1_clean and q2_clean\n",
    "\n",
    "df_A.drop(['q1_clean', 'q2_clean'], axis=1, inplace=True)\n",
    "df_A.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A.to_csv('df_A.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>characters</th>\n",
       "      <th>words</th>\n",
       "      <th>q1_clean</th>\n",
       "      <th>q2_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>124</td>\n",
       "      <td>26</td>\n",
       "      <td>step step guide invest share market india</td>\n",
       "      <td>step step guide invest share market</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>21</td>\n",
       "      <td>story kohinoor kohinoor diamond</td>\n",
       "      <td>would happen indian government stole kohinoor ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "      <td>24</td>\n",
       "      <td>increase speed internet connection using vpn</td>\n",
       "      <td>internet speed increased hacking dns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>116</td>\n",
       "      <td>20</td>\n",
       "      <td>mentally lonely solve</td>\n",
       "      <td>find remainder mathmath divided</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>116</td>\n",
       "      <td>20</td>\n",
       "      <td>one dissolve water quikly sugar salt methane c...</td>\n",
       "      <td>fish would survive salt water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404285</th>\n",
       "      <td>0</td>\n",
       "      <td>165</td>\n",
       "      <td>27</td>\n",
       "      <td>many keywords racket programming language late...</td>\n",
       "      <td>many keywords perl programming language latest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404286</th>\n",
       "      <td>1</td>\n",
       "      <td>84</td>\n",
       "      <td>17</td>\n",
       "      <td>believe life death</td>\n",
       "      <td>true life death</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404287</th>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>one coin</td>\n",
       "      <td>whats coin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404288</th>\n",
       "      <td>0</td>\n",
       "      <td>222</td>\n",
       "      <td>42</td>\n",
       "      <td>approx annual cost living studying uic chicago...</td>\n",
       "      <td>little hairfall problem want use hair styling ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404289</th>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>18</td>\n",
       "      <td>like sex cousin</td>\n",
       "      <td>like sex cousin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>402691 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        is_duplicate  characters  words  \\\n",
       "0                  0         124     26   \n",
       "1                  0         140     21   \n",
       "2                  0         133     24   \n",
       "3                  0         116     20   \n",
       "4                  0         116     20   \n",
       "...              ...         ...    ...   \n",
       "404285             0         165     27   \n",
       "404286             1          84     17   \n",
       "404287             0          35      7   \n",
       "404288             0         222     42   \n",
       "404289             0          83     18   \n",
       "\n",
       "                                                 q1_clean  \\\n",
       "0               step step guide invest share market india   \n",
       "1                         story kohinoor kohinoor diamond   \n",
       "2            increase speed internet connection using vpn   \n",
       "3                                   mentally lonely solve   \n",
       "4       one dissolve water quikly sugar salt methane c...   \n",
       "...                                                   ...   \n",
       "404285  many keywords racket programming language late...   \n",
       "404286                                 believe life death   \n",
       "404287                                           one coin   \n",
       "404288  approx annual cost living studying uic chicago...   \n",
       "404289                                    like sex cousin   \n",
       "\n",
       "                                                 q2_clean  \n",
       "0                     step step guide invest share market  \n",
       "1       would happen indian government stole kohinoor ...  \n",
       "2                    internet speed increased hacking dns  \n",
       "3                         find remainder mathmath divided  \n",
       "4                           fish would survive salt water  \n",
       "...                                                   ...  \n",
       "404285  many keywords perl programming language latest...  \n",
       "404286                                    true life death  \n",
       "404287                                         whats coin  \n",
       "404288  little hairfall problem want use hair styling ...  \n",
       "404289                                    like sex cousin  \n",
       "\n",
       "[402691 rows x 5 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "- tf-idf\n",
    "- word2vec\n",
    "- word count\n",
    "- number of the same words in both questions\n",
    "- ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickokwir/miniconda/envs/DeepLearning/lib/python3.9/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df['is_duplicate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with target\n",
    "X = pd.DataFrame(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the length of questions and apply to X df\n",
    "X['q1_len'] = df['q1_clean'].apply(lambda x: len(x))\n",
    "X['q2_len'] = df['q2_clean'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate weight of each word in corpus\n",
    "def get_weight(count, eps=10000, min_count=2):\n",
    "    return 0 if count < min_count else 1 / (count + eps)\n",
    "\n",
    "# join all questions together\n",
    "pairs_qs = df['q1_clean'].str.split().astype(str) + df['q2_clean'].str.split().astype(str) \n",
    "words = (\" \".join(pairs_qs)).lower().split()\n",
    "counts = Counter(words)\n",
    "weights = {word: get_weight(count) for word, count in counts.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['word_count'] = pairs_qs.apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the number of unique words in each question\n",
    "def unique_words(text):\n",
    "    doc = nlp(text)\n",
    "    unique_words = set([token.text for token in doc if token.is_stop != True and token.is_punct != True])\n",
    "    return len(unique_words)\n",
    "\n",
    "# find common words in each question\n",
    "def common_words(text):\n",
    "    doc = nlp(text)\n",
    "    common_words = set([token.text for token in doc if token.is_stop == True and token.is_punct == True])\n",
    "    return len(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>q1_len</th>\n",
       "      <th>q2_len</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>67</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>36</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_duplicate  q1_len  q2_len  word_count\n",
       "0             0      41      35          12\n",
       "1             0      31      67          12\n",
       "2             0      44      36          10"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:33: RuntimeWarning: invalid value encountered in scalar divide\n",
      "<timed exec>:38: RuntimeWarning: invalid value encountered in scalar divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.8 s, sys: 657 ms, total: 34.5 s\n",
      "Wall time: 33.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stops = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "def word_shares(row):\n",
    "    q1_list = str(row['q1_clean']).lower().split()\n",
    "    q1 = set(q1_list)\n",
    "    q1words = q1.difference(stops)\n",
    "    if len(q1words) == 0:\n",
    "        return '0:0:0:0:0:0:0:0:0'\n",
    "\n",
    "    q2_list = str(row['q2_clean']).lower().split()\n",
    "    q2 = set(q2_list)\n",
    "    q2words = q2.difference(stops)\n",
    "    if len(q2words) == 0:\n",
    "        return '0:0:0:0:0:0:0:0:0'\n",
    "\n",
    "    words_hamming = sum(1 for i in zip(q1_list, q2_list) if i[0]==i[1])/max(len(q1_list), len(q2_list))\n",
    "\n",
    "    q1stops = q1.intersection(stops)\n",
    "    q2stops = q2.intersection(stops)\n",
    "\n",
    "    q1_2gram = set([i for i in zip(q1_list, q1_list[1:])])\n",
    "    q2_2gram = set([i for i in zip(q2_list, q2_list[1:])])\n",
    "\n",
    "    shared_2gram = q1_2gram.intersection(q2_2gram)\n",
    "\n",
    "    shared_words = q1words.intersection(q2words)\n",
    "    shared_weights = [weights.get(w, 0) for w in shared_words]\n",
    "    q1_weights = [weights.get(w, 0) for w in q1words]\n",
    "    q2_weights = [weights.get(w, 0) for w in q2words]\n",
    "    total_weights = q1_weights + q1_weights\n",
    "\n",
    "    R1 = np.sum(shared_weights) / np.sum(total_weights) #tfidf share\n",
    "    R2 = len(shared_words) / (len(q1words) + len(q2words) - len(shared_words)) #count share\n",
    "    R31 = len(q1stops) / len(q1words) #stops in q1\n",
    "    R32 = len(q2stops) / len(q2words) #stops in q2\n",
    "    Rcosine_denominator = (np.sqrt(np.dot(q1_weights,q1_weights))*np.sqrt(np.dot(q2_weights,q2_weights)))\n",
    "    Rcosine = np.dot(shared_weights, shared_weights)/Rcosine_denominator\n",
    "    if len(q1_2gram) + len(q2_2gram) == 0:\n",
    "        R2gram = 0\n",
    "    else:\n",
    "        R2gram = len(shared_2gram) / (len(q1_2gram) + len(q2_2gram))\n",
    "    \n",
    "    fuzzy_match = fuzz.token_sort_ratio(q1_list, q2_list)\n",
    "    \n",
    "    return '{}:{}:{}:{}:{}:{}:{}:{}:{}'.format(R1, R2, len(shared_words), R31, R32, R2gram, \n",
    "                                                  Rcosine, words_hamming, fuzzy_match)\n",
    "\n",
    "X['word_shares'] = df.apply(word_shares, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>q1_len</th>\n",
       "      <th>q2_len</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>12</td>\n",
       "      <td>nan:0.8333333333333334:5:0.0:0.0:0.45454545454...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>67</td>\n",
       "      <td>12</td>\n",
       "      <td>nan:0.2222222222222222:2:0.0:0.0:0.18181818181...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>36</td>\n",
       "      <td>10</td>\n",
       "      <td>nan:0.2222222222222222:2:0.0:0.0:0.0:nan:0.166...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_duplicate  q1_len  q2_len  word_count  \\\n",
       "0             0      41      35          12   \n",
       "1             0      31      67          12   \n",
       "2             0      44      36          10   \n",
       "\n",
       "                                         word_shares  \n",
       "0  nan:0.8333333333333334:5:0.0:0.0:0.45454545454...  \n",
       "1  nan:0.2222222222222222:2:0.0:0.0:0.18181818181...  \n",
       "2  nan:0.2222222222222222:2:0.0:0.0:0.0:nan:0.166...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['word_match']       = X['word_shares'].apply(lambda x: float(x.split(':')[0]))\n",
    "X['tfidf_word_match'] = X['word_shares'].apply(lambda x: float(x.split(':')[1]))\n",
    "X['shared_count']     = X['word_shares'].apply(lambda x: float(x.split(':')[2]))\n",
    "\n",
    "X['stops1_ratio']     = X['word_shares'].apply(lambda x: float(x.split(':')[3]))\n",
    "X['stops2_ratio']     = X['word_shares'].apply(lambda x: float(x.split(':')[4]))\n",
    "X['shared_2gram']     = X['word_shares'].apply(lambda x: float(x.split(':')[5]))\n",
    "X['cosine']           = X['word_shares'].apply(lambda x: float(x.split(':')[6]))\n",
    "X['words_hamming']    = X['word_shares'].apply(lambda x: float(x.split(':')[7]))\n",
    "X['fuzzy_match']    = X['word_shares'].apply(lambda x: float(x.split(':')[8]))\n",
    "\n",
    "\n",
    "X['len_word_q1'] = df['q1_clean'].apply(lambda x: len(str(x).split()))\n",
    "X['len_word_q2'] = df['q2_clean'].apply(lambda x: len(str(x).split()))\n",
    "X['diff_len_word'] = X['len_word_q1'] - X['len_word_q2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>q1_len</th>\n",
       "      <th>q2_len</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_shares</th>\n",
       "      <th>word_match</th>\n",
       "      <th>tfidf_word_match</th>\n",
       "      <th>shared_count</th>\n",
       "      <th>stops1_ratio</th>\n",
       "      <th>stops2_ratio</th>\n",
       "      <th>shared_2gram</th>\n",
       "      <th>cosine</th>\n",
       "      <th>words_hamming</th>\n",
       "      <th>fuzzy_match</th>\n",
       "      <th>len_word_q1</th>\n",
       "      <th>len_word_q2</th>\n",
       "      <th>diff_len_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>12</td>\n",
       "      <td>nan:0.8333333333333334:5:0.0:0.0:0.45454545454...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>92.0</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>67</td>\n",
       "      <td>12</td>\n",
       "      <td>nan:0.2222222222222222:2:0.0:0.0:0.18181818181...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>59.0</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>36</td>\n",
       "      <td>10</td>\n",
       "      <td>nan:0.2222222222222222:2:0.0:0.0:0.0:nan:0.166...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>65.0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_duplicate  q1_len  q2_len  word_count  \\\n",
       "0             0      41      35          12   \n",
       "1             0      31      67          12   \n",
       "2             0      44      36          10   \n",
       "\n",
       "                                         word_shares  word_match  \\\n",
       "0  nan:0.8333333333333334:5:0.0:0.0:0.45454545454...         NaN   \n",
       "1  nan:0.2222222222222222:2:0.0:0.0:0.18181818181...         NaN   \n",
       "2  nan:0.2222222222222222:2:0.0:0.0:0.0:nan:0.166...         NaN   \n",
       "\n",
       "   tfidf_word_match  shared_count  stops1_ratio  stops2_ratio  shared_2gram  \\\n",
       "0          0.833333           5.0           0.0           0.0      0.454545   \n",
       "1          0.222222           2.0           0.0           0.0      0.181818   \n",
       "2          0.222222           2.0           0.0           0.0      0.000000   \n",
       "\n",
       "   cosine  words_hamming  fuzzy_match  len_word_q1  len_word_q2  diff_len_word  \n",
       "0     NaN       0.857143         92.0            7            6              1  \n",
       "1     NaN       0.000000         59.0            4            9             -5  \n",
       "2     NaN       0.166667         65.0            6            5              1  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop word_shares column\n",
    "X.drop(['word_shares'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan,  0.])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get unique values in cosine column\n",
    "X['cosine'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(['cosine'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan,  0.])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['word_match'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(['word_match'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>q1_len</th>\n",
       "      <th>q2_len</th>\n",
       "      <th>word_count</th>\n",
       "      <th>tfidf_word_match</th>\n",
       "      <th>shared_count</th>\n",
       "      <th>stops1_ratio</th>\n",
       "      <th>stops2_ratio</th>\n",
       "      <th>shared_2gram</th>\n",
       "      <th>words_hamming</th>\n",
       "      <th>fuzzy_match</th>\n",
       "      <th>len_word_q1</th>\n",
       "      <th>len_word_q2</th>\n",
       "      <th>diff_len_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>12</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>92.0</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>67</td>\n",
       "      <td>12</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>59.0</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>36</td>\n",
       "      <td>10</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>65.0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_duplicate  q1_len  q2_len  word_count  tfidf_word_match  shared_count  \\\n",
       "0             0      41      35          12          0.833333           5.0   \n",
       "1             0      31      67          12          0.222222           2.0   \n",
       "2             0      44      36          10          0.222222           2.0   \n",
       "\n",
       "   stops1_ratio  stops2_ratio  shared_2gram  words_hamming  fuzzy_match  \\\n",
       "0           0.0           0.0      0.454545       0.857143         92.0   \n",
       "1           0.0           0.0      0.181818       0.000000         59.0   \n",
       "2           0.0           0.0      0.000000       0.166667         65.0   \n",
       "\n",
       "   len_word_q1  len_word_q2  diff_len_word  \n",
       "0            7            6              1  \n",
       "1            4            9             -5  \n",
       "2            6            5              1  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv\n",
    "X.to_csv('clean_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('clean_df.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q1_len</th>\n",
       "      <th>q2_len</th>\n",
       "      <th>word_count</th>\n",
       "      <th>tfidf_word_match</th>\n",
       "      <th>shared_count</th>\n",
       "      <th>shared_2gram</th>\n",
       "      <th>words_hamming</th>\n",
       "      <th>fuzzy_match</th>\n",
       "      <th>len_word_q1</th>\n",
       "      <th>len_word_q2</th>\n",
       "      <th>diff_len_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>12</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>92.0</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31</td>\n",
       "      <td>67</td>\n",
       "      <td>12</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>59.0</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44</td>\n",
       "      <td>36</td>\n",
       "      <td>10</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>65.0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   q1_len  q2_len  word_count  tfidf_word_match  shared_count  shared_2gram  \\\n",
       "0      41      35          12          0.833333           5.0      0.454545   \n",
       "1      31      67          12          0.222222           2.0      0.181818   \n",
       "2      44      36          10          0.222222           2.0      0.000000   \n",
       "\n",
       "   words_hamming  fuzzy_match  len_word_q1  len_word_q2  diff_len_word  \n",
       "0       0.857143         92.0            7            6              1  \n",
       "1       0.000000         59.0            4            9             -5  \n",
       "2       0.166667         65.0            6            5              1  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take the first 10000 rows from df sample data using iloc\n",
    "sample = df.iloc[:100000]\n",
    "sample.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "Different modeling techniques can be used:\n",
    "\n",
    "- logistic regression\n",
    "- XGBoost\n",
    "- LSTMs\n",
    "- etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# import standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# import stratified k fold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# import logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# import pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# import smote from imblearn\n",
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sample['is_duplicate']\n",
    "X = sample.drop(['is_duplicate', 'stops1_ratio', 'stops2_ratio'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = sm.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test using 80:20 ratio using iloc\n",
    "y_train = y.iloc[:int(y.shape[0]*0.8)]\n",
    "y_test = y.iloc[int(y.shape[0]*0.8):]\n",
    "X_train = X.iloc[:int(X.shape[0]*0.8)]\n",
    "X_test = X.iloc[int(X.shape[0]*0.8):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100180,), (25046,), (100180, 11), (25046, 11))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_test.shape, X_train.shape, X_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Test different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = 'accuracy'\n",
    "kfold = StratifiedKFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickokwir/miniconda/envs/DeepLearning/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: [0.5303042401980356]\n",
      "RF: [0.5303042401980356, 0.8544677792861135]\n"
     ]
    }
   ],
   "source": [
    "# build pipeline without scaling\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('RF', RandomForestClassifier()))\n",
    "\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "#evaluate each model in turn with cross validation\n",
    "for name, model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    results.append(accuracy_score(y_test, model.predict(X_test)))\n",
    "    names.append(name)\n",
    "    msg = f'{name}: {results}'\n",
    "    print(msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xgboost\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.81      0.89     25046\n",
      "\n",
      "    accuracy                           0.81     25046\n",
      "   macro avg       0.50      0.40      0.45     25046\n",
      "weighted avg       1.00      0.81      0.89     25046\n",
      "\n",
      "[[    0     0]\n",
      " [ 4834 20212]]\n",
      "0.8069951289627086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickokwir/miniconda/envs/DeepLearning/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/patrickokwir/miniconda/envs/DeepLearning/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/patrickokwir/miniconda/envs/DeepLearning/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# xgb classifier with learning rate 0.01\n",
    "Xgb = xgb.XGBClassifier()\n",
    "Xgb.fit(X_train, y_train)\n",
    "y_pred = Xgb.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search for best hyperparameters for XGBoost\n",
    "def grid_search(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Grid search for best hyperparameters for XGBoost\n",
    "    \"\"\"\n",
    "    # import XGBoost\n",
    "    from xgboost import XGBClassifier\n",
    "    # import GridSearchCV\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    # import RandomizedSearchCV\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "    # define XGBoost parameters\n",
    "    xgb_params = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'min_child_weight': [1, 3, 5],\n",
    "        'gamma': [0.5, 1, 1.5, 2],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'reg_alpha': [1e-5, 1e-4, 1e-3],\n",
    "        'reg_lambda': [1e-5, 1e-4, 1e-3],\n",
    "        'scale_pos_weight': [1, 3, 5],\n",
    "        'objective': ['binary:logistic', 'multi:softmax'],\n",
    "        'nthread': [4],\n",
    "        'seed': [42]\n",
    "    }\n",
    "    # define XGBoost grid search\n",
    "    xgb_grid = GridSearchCV(XGBClassifier(), xgb_params, cv=2, n_jobs=-1, verbose=1)\n",
    "    # fit XGBoost grid search\n",
    "    xgb_grid.fit(X_train, y_train)\n",
    "    # print best XGBoost hyperparameters\n",
    "    print('Best XGBoost hyperparameters: ', xgb_grid.best_params_)\n",
    "    # print best XGBoost score\n",
    "    print('Best XGBoost score: ', xgb_grid.best_score_)\n",
    "    # predict on test set\n",
    "    y_pred = xgb_grid.predict(X_test)\n",
    "    # print classification report\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    # print confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 110)               1320      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 110)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 500)               55500     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 500)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 100)               50100     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 30)                3030      \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 30)                0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 2)                 62        \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 110,015\n",
      "Trainable params: 110,015\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# LSTM model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import layers\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(110, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100, activation='LeakyReLU'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "334/334 [==============================] - 4s 10ms/step - loss: 0.6807 - accuracy: 0.6214 - val_loss: 0.8308 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/120\n",
      "334/334 [==============================] - 3s 10ms/step - loss: 0.6650 - accuracy: 0.6250 - val_loss: 0.9227 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/120\n",
      "334/334 [==============================] - 3s 10ms/step - loss: 0.6620 - accuracy: 0.6250 - val_loss: 0.9650 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/120\n",
      "334/334 [==============================] - 3s 10ms/step - loss: 0.6616 - accuracy: 0.6250 - val_loss: 0.9791 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/120\n",
      "334/334 [==============================] - 3s 10ms/step - loss: 0.6616 - accuracy: 0.6250 - val_loss: 0.9798 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/120\n",
      "334/334 [==============================] - 3s 9ms/step - loss: 0.6616 - accuracy: 0.6250 - val_loss: 0.9787 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/120\n",
      "334/334 [==============================] - 3s 9ms/step - loss: 0.6616 - accuracy: 0.6250 - val_loss: 0.9773 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/120\n",
      "334/334 [==============================] - 3s 9ms/step - loss: 0.6616 - accuracy: 0.6250 - val_loss: 0.9806 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/120\n",
      "226/334 [===================>..........] - ETA: 0s - loss: 0.6627 - accuracy: 0.6227"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/patrickokwir/Desktop/Lighthouse-data-notes/NLP project/mini_project_V2.ipynb Cell 103\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/patrickokwir/Desktop/Lighthouse-data-notes/NLP%20project/mini_project_V2.ipynb#Y203sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train,y_train,validation_data\u001b[39m=\u001b[39;49m(X_test,y_test),epochs\u001b[39m=\u001b[39;49m\u001b[39m120\u001b[39;49m,batch_size\u001b[39m=\u001b[39;49m\u001b[39m300\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,)\n",
      "File \u001b[0;32m~/miniconda/envs/DeepLearning/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda/envs/DeepLearning/lib/python3.9/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda/envs/DeepLearning/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda/envs/DeepLearning/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda/envs/DeepLearning/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda/envs/DeepLearning/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/miniconda/envs/DeepLearning/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda/envs/DeepLearning/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniconda/envs/DeepLearning/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=120,batch_size=300, verbose=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear session to avoid clutter from old models / layers.\n",
    "from keras import backend as K \n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Net Using RAW Data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_A.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take sample of first 10000 rows from df sample data using iloc\n",
    "sample = df.iloc[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sample['combined']\n",
    "y = sample['is_duplicate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y.iloc[:int(y.shape[0]*0.8)]\n",
    "y_test = y.iloc[int(y.shape[0]*0.8):]\n",
    "X_train = X.iloc[:int(X.shape[0]*0.8)]\n",
    "X_test = X.iloc[int(X.shape[0]*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=3000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "embedding_dim = 50\n",
    "maxlen = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=50,\n",
    "                    verbose=False,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10)\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
