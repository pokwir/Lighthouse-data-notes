{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60270b18-aa24-4e22-a899-ea6539669ef4",
   "metadata": {},
   "source": [
    "**ML in Spark**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ebfac2-cf15-4a38-b05b-e04cd903cb23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76d3eab1-901d-43e8-9aa5-8b87c9459b04",
   "metadata": {},
   "source": [
    "### Spark Context\n",
    "SparkContext is the internal engine that allows the connections with the clusters. If you want to run an operation, you need a SparkContext."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d3449ec-6e5c-433a-8e4e-292245c22a18",
   "metadata": {},
   "source": [
    "### Create a SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b16be4d-f2ec-44e0-8f87-30e813397f3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/24 23:49:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/05/24 23:49:28 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# First of all, you need to initiate a SparkContext.\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "sc =SparkContext()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "228be66f-0710-4826-b07f-134c528ef422",
   "metadata": {},
   "source": [
    "Now that the SparkContext is ready, you can create a collection of data called RDD, Resilient Distributed Dataset. Computation in an RDD is automatically parallelized across the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23b240fb-c491-4224-9cfd-42396a23533d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nums= sc.parallelize([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226c43c1-700a-48dd-ba6a-02d9b8dd866a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0a0f672-fbdb-4195-994e-fa09d41bf9bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# you can access the first row\n",
    "nums.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82ceff5-ee05-46da-ad6e-14e3f3d6b302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0a63caf-4e2c-4929-b162-60e6117c2391",
   "metadata": {},
   "source": [
    "You can apply a transformation to the data with a lambda function. In the PySpark example below, you return the square of nums. It is a map transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cdd08b8-18d2-4e10-a83b-c06da2ee5d8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \n",
      "4 \n",
      "9 \n",
      "16 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "squared = nums.map(lambda x: x*x).collect()\n",
    "for num in squared:\n",
    "    print('%i ' % (num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483a2d50-95af-42c1-b60d-7e2d106b9f53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b697a218-510d-4998-bad9-bb04abf49e37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4eee4d8-0aaa-4886-9bed-58f356d2b23c",
   "metadata": {},
   "source": [
    "### SQLContext"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1517d499-ac46-46b8-b468-b4342390009d",
   "metadata": {},
   "source": [
    "A more convenient way is to use the DataFrame. SparkContext is already set, you can use it to create the DataFrame. You also need to declare the SQLContext"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5cca1dee-e780-4b77-bb95-8d6ce595196e",
   "metadata": {},
   "source": [
    "SQLContext allows connecting the engine with different data sources. It is used to initiate the functionalities of Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcf6baef-fe78-4d20-81d1-255e8dc8c263",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrick/opt/anaconda3/envs/Python_3_9/lib/python3.9/site-packages/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6278ec1-c74b-4109-ac52-1af6d1a0a4f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now in this Spark tutorial Python, let's create a list of tuples. \\\n",
    "#Each tuple will contain the name of the people and their age. Four steps are required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ada44dbd-9ca5-44ab-a015-58e1ff285819",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the list of tuple with the information\n",
    "list_p = [('John',19),('Smith',29),('Adam',35),('Henry',50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72719dc4-80ba-4596-a7bf-fd233dde9b31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build the RDD\n",
    "rdd = sc.parallelize(list_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7adf1d74-9e07-4c36-a630-820bd637ef1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Convert to Tuples\n",
    "ppl = rdd.map(lambda x: Row(name=x[0], age=int(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c11bffc0-d50f-4460-b3d5-5d092b3981d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame context\n",
    "DF_ppl = sqlContext.createDataFrame(ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f551aae-4467-4c1e-b7ca-392643993dd7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#If you want to access the type of each feature, you can use printSchema()\n",
    "\n",
    "DF_ppl.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15917907-0008-49f4-aac9-4c2ee90376cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "| John| 19|\n",
      "|Smith| 29|\n",
      "| Adam| 35|\n",
      "+-----+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF_ppl.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e7ca2d-44e8-4e75-ac40-9ad3d046fe42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c44cc6d6-47af-4935-bef7-fd119a7a1877",
   "metadata": {},
   "source": [
    "# Machine Learning Example with PySpark\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8563bdd-e55b-4a01-b8ea-4a3d33924a87",
   "metadata": {},
   "source": [
    "Now that you have a brief idea of Spark and SQLContext, you are ready to build your first Machine Learning program using Spark.\n",
    "\n",
    "Following are the steps to build a Machine Learning program with PySpark:\n",
    "\n",
    "1. Basic operation with PySpark\\\n",
    "2. Data preprocessing\\\n",
    "3. Build a data processing pipeline\\\n",
    "4. Build the classifier: logistic\\\n",
    "5. Train and evaluate the model\\\n",
    "6. Tune the hyperparameter\\\n",
    "Note that, the dataset we use is not very big and you may think that the computation takes a long time. Spark is designed to process a considerable amount of data. Spark's performance increases relatively to other machine learning libraries when the dataset grows larger."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da188443",
   "metadata": {},
   "source": [
    "## Step 1: Basic operation with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e6801bd-7d1e-411a-b346-c3e495d32466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Age: long (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: long (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- education_num: long (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- capital_gain: long (nullable = true)\n",
      " |-- capital_loss: long (nullable = true)\n",
      " |-- hours_week: long (nullable = true)\n",
      " |-- native_country: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/sadhana1002/PredictingSalaryClass-Classification/master/adult.csv\"\n",
    "df = sqlContext.createDataFrame(pd.read_csv(url, \n",
    "                                      names=['Age','workclass',\n",
    "                                             'fnlwgt','education',\n",
    "                                             'education_num',\n",
    "                                             'marital',\n",
    "                                             'occupation',\n",
    "                                             'relationship','race',\n",
    "                                             'sex','capital_gain',\n",
    "                                             'capital_loss',\n",
    "                                             'hours_week',\n",
    "                                             'native_country','label']))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94923bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+------+----------+-------------+-------------------+------------------+--------------+------+-------+------------+------------+----------+--------------+------+\n",
      "|Age|workclass        |fnlwgt|education |education_num|marital            |occupation        |relationship  |race  |sex    |capital_gain|capital_loss|hours_week|native_country|label |\n",
      "+---+-----------------+------+----------+-------------+-------------------+------------------+--------------+------+-------+------------+------------+----------+--------------+------+\n",
      "|39 | State-gov       |77516 | Bachelors|13           | Never-married     | Adm-clerical     | Not-in-family| White| Male  |2174        |0           |40        | United-States| <=50K|\n",
      "|50 | Self-emp-not-inc|83311 | Bachelors|13           | Married-civ-spouse| Exec-managerial  | Husband      | White| Male  |0           |0           |13        | United-States| <=50K|\n",
      "|38 | Private         |215646| HS-grad  |9            | Divorced          | Handlers-cleaners| Not-in-family| White| Male  |0           |0           |40        | United-States| <=50K|\n",
      "|53 | Private         |234721| 11th     |7            | Married-civ-spouse| Handlers-cleaners| Husband      | Black| Male  |0           |0           |40        | United-States| <=50K|\n",
      "|28 | Private         |338409| Bachelors|13           | Married-civ-spouse| Prof-specialty   | Wife         | Black| Female|0           |0           |40        | Cuba         | <=50K|\n",
      "+---+-----------------+------+----------+-------------+-------------------+------------------+--------------+------+-------+------------+------------+----------+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5, truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12f63f2d",
   "metadata": {},
   "source": [
    "To convert the continuous variable in the right format, you can use recast the columns. You can use withColumn to tell Spark which column to operate the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0a04178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Age: float (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: float (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- education_num: float (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- capital_gain: float (nullable = true)\n",
      " |-- capital_loss: float (nullable = true)\n",
      " |-- hours_week: float (nullable = true)\n",
      " |-- native_country: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#To convert the continuous variable in the right format, you can use recast the columns. \\\n",
    "#You can use withColumn to tell Spark which column to operate the transformation.\n",
    "\n",
    "# Import all from `sql.types`\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Write a custom function to convert the data type of DataFrame columns\n",
    "def convertColumn(df, names, newType):\n",
    "    for name in names: \n",
    "        df = df.withColumn(name, df[name].cast(newType))\n",
    "    return df\n",
    "\n",
    "# List of continuous features\n",
    "CONTI_FEATURES  = ['Age', 'fnlwgt','capital_gain', 'education_num', 'capital_loss', 'hours_week']\n",
    "\n",
    "# Convert the type\n",
    "df = convertColumn(df, CONTI_FEATURES, FloatType())\n",
    "\n",
    "# Check the dataset\n",
    "df.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a25c2d16",
   "metadata": {},
   "source": [
    "### Select columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "def1dc00",
   "metadata": {},
   "source": [
    "You can select and show the rows with select and the names of the features. Below, age and fnlwgt are selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7144eaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "| age|  fnlwgt|\n",
      "+----+--------+\n",
      "|39.0| 77516.0|\n",
      "|50.0| 83311.0|\n",
      "|38.0|215646.0|\n",
      "|53.0|234721.0|\n",
      "|28.0|338409.0|\n",
      "+----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('age', 'fnlwgt').show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "839acf26",
   "metadata": {},
   "source": [
    "### Count by group"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29460158",
   "metadata": {},
   "source": [
    "If you want to count the number of occurrence by group, you can chain:\n",
    "\n",
    "`groupBy()`\\\n",
    "`count()`\\\n",
    "together. In the PySpark example below, you count the number of rows by the education level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7107834a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|    education|count|\n",
      "+-------------+-----+\n",
      "|    Preschool|   51|\n",
      "|      1st-4th|  168|\n",
      "|      5th-6th|  333|\n",
      "|    Doctorate|  413|\n",
      "|         12th|  433|\n",
      "|          9th|  514|\n",
      "|  Prof-school|  576|\n",
      "|      7th-8th|  646|\n",
      "|         10th|  933|\n",
      "|   Assoc-acdm| 1067|\n",
      "|         11th| 1175|\n",
      "|    Assoc-voc| 1382|\n",
      "|      Masters| 1723|\n",
      "|    Bachelors| 5355|\n",
      "| Some-college| 7291|\n",
      "|      HS-grad|10501|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"education\").count().sort(\"count\",ascending=True).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee2d6db0",
   "metadata": {},
   "source": [
    "### Describe the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50a21b1d",
   "metadata": {},
   "source": [
    "To get a summary statistics, of the data, you can use describe(). It will compute the :\n",
    "\n",
    "`count`\\\n",
    "`mean`\\\n",
    "`standard deviation`\\\n",
    "`min`\\\n",
    "`max`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b57c9282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/24 23:49:39 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 11:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------+------------------+-------------+------------------+---------+-----------------+------------+-------------------+-------+------------------+------------------+------------------+--------------+------+\n",
      "|summary|               Age|   workclass|            fnlwgt|    education|     education_num|  marital|       occupation|relationship|               race|    sex|      capital_gain|      capital_loss|        hours_week|native_country| label|\n",
      "+-------+------------------+------------+------------------+-------------+------------------+---------+-----------------+------------+-------------------+-------+------------------+------------------+------------------+--------------+------+\n",
      "|  count|             32561|       32561|             32561|        32561|             32561|    32561|            32561|       32561|              32561|  32561|             32561|             32561|             32561|         32561| 32561|\n",
      "|   mean| 38.58164675532078|        null|189778.36651208502|         null|  10.0806793403151|     null|             null|        null|               null|   null|1077.6488437087312|   87.303829734959|40.437455852092995|          null|  null|\n",
      "| stddev|13.640432553581343|        null| 105549.9776970223|         null|2.5727203320673877|     null|             null|        null|               null|   null| 7385.292084840335|402.96021864899984|12.347428681731847|          null|  null|\n",
      "|    min|              17.0|           ?|           12285.0|         10th|               1.0| Divorced|                ?|     Husband| Amer-Indian-Eskimo| Female|               0.0|               0.0|               1.0|             ?| <=50K|\n",
      "|    max|              90.0| Without-pay|         1484705.0| Some-college|              16.0|  Widowed| Transport-moving|        Wife|              White|   Male|           99999.0|            4356.0|              99.0|    Yugoslavia|  >50K|\n",
      "+-------+------------------+------------+------------------+-------------+------------------+---------+-----------------+------------+-------------------+-------+------------------+------------------+------------------+--------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c5eda90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|               age|\n",
      "+-------+------------------+\n",
      "|  count|             32561|\n",
      "|   mean| 38.58164675532078|\n",
      "| stddev|13.640432553581343|\n",
      "|    min|              17.0|\n",
      "|    max|              90.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#If you want the summary statistic of only one column, add the name of the column inside describe()\n",
    "\n",
    "df.describe('age').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f0f163c",
   "metadata": {},
   "source": [
    "### Drop column"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a38eb2af",
   "metadata": {},
   "source": [
    "There are two intuitive commands to drop columns:\n",
    "\n",
    "`drop():` Drop a column\\\n",
    "`dropna():` Drop NA's\\\n",
    "Below you drop the column education_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9a7be05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Age',\n",
       " 'workclass',\n",
       " 'fnlwgt',\n",
       " 'education',\n",
       " 'marital',\n",
       " 'occupation',\n",
       " 'relationship',\n",
       " 'race',\n",
       " 'sex',\n",
       " 'capital_gain',\n",
       " 'capital_loss',\n",
       " 'hours_week',\n",
       " 'native_country',\n",
       " 'label']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop('education_num').columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a7523ad",
   "metadata": {},
   "source": [
    "### Filter data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9dc239bd",
   "metadata": {},
   "source": [
    "You can use `filter() `to apply descriptive statistics in a subset of data.\\\n",
    " `For instance, you can count the number of people above 40:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c23e227e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13443"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df.Age > 40).count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8cf6934d",
   "metadata": {},
   "source": [
    "### Descriptive statistics by group"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d55ab8a",
   "metadata": {},
   "source": [
    "Finally, you can group data by group and compute statistical operations like the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ec8df91",
   "metadata": {},
   "outputs": [],
   "source": [
    "marital_status_cg = df.groupby('marital').agg({'capital_gain': 'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f62887f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|             marital| avg(capital_gain)|\n",
      "+--------------------+------------------+\n",
      "|             Widowed| 571.0715005035247|\n",
      "| Married-spouse-a...| 653.9832535885167|\n",
      "|   Married-AF-spouse| 432.6521739130435|\n",
      "|  Married-civ-spouse|1764.8595085470085|\n",
      "|            Divorced| 728.4148098131893|\n",
      "|       Never-married|376.58831788823363|\n",
      "|           Separated| 535.5687804878049|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# round avg(capital_gain), 2 digits\n",
    "marital_status_cg.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef851f71",
   "metadata": {},
   "source": [
    "# Step 2: Data preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6e241e6",
   "metadata": {},
   "source": [
    "Data processing is a critical step in machine learning. After you remove garbage data, you get some important insights.\n",
    "\n",
    "For instance, you know that age is not a linear function with the income. When people are young, their income is usually lower than mid-age. After retirement, a household uses their saving, meaning a decrease in income. To capture this pattern, you can add a square to the age feature"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ccfbbf5e",
   "metadata": {},
   "source": [
    "### Add age square"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "935d791f",
   "metadata": {},
   "source": [
    "To add a new feature, you need to:\n",
    "\n",
    "`Select the column`\\\n",
    "`Apply the transformation and add it to the DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Age: float (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: float (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- education_num: float (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- capital_gain: float (nullable = true)\n",
      " |-- capital_loss: float (nullable = true)\n",
      " |-- hours_week: float (nullable = true)\n",
      " |-- native_country: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- age_square: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add square of age feature\n",
    "\n",
    "# import pyspark sql functions\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "#1 Select the column \n",
    "age_square = df.select(col('age')**2)\n",
    "\n",
    "#2 Apply the transformation and add it to the DataFrame\n",
    "df = df.withColumn('age_square', col('age')**2)\n",
    "\n",
    "# Check the results\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82a1e9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-----------------+--------+----------+-------------+-------------------+------------------+--------------+------+-------+------------+------------+----------+--------------+------+\n",
      "| age|age_square|        workclass|  fnlwgt| education|education_num|            marital|        occupation|  relationship|  race|    sex|capital_gain|capital_loss|hours_week|native_country| label|\n",
      "+----+----------+-----------------+--------+----------+-------------+-------------------+------------------+--------------+------+-------+------------+------------+----------+--------------+------+\n",
      "|39.0|    1521.0|        State-gov| 77516.0| Bachelors|         13.0|      Never-married|      Adm-clerical| Not-in-family| White|   Male|      2174.0|         0.0|      40.0| United-States| <=50K|\n",
      "|50.0|    2500.0| Self-emp-not-inc| 83311.0| Bachelors|         13.0| Married-civ-spouse|   Exec-managerial|       Husband| White|   Male|         0.0|         0.0|      13.0| United-States| <=50K|\n",
      "|38.0|    1444.0|          Private|215646.0|   HS-grad|          9.0|           Divorced| Handlers-cleaners| Not-in-family| White|   Male|         0.0|         0.0|      40.0| United-States| <=50K|\n",
      "|53.0|    2809.0|          Private|234721.0|      11th|          7.0| Married-civ-spouse| Handlers-cleaners|       Husband| Black|   Male|         0.0|         0.0|      40.0| United-States| <=50K|\n",
      "|28.0|     784.0|          Private|338409.0| Bachelors|         13.0| Married-civ-spouse|    Prof-specialty|          Wife| Black| Female|         0.0|         0.0|      40.0|          Cuba| <=50K|\n",
      "+----+----------+-----------------+--------+----------+-------------+-------------------+------------------+--------------+------+-------+------------+------------+----------+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# You can change the order of the variables with select. Below, you bring agesquare right after age.\n",
    "\n",
    "COLUMNS = ['age', 'age_square', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital',\n",
    "           'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss',\n",
    "           'hours_week', 'native_country', 'label']\n",
    "df = df.select(COLUMNS)\n",
    "df.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e6b11ff",
   "metadata": {},
   "source": [
    "### Exclude Holand-Netherlands"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d156983",
   "metadata": {},
   "source": [
    "When a group within a feature has only one observation, it brings no information to the model. On the contrary, it can lead to an error during the cross-validation.\n",
    "\n",
    "Let's check the origin of the household"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8137b704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|      native_country|count(native_country)|\n",
      "+--------------------+---------------------+\n",
      "|  Holand-Netherlands|                    1|\n",
      "|            Scotland|                   12|\n",
      "|            Honduras|                   13|\n",
      "|             Hungary|                   13|\n",
      "| Outlying-US(Guam...|                   14|\n",
      "|          Yugoslavia|                   16|\n",
      "|                Laos|                   18|\n",
      "|            Thailand|                   18|\n",
      "|            Cambodia|                   19|\n",
      "|     Trinadad&Tobago|                   19|\n",
      "|                Hong|                   20|\n",
      "|             Ireland|                   24|\n",
      "|             Ecuador|                   28|\n",
      "|              France|                   29|\n",
      "|              Greece|                   29|\n",
      "|                Peru|                   31|\n",
      "|           Nicaragua|                   34|\n",
      "|            Portugal|                   37|\n",
      "|                Iran|                   43|\n",
      "|               Haiti|                   44|\n",
      "+--------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.native_country == 'Holand-Netherlands').count()\n",
    "df.groupby('native_country').agg({'native_country': 'count'}).sort(asc(\"count(native_country)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2266a37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove Holand-Netherlands\n",
    "df_remove = df.filter(df.native_country != 'Holand-Netherlands')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c060f49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|      native_country|count(native_country)|\n",
      "+--------------------+---------------------+\n",
      "|  Holand-Netherlands|                    1|\n",
      "|            Scotland|                   12|\n",
      "|            Honduras|                   13|\n",
      "|             Hungary|                   13|\n",
      "| Outlying-US(Guam...|                   14|\n",
      "|          Yugoslavia|                   16|\n",
      "|                Laos|                   18|\n",
      "|            Thailand|                   18|\n",
      "|            Cambodia|                   19|\n",
      "|     Trinadad&Tobago|                   19|\n",
      "|                Hong|                   20|\n",
      "|             Ireland|                   24|\n",
      "|             Ecuador|                   28|\n",
      "|              France|                   29|\n",
      "|              Greece|                   29|\n",
      "|                Peru|                   31|\n",
      "|           Nicaragua|                   34|\n",
      "|            Portugal|                   37|\n",
      "|                Iran|                   43|\n",
      "|               Haiti|                   44|\n",
      "+--------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('native_country').agg({'native_country': 'count'}).sort(asc(\"count(native_country)\")).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8b0dd97",
   "metadata": {},
   "source": [
    "# Step 3: Build a data processing pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf0fd600",
   "metadata": {},
   "source": [
    "Similar to scikit-learn, Pyspark has a pipeline API.\n",
    "\n",
    "A pipeline is very convenient to maintain the structure of the data. You push the data into the pipeline. Inside the pipeline, various operations are done, the output is used to feed the algorithm.\n",
    "\n",
    "For instance, one universal transformation in machine learning consists of converting a string to one hot encoder, i.e., one column by a group. One hot encoder is usually a matrix full of zeroes.\n",
    "\n",
    "The steps to transform the data are very similar to scikit-learn. You need to:\n",
    "\n",
    "`Index the string to numeric`\\\n",
    "`Create the one hot encoder`\\\n",
    "`Transform the data`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1f093d7",
   "metadata": {},
   "source": [
    "Two APIs do the job: `StringIndexer, OneHotEncoder`\n",
    "\n",
    "First of all, you select the string column to index. The `inputCol` is the name of the column in the dataset. `outputCol` is the new name given to the transformed column.\\\n",
    "Fit the data and transform it\\\n",
    "Create the news columns based on the group. For instance, if there are 10 groups in the feature, the new matrix will have 10 columns, one for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea8e665b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-----------------+-------+----------+-------------+-------------------+----------------+--------------+------+-----+------------+------------+----------+--------------+------+-----------------+-------------+\n",
      "| age|age_square|        workclass| fnlwgt| education|education_num|            marital|      occupation|  relationship|  race|  sex|capital_gain|capital_loss|hours_week|native_country| label|workclass_encoded|workclass_vec|\n",
      "+----+----------+-----------------+-------+----------+-------------+-------------------+----------------+--------------+------+-----+------------+------------+----------+--------------+------+-----------------+-------------+\n",
      "|39.0|    1521.0|        State-gov|77516.0| Bachelors|         13.0|      Never-married|    Adm-clerical| Not-in-family| White| Male|      2174.0|         0.0|      40.0| United-States| <=50K|              4.0|(9,[4],[1.0])|\n",
      "|50.0|    2500.0| Self-emp-not-inc|83311.0| Bachelors|         13.0| Married-civ-spouse| Exec-managerial|       Husband| White| Male|         0.0|         0.0|      13.0| United-States| <=50K|              1.0|(9,[1],[1.0])|\n",
      "+----+----------+-----------------+-------+----------+-------------+-------------------+----------------+--------------+------+-----+------------+------------+----------+--------------+------+-----------------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Example encoder\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"workclass\", outputCol=\"workclass_encoded\")\n",
    "model = stringIndexer.fit(df)\n",
    "indexed = model.transform(df)\n",
    "encoder = OneHotEncoder(dropLast=False, inputCol=\"workclass_encoded\", outputCol=\"workclass_vec\").fit(indexed)\n",
    "encoded = encoder.transform(indexed)\n",
    "encoded.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41769c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b88190d4",
   "metadata": {},
   "source": [
    "### Build the pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62dc1d9e",
   "metadata": {},
   "source": [
    "You will build a pipeline to convert all the precise features and add them to the final dataset. The pipeline will have four operations, but feel free to add as many operations as you want.\n",
    "\n",
    "`Encode the categorical data`\\\n",
    "`Index the label feature`\\\n",
    "`Add continuous variable`\\\n",
    "`Assemble the steps.`\\\n",
    "Each step is stored in a list named stages. This list will tell the VectorAssembler what operation to perform inside the pipeline.\\\n",
    "\n",
    "`Encode the categorical data`\\\n",
    "This step is very similar to the above example, except that you loop over all the categorical features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3823cf35",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6be0052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "CATE_FEATURES = ['workclass', 'education', 'marital', 'occupation', 'relationship', 'race', 'sex', 'native_country']\n",
    "stages = [] # stages in our Pipeline\n",
    "for categoricalCol in CATE_FEATURES:\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
    "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()],\n",
    "                                     outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa91139d",
   "metadata": {},
   "source": [
    "**Index the label feature**\\\n",
    "Spark, like many other libraries, does not accept string values for the label. You convert the label feature with StringIndexer and add it to the list stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a130cc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert label into label indices using the StringIndexer\n",
    "label_stringIdx =  StringIndexer(inputCol=\"label\", outputCol=\"newlabel\")\n",
    "stages += [label_stringIdx]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e7d2c18",
   "metadata": {},
   "source": [
    "**Add continuous variable**\\\n",
    "The `inputCols` of the `VectorAssembler` is a list of columns. You can create a new list containing all the new columns. The code below populate the list with encoded categorical features and continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ecb71fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "assemblerInputs = [c + \"classVec\" for c in CATE_FEATURES] + CONTI_FEATURES"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5c1a903",
   "metadata": {},
   "source": [
    "**Assemble the steps.**\\\n",
    "Finally, you pass all the steps in the VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b31c7340",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "016a332a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "Age does not exist. Available: age, age_square, workclass, fnlwgt, education, education_num, marital, occupation, relationship, race, sex, capital_gain, capital_loss, hours_week, native_country, label, workclassIndex, workclassclassVec, educationIndex, educationclassVec, maritalIndex, maritalclassVec, occupationIndex, occupationclassVec, relationshipIndex, relationshipclassVec, raceIndex, raceclassVec, sexIndex, sexclassVec, native_countryIndex, native_countryclassVec, newlabel",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m pipeline \u001b[39m=\u001b[39m Pipeline(stages\u001b[39m=\u001b[39mstages)\n\u001b[1;32m      3\u001b[0m pipelineModel \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39mfit(df_remove)\n\u001b[0;32m----> 4\u001b[0m model \u001b[39m=\u001b[39m pipelineModel\u001b[39m.\u001b[39;49mtransform(df_remove)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Python_3_9/lib/python3.9/site-packages/pyspark/ml/base.py:262\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_transform(dataset)\n\u001b[1;32m    261\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform(dataset)\n\u001b[1;32m    263\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParams must be a param map but got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Python_3_9/lib/python3.9/site-packages/pyspark/ml/pipeline.py:304\u001b[0m, in \u001b[0;36mPipelineModel._transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_transform\u001b[39m(\u001b[39mself\u001b[39m, dataset: DataFrame) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[1;32m    303\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstages:\n\u001b[0;32m--> 304\u001b[0m         dataset \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39;49mtransform(dataset)\n\u001b[1;32m    305\u001b[0m     \u001b[39mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Python_3_9/lib/python3.9/site-packages/pyspark/ml/base.py:262\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_transform(dataset)\n\u001b[1;32m    261\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform(dataset)\n\u001b[1;32m    263\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParams must be a param map but got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Python_3_9/lib/python3.9/site-packages/pyspark/ml/wrapper.py:398\u001b[0m, in \u001b[0;36mJavaTransformer._transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 398\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrame(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mtransform(dataset\u001b[39m.\u001b[39;49m_jdf), dataset\u001b[39m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Python_3_9/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Python_3_9/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: Age does not exist. Available: age, age_square, workclass, fnlwgt, education, education_num, marital, occupation, relationship, race, sex, capital_gain, capital_loss, hours_week, native_country, label, workclassIndex, workclassclassVec, educationIndex, educationclassVec, maritalIndex, maritalclassVec, occupationIndex, occupationclassVec, relationshipIndex, relationshipclassVec, raceIndex, raceclassVec, sexIndex, sexclassVec, native_countryIndex, native_countryclassVec, newlabel"
     ]
    }
   ],
   "source": [
    "# Create a Pipeline.\n",
    "pipeline = Pipeline(stages=stages)\n",
    "pipelineModel = pipeline.fit(df_remove)\n",
    "model = pipelineModel.transform(df_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d5685a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'StringIndexerModel' object has no attribute 'take'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mtake(\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'StringIndexerModel' object has no attribute 'take'"
     ]
    }
   ],
   "source": [
    "model.take(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3ec1949",
   "metadata": {},
   "source": [
    "# Step 4: Build the classifier: logistic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57b272ec",
   "metadata": {},
   "source": [
    "To make the computation faster, we convert features to DenseVector type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "625f5b90",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'StringIndexerModel' object has no attribute 'rdd'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinalg\u001b[39;00m \u001b[39mimport\u001b[39;00m DenseVector\n\u001b[0;32m----> 2\u001b[0m input_data \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mrdd\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m x: (x[\u001b[39m\"\u001b[39m\u001b[39mnewlabel\u001b[39m\u001b[39m\"\u001b[39m], DenseVector(x[\u001b[39m\"\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m\"\u001b[39m])))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'StringIndexerModel' object has no attribute 'rdd'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import DenseVector\n",
    "input_data = model.rdd.map(lambda x: (x[\"newlabel\"], DenseVector(x[\"features\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "532018b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#You are ready to create the train data as a DataFrame. You use the sqlContext\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_train \u001b[39m=\u001b[39m sqlContext\u001b[39m.\u001b[39mcreateDataFrame(input_data, [\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m      3\u001b[0m df_train\u001b[39m.\u001b[39mshow(\u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_data' is not defined"
     ]
    }
   ],
   "source": [
    "#You are ready to create the train data as a DataFrame. You use the sqlContext\n",
    "df_train = sqlContext.createDataFrame(input_data, [\"label\", \"features\"])\n",
    "df_train.show(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58fd0ffb",
   "metadata": {},
   "source": [
    "**Create a train/test set**\\\n",
    "split 80/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4aee59dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Split the data into train and test sets\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_data, test_data \u001b[39m=\u001b[39m df_train\u001b[39m.\u001b[39mrandomSplit([\u001b[39m.8\u001b[39m,\u001b[39m.2\u001b[39m],seed\u001b[39m=\u001b[39m\u001b[39m1234\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Split the data into train and test sets\n",
    "train_data, test_data = df_train.randomSplit([.8,.2],seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "68313a22",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#Let's count how many people with income below/above 50k in both training and test set\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m train_data\u001b[39m.\u001b[39mgroupby(\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39magg({\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mcount\u001b[39m\u001b[39m'\u001b[39m})\u001b[39m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "#Let's count how many people with income below/above 50k in both training and test set\n",
    "\n",
    "train_data.groupby('label').agg({'label': 'count'}).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a8a5a36",
   "metadata": {},
   "source": [
    "## Build the logistic regressor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9dba587",
   "metadata": {},
   "source": [
    "Last but not least, you can build the classifier. Pyspark has an API called LogisticRegression to perform logistic regression.\n",
    "\n",
    "You initialize lr by indicating the label column and feature columns. You set a maximum of 10 iterations and add a regularization parameter with a value of 0.3. Note that in the next section, you will use cross-validation with a parameter grid to tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "14664df6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 11\u001b[0m\n\u001b[1;32m      5\u001b[0m lr \u001b[39m=\u001b[39m LogisticRegression(labelCol\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m                         featuresCol\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m                         maxIter\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,\n\u001b[1;32m      8\u001b[0m                         regParam\u001b[39m=\u001b[39m\u001b[39m0.3\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[39m# Fit the data to the model\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m linearModel \u001b[39m=\u001b[39m lr\u001b[39m.\u001b[39mfit(train_data)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Import `LinearRegression`\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Initialize `lr`\n",
    "lr = LogisticRegression(labelCol=\"label\",\n",
    "                        featuresCol=\"features\",\n",
    "                        maxIter=10,\n",
    "                        regParam=0.3)\n",
    "\n",
    "# Fit the data to the model\n",
    "linearModel = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1674a340",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'linearModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Print the coefficients and intercept for logistic regression\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCoefficients: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(linearModel\u001b[39m.\u001b[39mcoefficients))\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mIntercept: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(linearModel\u001b[39m.\u001b[39mintercept))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'linearModel' is not defined"
     ]
    }
   ],
   "source": [
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(linearModel.coefficients))\n",
    "print(\"Intercept: \" + str(linearModel.intercept))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d7b4722",
   "metadata": {},
   "source": [
    "# Step 5: Train and evaluate the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "828f04eb",
   "metadata": {},
   "source": [
    "To generate predictions for your test set,\n",
    "\n",
    "You can use linearModel with `transform()` on test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "376172e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'linearModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Make predictions on test data using the transform() method.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m predictions \u001b[39m=\u001b[39m linearModel\u001b[39m.\u001b[39mtransform(test_data)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'linearModel' is not defined"
     ]
    }
   ],
   "source": [
    "# Make predictions on test data using the transform() method.\n",
    "predictions = linearModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9c2ee8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predictions\u001b[39m.\u001b[39mprintSchema()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9dcff1b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m selected \u001b[39m=\u001b[39m predictions\u001b[39m.\u001b[39mselect(\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mprediction\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mprobability\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m selected\u001b[39m.\u001b[39mshow(\u001b[39m20\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "selected = predictions.select(\"label\", \"prediction\", \"probability\")\n",
    "selected.show(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "252424e9",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fdbfa735",
   "metadata": {},
   "source": [
    "You need to look at the accuracy metric to see how well (or bad) the model performs. Currently, there is no API to compute the accuracy measure in Spark. The default value is the ROC, receiver operating characteristic curve. It is a different metric that take into account the false positive rate.\n",
    "\n",
    "Before you look at the ROC, let's construct the accuracy measure. You are more familiar with this metric. The accuracy measure is the sum of the correct prediction over the total number of observations.\n",
    "\n",
    "You create a DataFrame with the label and the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6c991d38",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cm \u001b[39m=\u001b[39m predictions\u001b[39m.\u001b[39mselect(\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mprediction\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "cm = predictions.select(\"label\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c43b7560",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#You can check the number of class in the label and the prediction\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m cm\u001b[39m.\u001b[39mgroupby(\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39magg({\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mcount\u001b[39m\u001b[39m'\u001b[39m})\u001b[39m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cm' is not defined"
     ]
    }
   ],
   "source": [
    "#You can check the number of class in the label and the prediction\n",
    "cm.groupby('label').agg({'label': 'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a1f52804",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cm\u001b[39m.\u001b[39mgroupby(\u001b[39m'\u001b[39m\u001b[39mprediction\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39magg({\u001b[39m'\u001b[39m\u001b[39mprediction\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mcount\u001b[39m\u001b[39m'\u001b[39m})\u001b[39m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cm' is not defined"
     ]
    }
   ],
   "source": [
    "cm.groupby('prediction').agg({'prediction': 'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8121a790",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#You can compute the accuracy by computing the count when the label is correctly classified over the total number of rows.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m cm\u001b[39m.\u001b[39mfilter(cm\u001b[39m.\u001b[39mlabel \u001b[39m==\u001b[39m cm\u001b[39m.\u001b[39mprediction)\u001b[39m.\u001b[39mcount() \u001b[39m/\u001b[39m cm\u001b[39m.\u001b[39mcount()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cm' is not defined"
     ]
    }
   ],
   "source": [
    "#You can compute the accuracy by computing the count when the label is correctly classified over the total number of rows.\n",
    "cm.filter(cm.label == cm.prediction).count() / cm.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "00115c6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'linearModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m     acc \u001b[39m=\u001b[39m cm\u001b[39m.\u001b[39mfilter(cm\u001b[39m.\u001b[39mlabel \u001b[39m==\u001b[39m cm\u001b[39m.\u001b[39mprediction)\u001b[39m.\u001b[39mcount() \u001b[39m/\u001b[39m cm\u001b[39m.\u001b[39mcount()\n\u001b[1;32m      7\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mModel accuracy: \u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39m%%\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (acc \u001b[39m*\u001b[39m \u001b[39m100\u001b[39m)) \n\u001b[0;32m----> 8\u001b[0m accuracy_m(model \u001b[39m=\u001b[39m linearModel)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'linearModel' is not defined"
     ]
    }
   ],
   "source": [
    "#You can wrap everything together and write a function to compute the accuracy.\n",
    "\n",
    "def accuracy_m(model): \n",
    "    predictions = model.transform(test_data)\n",
    "    cm = predictions.select(\"label\", \"prediction\")\n",
    "    acc = cm.filter(cm.label == cm.prediction).count() / cm.count()\n",
    "    print(\"Model accuracy: %.3f%%\" % (acc * 100)) \n",
    "accuracy_m(model = linearModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68900b78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62217bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be51a29a",
   "metadata": {},
   "source": [
    "### ROC metrics\n",
    "\n",
    "The module BinaryClassificationEvaluator includes the ROC measures. The Receiver Operating Characteristic curve is another common tool used with binary classification. It is very similar to the precision/recall curve, but instead of plotting precision versus recall, the ROC curve shows the true positive rate (i.e. recall) against the false positive rate. The false positive rate is the ratio of negative instances that are incorrectly classified as positive. It is equal to one minus the true negative rate. The true negative rate is also called specificity. Hence the ROC curve plots `sensitivity` (recall) versus 1 - `specificity.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6796c70b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39m# Evaluate model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m evaluator \u001b[39m=\u001b[39m BinaryClassificationEvaluator(rawPredictionCol\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrawPrediction\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[39mprint\u001b[39m(evaluator\u001b[39m.\u001b[39mevaluate(predictions))\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(evaluator\u001b[39m.\u001b[39mgetMetricName())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "### Use ROC \n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "print(evaluator.evaluate(predictions))\n",
    "print(evaluator.getMetricName())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "354fa12f",
   "metadata": {},
   "source": [
    "# Step 6 (Stretch): Tune the hyperparameter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7230782",
   "metadata": {},
   "source": [
    "Last but not least, you can tune the hyperparameters. Similar to scikit-learn you create a parameter grid, and you add the parameters you want to tune.\n",
    "\n",
    "To reduce the time of the computation, you only tune the regularization parameter with only two values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4dfcc2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.5])\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ae1a35a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 11\u001b[0m\n\u001b[1;32m      6\u001b[0m cv \u001b[39m=\u001b[39m CrossValidator(estimator\u001b[39m=\u001b[39mlr,\n\u001b[1;32m      7\u001b[0m                     estimatorParamMaps\u001b[39m=\u001b[39mparamGrid,\n\u001b[1;32m      8\u001b[0m                     evaluator\u001b[39m=\u001b[39mevaluator, numFolds\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[39m# Run cross validations\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m cvModel \u001b[39m=\u001b[39m cv\u001b[39m.\u001b[39mfit(train_data)\n\u001b[1;32m     12\u001b[0m \u001b[39m# likely take a fair amount of time\u001b[39;00m\n\u001b[1;32m     13\u001b[0m end_time \u001b[39m=\u001b[39m time()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Finally, you evaluate the model with using the cross-validation method with 5 folds. It takes some time to train.\n",
    "from time import *\n",
    "start_time = time()\n",
    "\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(train_data)\n",
    "# likely take a fair amount of time\n",
    "end_time = time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Time to train model: %.3f seconds\" % elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "30ba60d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cvModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#The best regularization hyperparameter is 0.01, with an accuracy of 85.316 percent.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m accuracy_m(model \u001b[39m=\u001b[39m cvModel)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cvModel' is not defined"
     ]
    }
   ],
   "source": [
    "#The best regularization hyperparameter is 0.01, with an accuracy of 85.316 percent.\n",
    "\n",
    "accuracy_m(model = cvModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c5bdf571",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cvModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# You can extract the recommended parameter by chaining cvModel.bestModel with extractParamMap()\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m bestModel \u001b[39m=\u001b[39m cvModel\u001b[39m.\u001b[39mbestModel\n\u001b[1;32m      4\u001b[0m bestModel\u001b[39m.\u001b[39mextractParamMap()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cvModel' is not defined"
     ]
    }
   ],
   "source": [
    "# You can extract the recommended parameter by chaining cvModel.bestModel with extractParamMap()\n",
    "\n",
    "bestModel = cvModel.bestModel\n",
    "bestModel.extractParamMap()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07d28773",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_3_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
