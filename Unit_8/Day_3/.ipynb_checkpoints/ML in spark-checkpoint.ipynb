{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60270b18-aa24-4e22-a899-ea6539669ef4",
   "metadata": {},
   "source": [
    "**ML in Spark**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ebfac2-cf15-4a38-b05b-e04cd903cb23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76d3eab1-901d-43e8-9aa5-8b87c9459b04",
   "metadata": {},
   "source": [
    "### Spark Context\n",
    "SparkContext is the internal engine that allows the connections with the clusters. If you want to run an operation, you need a SparkContext."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3449ec-6e5c-433a-8e4e-292245c22a18",
   "metadata": {},
   "source": [
    "### Create a SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b16be4d-f2ec-44e0-8f87-30e813397f3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First of all, you need to initiate a SparkContext.\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "sc =SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228be66f-0710-4826-b07f-134c528ef422",
   "metadata": {},
   "source": [
    "Now that the SparkContext is ready, you can create a collection of data called RDD, Resilient Distributed Dataset. Computation in an RDD is automatically parallelized across the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23b240fb-c491-4224-9cfd-42396a23533d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nums= sc.parallelize([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226c43c1-700a-48dd-ba6a-02d9b8dd866a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0a0f672-fbdb-4195-994e-fa09d41bf9bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# you can access the first row\n",
    "nums.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82ceff5-ee05-46da-ad6e-14e3f3d6b302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0a63caf-4e2c-4929-b162-60e6117c2391",
   "metadata": {},
   "source": [
    "You can apply a transformation to the data with a lambda function. In the PySpark example below, you return the square of nums. It is a map transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cdd08b8-18d2-4e10-a83b-c06da2ee5d8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \n",
      "4 \n",
      "9 \n",
      "16 \n"
     ]
    }
   ],
   "source": [
    "squared = nums.map(lambda x: x*x).collect()\n",
    "for num in squared:\n",
    "    print('%i ' % (num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483a2d50-95af-42c1-b60d-7e2d106b9f53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b697a218-510d-4998-bad9-bb04abf49e37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4eee4d8-0aaa-4886-9bed-58f356d2b23c",
   "metadata": {},
   "source": [
    "### SQLContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1517d499-ac46-46b8-b468-b4342390009d",
   "metadata": {},
   "source": [
    "A more convenient way is to use the DataFrame. SparkContext is already set, you can use it to create the DataFrame. You also need to declare the SQLContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cca1dee-e780-4b77-bb95-8d6ce595196e",
   "metadata": {},
   "source": [
    "SQLContext allows connecting the engine with different data sources. It is used to initiate the functionalities of Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcf6baef-fe78-4d20-81d1-255e8dc8c263",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6278ec1-c74b-4109-ac52-1af6d1a0a4f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now in this Spark tutorial Python, let's create a list of tuples. \\\n",
    "#Each tuple will contain the name of the people and their age. Four steps are required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ada44dbd-9ca5-44ab-a015-58e1ff285819",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the list of tuple with the information\n",
    "list_p = [('John',19),('Smith',29),('Adam',35),('Henry',50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72719dc4-80ba-4596-a7bf-fd233dde9b31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build the RDD\n",
    "rdd = sc.parallelize(list_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7adf1d74-9e07-4c36-a630-820bd637ef1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Convert to Tuples\n",
    "ppl = rdd.map(lambda x: Row(name=x[0], age=int(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c11bffc0-d50f-4460-b3d5-5d092b3981d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame context\n",
    "DF_ppl = sqlContext.createDataFrame(ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f551aae-4467-4c1e-b7ca-392643993dd7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#If you want to access the type of each feature, you can use printSchema()\n",
    "\n",
    "DF_ppl.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15917907-0008-49f4-aac9-4c2ee90376cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "| John| 19|\n",
      "|Smith| 29|\n",
      "| Adam| 35|\n",
      "+-----+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF_ppl.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e7ca2d-44e8-4e75-ac40-9ad3d046fe42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c44cc6d6-47af-4935-bef7-fd119a7a1877",
   "metadata": {},
   "source": [
    "# Machine Learning Example with PySpark\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8563bdd-e55b-4a01-b8ea-4a3d33924a87",
   "metadata": {},
   "source": [
    "Now that you have a brief idea of Spark and SQLContext, you are ready to build your first Machine Learning program using Spark.\n",
    "\n",
    "Following are the steps to build a Machine Learning program with PySpark:\n",
    "\n",
    "1. Basic operation with PySpark\\\n",
    "2. Data preprocessing\\\n",
    "3. Build a data processing pipeline\\\n",
    "4. Build the classifier: logistic\\\n",
    "5. Train and evaluate the model\\\n",
    "6. Tune the hyperparameter\\\n",
    "Note that, the dataset we use is not very big and you may think that the computation takes a long time. Spark is designed to process a considerable amount of data. Spark's performance increases relatively to other machine learning libraries when the dataset grows larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6801bd-7d1e-411a-b346-c3e495d32466",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
