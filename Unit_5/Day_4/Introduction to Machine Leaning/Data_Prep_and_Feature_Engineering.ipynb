{"cells":[{"cell_type":"markdown","metadata":{"id":"7UY-xufLMVoD"},"source":["# Data Preparation and Feature Engineering Tutorial\n","\n","Notebook from [Andrew Berry](https://drive.google.com/file/d/1In_FOHxnPUeokSrcBq0lo1R6mGQ_6hEM/view).\n","\n","In this tutorial, we will discuss data preparation and feature engineering techniques within the ML process.\n","\n","## Data Preparation\n","\n","The main goal of this phase is to prepare the data for exploratory data analysis, inferential analysis, or prediction (modeling). In other words, we're making sure our data is in good shape, we have treated our missing values, dealt with weird data, and cleaned it up.\n","\n","In this notebook, we will cover three common data preparation techniques:\n","1. Outlier Detection\n","1. Handling Missing Values\n","1. Variable Transformation"]},{"cell_type":"markdown","metadata":{"id":"zTjdWTd2MVoH"},"source":["---\n","## 1. Outlier Detection\n","\n","- Data can have incorrect values (human error, system error)\n","- **Outlier**: an observation point that is distant from other observations\n","- Outliers are helpful for pointing out what can be wrong\n","- It is sometimes obvious how to deal with errors, but often it requires domain-specific knowledge to determine the proper course of action.\n","\n","> Note: Before simply deleting outliers, determine if this is needed. It depends on your use case and if the outliers are important (e.g., fraud detection).\n","\n","### Outlier Detection: demo\n","- Docs: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html\n","- [Example source](https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["sklearn: 1.2.2\n"]}],"source":["# scikit-learn version find\n","import sklearn\n","print('sklearn: %s' % sklearn.__version__)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"ename":"ImportError","evalue":"\n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_boston\n","File \u001b[0;32m~/opt/anaconda3/envs/THE_ONE/lib/python3.8/site-packages/sklearn/datasets/__init__.py:156\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mload_boston\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    106\u001b[0m     msg \u001b[39m=\u001b[39m textwrap\u001b[39m.\u001b[39mdedent(\n\u001b[1;32m    107\u001b[0m         \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[39m        `load_boston` has been removed from scikit-learn since version 1.2.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39m        \"\"\"\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     )\n\u001b[0;32m--> 156\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(msg)\n\u001b[1;32m    157\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mglobals\u001b[39m()[name]\n","\u001b[0;31mImportError\u001b[0m: \n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n"]}],"source":["from sklearn.datasets import load_boston"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T1pMi_k5MVoI"},"outputs":[],"source":["\n","import pandas as pd\n","import numpy as np\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","boston = load_boston()\n","X = boston.data\n","y = boston.target\n","columns = boston.feature_names\n","\n","#create the dataframe\n","boston_df = pd.DataFrame(boston.data)\n","boston_df.columns = columns\n","boston_df.head()"]},{"cell_type":"markdown","metadata":{"id":"EpLGL1UnMVoJ"},"source":["Boston real-estate data:\n","\n","- CRIM per capita crime rate by town\n","\n","- ZN proportion of residential land zoned for lots over 25,000 sq.ft.\n","\n","- INDUS proportion of non-retail business acres per town\n","\n","- CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n","\n","- NOX nitric oxides concentration (parts per 10 million)\n","\n","- RM average number of rooms per dwelling\n","\n","- AGE proportion of owner-occupied units built prior to 1940\n","\n","- DIS weighted distances to five Boston employment centres\n","\n","- RAD index of accessibility to radial highways\n","\n","- TAX full-value property-tax rate per $10,000\n","\n","- PTRATIO pupil-teacher ratio by town\n","\n","- B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n","\n","- LSTAT % lower status of the population\n","\n","- MEDV (target variable) Median value of owner-occupied homes in $1000â€™s\n","\n"]},{"cell_type":"markdown","metadata":{"id":"fqbhhx42MVoK"},"source":["**Method 1: Summary of the data**\n","\n","- Use your intuition\n","- Ask a domain expert\n","\n","Does anything stand out to you?\n","\n","Which summary statistics (mean, std, etc.) did you find yourself checking?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ITjESOQMVoK","scrolled":true},"outputs":[],"source":["boston_df.describe()"]},{"cell_type":"markdown","metadata":{"id":"zQKg8X70MVoL"},"source":["Notice:\n","- CRIM and DIS max, B min\n","- ZN, INDUS, AGE, LSTAT are percentages"]},{"cell_type":"markdown","metadata":{"id":"Qn7NEeEAMVoL"},"source":["**Method 2: Visualizing a Single Variable**\n","\n","Boxplots are good for visualzing distributions (skewness) and identifying outliers.\n","\n","sns.boxplot has a parameter `whis=1.5` that controls the whiskers. For very skewed data, use a higher value of `whis`.\n","> `whis` is the proportion of the IQR past the low and high quartiles to extend the plot whiskers. Points outside this range will be identified as outliers."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Whh5yBnEMVoM"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","sns.boxplot(x=boston_df['DIS']);\n","#sns.boxplot(x=boston_df['CRIM'], whis=15);\n","#sns.boxplot(x=boston_df['B'], whis=15);\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"rwqVL12lMVoN"},"source":["**Method 3: Visualizing Multi-Variables**\n","\n","Scatter plots are good at identifying outliers since every data point is plotted. Can also identify data points that don't follow a general trend.\n","\n","What conclusions can you draw from this plot?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m-KrE-E6MVoO"},"outputs":[],"source":["fig, ax = plt.subplots(figsize=(16,8))\n","ax.scatter(boston_df['CRIM'], y)\n","ax.set_xlabel('Per capita crime rate by town')\n","ax.set_ylabel('Median value of owner-occupied homes in $1000â€™s')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"veCck6hsMVoO"},"source":["**Method 4: Z-Score**\n","\n","A way to detect outliers is to remove values with a z-score greater than 3. The z-score is measured in terms of standard deviations from the mean.\n","\n","$$\n","Z = \\dfrac{x-\\mu}{\\sigma}\n","$$\n","\n","- Z-score of 0 indicates the value is the mean\n","- Z-score of 1 indicates the value is within 1 standard deviation from the mean. \n","- Z-score of 2 indicates the value is within 2 standard deviations from the mean.\n","- Z-score of 3 indicates the value is within 3 standard deviations from the mean.\n","- **Z-scores above 3 indicate the value is greater than 3 standard deviations from the mean. Data Scientists often label values with a z-score above 3 as outliers.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G5Q5RzG9MVoP"},"outputs":[],"source":["from scipy import stats\n","\n","#Finding Z Score on Column\n","stats.zscore(boston_df['ZN'])\n","\n","#Turning Absolute\n","#np.abs(stats.zscore(boston_df['ZN']))\n","\n","#view the outliers\n","boston_df['ZN'][(np.abs(stats.zscore(boston_df['ZN'])) > 3)]"]},{"cell_type":"markdown","metadata":{"id":"mxsbqwBsMVoQ"},"source":["### Dealing with Outliers\n","\n","- Can drop the observation (if appropriate)\n","- Can fix the observation (e.g., obvious typo)\n","- Explore what caused the outlier"]},{"cell_type":"markdown","metadata":{"id":"ful94chsMVoQ"},"source":["---\n","## 2. Handling Missing Values\n","\n","Many times we will be handed data with missing data or corrupted data. Most commonly, missing data are represented as NaNs. NaNs are blank elements in Pandas. \n","\n","- It can be a system error that causes missing values, or it wasn't captured.\n","- There are techinques to deal with missing data, but all of them are imperfect. \n","\n","Resource: https://www.geeksforgeeks.org/working-with-missing-data-in-pandas/\n","\n","### Null values: Demo\n","- Dataset: https://www.kaggle.com/uciml/pima-indians-diabetes-database/data#"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L2o3YDbQMVoQ"},"outputs":[],"source":["import pandas as pd\n","diabetes_df = pd.read_csv('data/diabetes.csv')\n","diabetes_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PcPBUC-UMVoR"},"outputs":[],"source":["diabetes_df.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UMS-eA0qMVoR"},"outputs":[],"source":["#Check percentage of data missing for each feature/column\n","round(100*(diabetes_df.isnull().sum()/len(diabetes_df)),2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JqpAixgSMVoR"},"outputs":[],"source":["diabetes_df.info()"]},{"cell_type":"markdown","metadata":{"id":"91buZWTGMVoS"},"source":["### Null values: Summary of the data\n","- Sometimes null values aren't exactly NaNs\n","- They are encoded as -1 or 9999 etc.\n","- Sometimes it's 0. \n","- Does 0 make sense for some of these categories??"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LhX2_vpiMVoS"},"outputs":[],"source":["diabetes_df.describe()"]},{"cell_type":"markdown","metadata":{"id":"O51vdGe5MVoT"},"source":["### Null values: Encoding true NaNs as NaNs\n","- Won't be used in summary calculations (e.g., average, count)\n","- Some columns have a lot of what we think could be missing values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MQPLwfTGMVoT"},"outputs":[],"source":["cols_missing_vals = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI'] # cols with inappropriate 0s\n","(diabetes_df[cols_missing_vals] == 0).sum() # count number of 0s"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c0ysbDvBMVoT"},"outputs":[],"source":["diabetes_df[cols_missing_vals] = diabetes_df[cols_missing_vals].replace(0, np.NaN) # replace 0's with NaNs\n","diabetes_df.isnull().sum()"]},{"cell_type":"markdown","metadata":{"id":"1NKRTtc6MVoT"},"source":["### Null values: Dropping Missing Values\n","- Could be a good idea if there aren't too many records removed\n","- Let's do this for Glucose and BMI columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hywdZolTMVoU"},"outputs":[],"source":["print(\"Shape before dropping NAs\", diabetes_df.shape)\n","\n","diabetes_df = diabetes_df.dropna(subset=['Glucose', 'BMI']) # drop rows with Glucose and BMI as NaN\n","\n","print(\"Shape after dropping NAs for Glucose and BMI columns\", diabetes_df.shape)"]},{"cell_type":"markdown","metadata":{"id":"qvxgcBeQMVoU"},"source":["### Null values: using the average"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r8lJhdlQMVoU"},"outputs":[],"source":["# Fill in missing values with the average (for SkinThickness)\n","diabetes_df['SkinThickness'] = diabetes_df['SkinThickness'].fillna(value=diabetes_df['SkinThickness'].mean())\n","diabetes_df.isnull().sum()"]},{"cell_type":"markdown","metadata":{"id":"5Wj1gESYMVoU"},"source":["---\n","## 3. Data Preparation: Variable Transformation\n","\n","- Basic transformations (e.g., logarithmic (making it more normally distributed))\n","- Binning (e.g., grouping numbers into bins)\n","- Scaling (e.g., setting everything between 0 and 1)\n","- Dummy variables (e.g., turning categories into multiple columns of binary variables) - BE CAREFUL\n","\n","Will learn more when we get into `scikit-learn` library and dive into unsupervised and supervised learning."]},{"cell_type":"markdown","metadata":{"id":"Dp-9yew2MVoV"},"source":["### Log transformation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6xJd8WrHMVoV","scrolled":false},"outputs":[],"source":["#plot original distribution of Insulin\n","diabetes_df['Insulin'].plot.hist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AoVYvOmWMVoV"},"outputs":[],"source":["#plot distribution of Insulin after log transformation\n","np.log(diabetes_df['Insulin']).plot.hist()"]},{"cell_type":"markdown","metadata":{"id":"quBRg5BjMVoV"},"source":["### Binning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s9KDhiItMVoW","scrolled":true},"outputs":[],"source":["#Binning with q-cut (bin according to quantiles)\n","pd.qcut(diabetes_df['Age'], q = 4).value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IXtO-QuGMVoW"},"outputs":[],"source":["#Binning with cut (equally sized bins)\n","pd.cut(diabetes_df['Age'], bins = 4).value_counts()"]},{"cell_type":"markdown","metadata":{"id":"SCx552k0MVoW"},"source":["### Scaling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lokq_v5jMVoW","scrolled":true},"outputs":[],"source":["#summary statistics before scaling\n","diabetes_df.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GIv7kLAAMVoX","scrolled":true},"outputs":[],"source":["#use MinMaxScaler to scale data into a given range ((0,1) by default)\n","from sklearn.preprocessing import MinMaxScaler\n","scaler = MinMaxScaler()\n","scaled = pd.DataFrame(scaler.fit_transform(diabetes_df))\n","\n","#summary statistics after scaling\n","scaled.describe()"]},{"cell_type":"markdown","metadata":{"id":"FiiShiaQMVoX"},"source":["### Convert a categorical feature into multiple dummy variables"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WvS9XI6-MVoX"},"outputs":[],"source":["#Print Pregnancies column\n","diabetes_df['Pregnancies']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8DfnRDO3MVoX"},"outputs":[],"source":["#convert Pregnancies column into multiple dummy variables\n","pd.get_dummies(diabetes_df['Pregnancies'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3NqABCG1MVoY","scrolled":false},"outputs":[],"source":["#convert Pregnancies column into multiple dummy variables in the same dataframe\n","pd.get_dummies(diabetes_df, prefix='preg', columns=['Pregnancies'])"]},{"cell_type":"markdown","metadata":{"id":"ozRD0ySUMVoY"},"source":["## Feature Engineering\n","\n","- A key part to any DS Job is to figure out which parts are relevant to our desired outcome.\n","- The goal is to make the simplest model possible with the highest predictive power.\n","- Example: If we determine the cause of sales at a cafe is determined by two variables, price and the weather, we have a lot more predictive power and leverage than a model with thousands of variables.\n","- However, sometimes a thousand variable model is needed to explain the data.\n","\n","- Feature engineering is like making an argument for an essay. There is a lot of things with varying relevance that can be included, the hard part is choosing the most relevant/correct ones, synthesizing different arguments into one. \n","\n","- The best features are domain and problem specific. \n","- Good features ideally:\n","    - Capture most important aspects of a problem\n","    - allow learning with a few examples\n","    - generalize to new scenarios. \n","\n","**Feature engineering examples:**\n","\n","- Taking a date and extracting out the week number, weekday, month etc.\n","    - Sales are often based on seasonality. \n","- Taking freeform text (tweets) and extracting the number of words, hashtags, emojis, and counts of words etc.\n","    -  Text \"metadata' can sometimes help with sentiment anlaysis\n","    \n","- Take geographical coordintes and getting continent, country, urban vs. rural.\n","    - Housing price can depend on features extracted from geographical coordinates.\n","- Predicting NBA games, we might extract the stats of the players, and coaches, and maybe look at the recent games. Home or Away games. \n","\n","\n","**Feature Engineering vs. Feature Selection**\n","\n","Through feature engineering we usually add more features to our data to make it more complex. In Feature selection, we are trying to choose thevbest features and remove features that do not add anything to our model. One common method is to remove features that have a low variance. "]},{"cell_type":"markdown","metadata":{"id":"cNO1A5rzMVoY"},"source":["### Feature Engineering Exercise (5-10 minutes)\n","\n","You're presented with the data below.\n","\n","Think of at least 5 features you might add.\n","\n","**Note: For this exercise you will be creating new columns.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"voiL1oVPMVoY"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","retail_df = pd.DataFrame([['Protein Bar','25-01-2021', 2.99, 1024, 1],\\\n","              ['Oat Milk','25-01-2021', 3.99, 729, 1],\\\n","              ['Banana','25-01-2021', 1.99, 256, 1]],\\\n","            columns=['Item', 'Date', 'Price', 'Sales', 'Store Id'])\n","\n","retail_df.loc[:,'Date'] = pd.to_datetime(retail_df['Date'])\n","retail_df.head()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"}},"nbformat":4,"nbformat_minor":0}
